"""
æ•™è‚²æ•°æ®æ ¸éªŒç¨‹åº - ä¸“æ³¨äºå¤§æ¨¡å‹åŸå§‹å›ç­”è´¨é‡æ ¸éªŒ
ä½¿ç”¨è”ç½‘æœç´¢æ ¸éªŒLLMåŸå§‹å›ç­”çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
"""

import os
import json
import glob
import time
import random
from datetime import datetime
from typing import Dict, List, Any, Optional
from openai import OpenAI, APIConnectionError, APIError, RateLimitError
from dataclasses import dataclass

# =============================================================================
# ğŸ”¥ é…ç½®ä¿¡æ¯ ğŸ”¥
# =============================================================================

VERIFICATION_CONFIG = {
    "api_key": "sk-24XB4aUrtxi5iGUIUwHDLsgkst4sy47hKHy4j9Mg97gLG1sC",
    "base_url": "https://api.lkeap.cloud.tencent.com/v1",
    "model": "deepseek-r1",
    "data_folder": "./data",  # JSONæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹
    "output_folder": "./verification_reports",  # æ ¸éªŒæŠ¥å‘Šè¾“å‡ºæ–‡ä»¶å¤¹
    "batch_size": 3,  # æ‰¹é‡å¤„ç†å¤§å°ï¼ˆå‡å°ä»¥ä¾¿æ›´ä»”ç»†æ ¸éªŒï¼‰
    "delay_between_requests": (5, 10)  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
}

# =============================================================================
# ğŸ”¥ æ•°æ®æ¨¡å‹ ğŸ”¥
# =============================================================================

@dataclass
class ResponseVerificationResult:
    """LLMå›ç­”æ ¸éªŒç»“æœæ•°æ®ç±»"""
    university: str
    metric: str
    original_response: str
    response_length: int
    
    # å›ç­”è´¨é‡è¯„ä¼°
    information_completeness: str  # ä¿¡æ¯å®Œæ•´æ€§ï¼šå®Œæ•´/éƒ¨åˆ†å®Œæ•´/ä¸å®Œæ•´
    factual_accuracy: str         # äº‹å®å‡†ç¡®æ€§ï¼šå‡†ç¡®/éƒ¨åˆ†å‡†ç¡®/ä¸å‡†ç¡®
    source_reliability: str       # æ•°æ®æºå¯é æ€§ï¼šå¯é /ä¸€èˆ¬/ä¸å¯é 
    response_relevance: str       # å›ç­”ç›¸å…³æ€§ï¼šé«˜åº¦ç›¸å…³/ç›¸å…³/ä¸ç›¸å…³
    
    # å…·ä½“é—®é¢˜è¯†åˆ«
    identified_issues: List[str]   # å‘ç°çš„å…·ä½“é—®é¢˜
    missing_information: List[str] # ç¼ºå¤±çš„ä¿¡æ¯
    contradictory_info: List[str]  # çŸ›ç›¾ä¿¡æ¯
    outdated_info: List[str]       # è¿‡æ—¶ä¿¡æ¯
    
    # ç½‘ç»œæ ¸éªŒç»“æœ
    web_verification_summary: str  # ç½‘ç»œæ ¸éªŒæ€»ç»“
    authoritative_sources: List[str] # æƒå¨æ•°æ®æº
    verified_facts: List[str]      # æ ¸éªŒç¡®è®¤çš„äº‹å®
    disputed_facts: List[str]      # æœ‰äº‰è®®çš„äº‹å®
    
    # ç»¼åˆè¯„ä¼°
    overall_quality_score: int     # ç»¼åˆè´¨é‡è¯„åˆ† (0-100)
    credibility_rating: str        # å¯ä¿¡åº¦è¯„çº§ï¼šé«˜/ä¸­/ä½
    requires_correction: bool      # æ˜¯å¦éœ€è¦çº æ­£
    
    # åŸå§‹æ•°æ®
    verification_details: str      # è¯¦ç»†æ ¸éªŒè¯´æ˜
    raw_verification_response: str # åŸå§‹æ ¸éªŒå›ç­”
    timestamp: str

# =============================================================================
# ğŸ”¥ LLMå›ç­”è´¨é‡æ ¸éªŒå™¨ ğŸ”¥
# =============================================================================

class LLMResponseVerifier:
    """LLMå›ç­”è´¨é‡æ ¸éªŒå™¨"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=VERIFICATION_CONFIG["api_key"],
            base_url=VERIFICATION_CONFIG["base_url"]
        )
        
        # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
        os.makedirs(VERIFICATION_CONFIG["output_folder"], exist_ok=True)
        
        # æŒ‡æ ‡æè¿°æ˜ å°„
        self.metric_descriptions = {
            "esi_1_percent": "ESIå‰1%å­¦ç§‘æ•°é‡",
            "esi_1_permille": "ESIå‰1â€°å­¦ç§‘æ•°é‡", 
            "undergraduate_majors_total": "æœ¬ç§‘ä¸“ä¸šæ€»æ•°",
            "major_accreditation": "ä¸“ä¸šè®¤è¯é€šè¿‡æ•°é‡",
            "national_first_class_majors": "å›½å®¶çº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹æ•°é‡",
            "provincial_first_class_majors": "çœçº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹æ•°é‡",
            "national_teaching_awards": "å›½å®¶çº§æ•™å­¦æˆæœå¥–æ•°é‡",
            "provincial_teaching_awards": "çœçº§æ•™å­¦æˆæœå¥–æ•°é‡",
            "youth_teacher_competition": "å…¨å›½é«˜æ ¡é’å¹´æ•™å¸ˆæ•™å­¦ç«èµ›è·å¥–æ•°é‡",
            "national_first_class_courses": "å›½å®¶çº§ä¸€æµæœ¬ç§‘è¯¾ç¨‹æ•°é‡",
            "provincial_first_class_courses": "çœçº§ä¸€æµæœ¬ç§‘è¯¾ç¨‹æ•°é‡",
            "national_smart_platform_courses": "å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°è¯¾ç¨‹æ•°é‡",
            "provincial_smart_platform_courses": "çœçº§æ™ºæ…§æ•™è‚²å¹³å°è¯¾ç¨‹æ•°é‡"
        }

    def load_data_files(self) -> List[Dict[str, Any]]:
        """åŠ è½½dataæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶"""
        data_files = []
        json_pattern = os.path.join(VERIFICATION_CONFIG["data_folder"], "*.json")
        
        for file_path in glob.glob(json_pattern):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    data_files.append({
                        "file_path": file_path,
                        "file_name": os.path.basename(file_path),
                        "data": data
                    })
                    print(f"âœ… å·²åŠ è½½: {os.path.basename(file_path)}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {file_path}: {e}")
        
        print(f"\nğŸ“ æ€»å…±åŠ è½½äº† {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        return data_files

    def create_response_verification_prompt(self, university: str, metric: str, original_response: str, year: int = 2024) -> str:
        """åˆ›å»ºLLMå›ç­”è´¨é‡æ ¸éªŒæç¤ºè¯"""
        metric_description = self.metric_descriptions.get(metric, metric)
        
        # æˆªå–å›ç­”çš„å‰1000å­—ç¬¦ç”¨äºå±•ç¤ºï¼ˆé¿å…æç¤ºè¯è¿‡é•¿ï¼‰
        response_preview = original_response[:1000] + "..." if len(original_response) > 1000 else original_response
        
        return f"""è¯·å¯¹ä»¥ä¸‹LLMåŸå§‹å›ç­”è¿›è¡Œå…¨é¢è´¨é‡æ ¸éªŒï¼š

ğŸ¯ **æ ¸éªŒç›®æ ‡**ï¼š
- **å­¦æ ¡åç§°**: {university}
- **æŸ¥è¯¢æŒ‡æ ‡**: {metric_description}
- **ç›®æ ‡å¹´ä»½**: {year}å¹´

ğŸ“‹ **åŸå§‹LLMå›ç­”**ï¼š
{response_preview}

ğŸ” **æ ¸éªŒç»´åº¦å’Œè¦æ±‚**ï¼š

1ï¸âƒ£ **ä¿¡æ¯å®Œæ•´æ€§æ ¸éªŒ**ï¼š
   - å›ç­”æ˜¯å¦åŒ…å«äº†æŸ¥è¯¢æŒ‡æ ‡çš„å®Œæ•´ä¿¡æ¯ï¼Ÿ
   - æ˜¯å¦æä¾›äº†å…·ä½“çš„æ•°é‡ã€åç§°ã€æ—¶é—´ç­‰å…³é”®ä¿¡æ¯ï¼Ÿ
   - ç¼ºå¤±å“ªäº›é‡è¦ä¿¡æ¯ï¼Ÿ

2ï¸âƒ£ **äº‹å®å‡†ç¡®æ€§æ ¸éªŒ**ï¼š
   - è¯·è”ç½‘æœç´¢éªŒè¯å›ç­”ä¸­çš„å…³é”®äº‹å®
   - å­¦æ ¡åç§°æ˜¯å¦æ­£ç¡®ï¼Ÿæ˜¯å¦å­˜åœ¨æ··æ·†ï¼Ÿ
   - æ•°æ®æ˜¯å¦ä¸æƒå¨æºä¸€è‡´ï¼Ÿ
   - æ—¶é—´ä¿¡æ¯æ˜¯å¦å‡†ç¡®ï¼Ÿ

3ï¸âƒ£ **æ•°æ®æºå¯é æ€§æ ¸éªŒ**ï¼š
   - å›ç­”ä¸­æåˆ°çš„æ•°æ®æºæ˜¯å¦æƒå¨ï¼Ÿ
   - æ˜¯å¦å¼•ç”¨äº†å®˜æ–¹ç½‘ç«™ã€æ”¿åºœéƒ¨é—¨ç­‰å¯ä¿¡æºï¼Ÿ
   - æ•°æ®æºçš„æ—¶æ•ˆæ€§å¦‚ä½•ï¼Ÿ

4ï¸âƒ£ **é€»è¾‘ä¸€è‡´æ€§æ ¸éªŒ**ï¼š
   - å›ç­”å†…éƒ¨æ˜¯å¦å­˜åœ¨çŸ›ç›¾ä¿¡æ¯ï¼Ÿ
   - æ•°æ®å‰åæ˜¯å¦ä¸€è‡´ï¼Ÿ
   - æ¨ç†é€»è¾‘æ˜¯å¦åˆç†ï¼Ÿ

5ï¸âƒ£ **æ—¶æ•ˆæ€§æ ¸éªŒ**ï¼š
   - æ•°æ®æ˜¯å¦ä¸º{year}å¹´çš„æœ€æ–°ä¿¡æ¯ï¼Ÿ
   - æ˜¯å¦ä½¿ç”¨äº†è¿‡æ—¶çš„æ•°æ®ï¼Ÿ
   - æ—¶é—´æ ‡æ³¨æ˜¯å¦æ˜ç¡®ï¼Ÿ

ğŸ›ï¸ **æƒå¨æ•°æ®æºå¯¹æ¯”**ï¼š
è¯·è”ç½‘æœç´¢ä»¥ä¸‹æƒå¨æºè¿›è¡Œå¯¹æ¯”éªŒè¯ï¼š
- {university}å®˜ç½‘
- æ•™è‚²éƒ¨å®˜ç½‘ (moe.gov.cn)
- çœæ•™è‚²å…å®˜ç½‘
- æƒå¨æ•™è‚²åª’ä½“
- ä¸“ä¸šè®¤è¯æœºæ„å®˜ç½‘

ğŸ“Š **æ ¸éªŒè¾“å‡ºæ ¼å¼**ï¼š

**ä¿¡æ¯å®Œæ•´æ€§**: [å®Œæ•´/éƒ¨åˆ†å®Œæ•´/ä¸å®Œæ•´]
**äº‹å®å‡†ç¡®æ€§**: [å‡†ç¡®/éƒ¨åˆ†å‡†ç¡®/ä¸å‡†ç¡®]
**æ•°æ®æºå¯é æ€§**: [å¯é /ä¸€èˆ¬/ä¸å¯é ]
**å›ç­”ç›¸å…³æ€§**: [é«˜åº¦ç›¸å…³/ç›¸å…³/ä¸ç›¸å…³]

**å‘ç°çš„é—®é¢˜**:
- é—®é¢˜1: [å…·ä½“æè¿°]
- é—®é¢˜2: [å…·ä½“æè¿°]

**ç¼ºå¤±ä¿¡æ¯**:
- ç¼ºå¤±1: [å…·ä½“æè¿°]
- ç¼ºå¤±2: [å…·ä½“æè¿°]

**ç½‘ç»œæ ¸éªŒç»“æœ**:
- æ ¸éªŒç¡®è®¤çš„äº‹å®: [åˆ—å‡ºç»è¿‡æ ¸éªŒç¡®è®¤çš„å†…å®¹]
- æœ‰äº‰è®®çš„äº‹å®: [åˆ—å‡ºå­˜åœ¨äº‰è®®æˆ–ä¸ä¸€è‡´çš„å†…å®¹]
- æƒå¨æ•°æ®æº: [åˆ—å‡ºæ‰¾åˆ°çš„æƒå¨æ•°æ®æºé“¾æ¥]

**ç»¼åˆè´¨é‡è¯„åˆ†**: [0-100åˆ†]
**å¯ä¿¡åº¦è¯„çº§**: [é«˜/ä¸­/ä½]
**æ˜¯å¦éœ€è¦çº æ­£**: [æ˜¯/å¦]

**è¯¦ç»†æ ¸éªŒè¯´æ˜**:
[æä¾›è¯¦ç»†çš„æ ¸éªŒè¿‡ç¨‹å’Œå‘ç°çš„å…·ä½“é—®é¢˜]

è¯·å¼€å§‹è”ç½‘æ ¸éªŒ..."""

    def verify_single_response(self, university: str, metric: str, original_response: str, year: int = 2024) -> ResponseVerificationResult:
        """æ ¸éªŒå•ä¸ªLLMå›ç­”"""
        try:
            print(f"ğŸ” æ­£åœ¨æ ¸éªŒå›ç­”: {university} - {metric}")
            print(f"   å›ç­”é•¿åº¦: {len(original_response)} å­—ç¬¦")
            
            # åˆ›å»ºæ ¸éªŒæç¤ºè¯
            prompt = self.create_response_verification_prompt(university, metric, original_response, year)
            
            # è°ƒç”¨APIè¿›è¡Œè”ç½‘æœç´¢æ ¸éªŒ
            response = self.client.chat.completions.create(
                model=VERIFICATION_CONFIG["model"],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ•™è‚²æ•°æ®è´¨é‡å®¡æŸ¥ä¸“å®¶ï¼Œæ“…é•¿é€šè¿‡è”ç½‘æœç´¢æ ¸éªŒLLMå›ç­”çš„å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œå¯ä¿¡åº¦ã€‚ä½ éœ€è¦ä¸¥æ ¼ã€å®¢è§‚åœ°è¯„ä¼°å›ç­”è´¨é‡ï¼ŒæŒ‡å‡ºå…·ä½“é—®é¢˜å’Œæ”¹è¿›å»ºè®®ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,  # å¢åŠ å›ç­”é•¿åº¦ä»¥è·å¾—è¯¦ç»†æ ¸éªŒ
                temperature=0.1,
                extra_body={
                    "enable_search": True  # å¯ç”¨è”ç½‘æœç´¢
                }
            )
            
            raw_verification_response = response.choices[0].message.content
            
            # è§£ææ ¸éªŒç»“æœ
            verification_result = self._parse_response_verification(
                university, metric, original_response, raw_verification_response
            )
            
            print(f"   âœ… æ ¸éªŒå®Œæˆ: {verification_result.credibility_rating}å¯ä¿¡åº¦")
            return verification_result
            
        except Exception as e:
            print(f"   âŒ æ ¸éªŒå¤±è´¥: {str(e)}")
            return ResponseVerificationResult(
                university=university,
                metric=metric,
                original_response=original_response,
                response_length=len(original_response),
                information_completeness="æ— æ³•è¯„ä¼°",
                factual_accuracy="æ— æ³•è¯„ä¼°",
                source_reliability="æ— æ³•è¯„ä¼°",
                response_relevance="æ— æ³•è¯„ä¼°",
                identified_issues=[f"æ ¸éªŒå¤±è´¥: {str(e)}"],
                missing_information=[],
                contradictory_info=[],
                outdated_info=[],
                web_verification_summary="æ ¸éªŒå¤±è´¥",
                authoritative_sources=[],
                verified_facts=[],
                disputed_facts=[],
                overall_quality_score=0,
                credibility_rating="æ— æ³•è¯„ä¼°",
                requires_correction=True,
                verification_details=f"APIè°ƒç”¨å¤±è´¥: {str(e)}",
                raw_verification_response="",
                timestamp=datetime.now().isoformat()
            )

    def _parse_response_verification(self, university: str, metric: str, original_response: str, verification_response: str) -> ResponseVerificationResult:
        """è§£æLLMå›ç­”æ ¸éªŒç»“æœ"""
        import re
        
        # æå–å„ä¸ªç»´åº¦çš„è¯„ä¼°ç»“æœ
        def extract_field(pattern, default="æœªçŸ¥"):
            match = re.search(pattern, verification_response, re.IGNORECASE)
            return match.group(1).strip() if match else default
        
        # æå–åˆ—è¡¨ä¿¡æ¯
        def extract_list(pattern):
            matches = re.findall(pattern, verification_response, re.IGNORECASE | re.MULTILINE)
            return [match.strip() for match in matches if match.strip()]
        
        # æå–è¯„ä¼°ç»´åº¦
        information_completeness = extract_field(r'ä¿¡æ¯å®Œæ•´æ€§[ï¼š:]\s*([^\n]+)', "æœªè¯„ä¼°")
        factual_accuracy = extract_field(r'äº‹å®å‡†ç¡®æ€§[ï¼š:]\s*([^\n]+)', "æœªè¯„ä¼°")
        source_reliability = extract_field(r'æ•°æ®æºå¯é æ€§[ï¼š:]\s*([^\n]+)', "æœªè¯„ä¼°")
        response_relevance = extract_field(r'å›ç­”ç›¸å…³æ€§[ï¼š:]\s*([^\n]+)', "æœªè¯„ä¼°")
        
        # æå–é—®é¢˜åˆ—è¡¨
        identified_issues = extract_list(r'é—®é¢˜\d+[ï¼š:]\s*([^\n]+)')
        missing_information = extract_list(r'ç¼ºå¤±\d+[ï¼š:]\s*([^\n]+)')
        
        # æå–çŸ›ç›¾å’Œè¿‡æ—¶ä¿¡æ¯
        contradictory_info = []
        outdated_info = []
        if "çŸ›ç›¾" in verification_response:
            contradictory_info = extract_list(r'çŸ›ç›¾.*?[ï¼š:]\s*([^\n]+)')
        if "è¿‡æ—¶" in verification_response:
            outdated_info = extract_list(r'è¿‡æ—¶.*?[ï¼š:]\s*([^\n]+)')
        
        # æå–æ ¸éªŒç»“æœ
        verified_facts = extract_list(r'æ ¸éªŒç¡®è®¤çš„äº‹å®[ï¼š:].*?-\s*([^\n]+)')
        disputed_facts = extract_list(r'æœ‰äº‰è®®çš„äº‹å®[ï¼š:].*?-\s*([^\n]+)')
        
        # æå–æƒå¨æ•°æ®æº
        authoritative_sources = []
        url_patterns = [r'https?://[^\s\)]+', r'[^\s]+\.edu\.cn', r'[^\s]+\.gov\.cn']
        for pattern in url_patterns:
            authoritative_sources.extend(re.findall(pattern, verification_response))
        
        # æå–è¯„åˆ†å’Œè¯„çº§
        score_match = re.search(r'ç»¼åˆè´¨é‡è¯„åˆ†[ï¼š:]\s*(\d+)', verification_response)
        overall_quality_score = int(score_match.group(1)) if score_match else 50
        
        credibility_rating = extract_field(r'å¯ä¿¡åº¦è¯„çº§[ï¼š:]\s*([^\n]+)', "ä¸­")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦çº æ­£
        requires_correction = "éœ€è¦çº æ­£" in verification_response and "æ˜¯" in verification_response
        
        # æå–è¯¦ç»†è¯´æ˜
        details_match = re.search(r'è¯¦ç»†æ ¸éªŒè¯´æ˜[ï¼š:]\s*(.*?)(?:\n\n|\Z)', verification_response, re.DOTALL)
        verification_details = details_match.group(1).strip() if details_match else "æ— è¯¦ç»†è¯´æ˜"
        
        # æå–ç½‘ç»œæ ¸éªŒæ€»ç»“
        web_summary_match = re.search(r'ç½‘ç»œæ ¸éªŒç»“æœ[ï¼š:](.*?)(?=\*\*|$)', verification_response, re.DOTALL)
        web_verification_summary = web_summary_match.group(1).strip() if web_summary_match else "æ— ç½‘ç»œæ ¸éªŒæ€»ç»“"
        
        return ResponseVerificationResult(
            university=university,
            metric=metric,
            original_response=original_response,
            response_length=len(original_response),
            information_completeness=information_completeness,
            factual_accuracy=factual_accuracy,
            source_reliability=source_reliability,
            response_relevance=response_relevance,
            identified_issues=identified_issues,
            missing_information=missing_information,
            contradictory_info=contradictory_info,
            outdated_info=outdated_info,
            web_verification_summary=web_verification_summary,
            authoritative_sources=list(set(authoritative_sources)),  # å»é‡
            verified_facts=verified_facts,
            disputed_facts=disputed_facts,
            overall_quality_score=overall_quality_score,
            credibility_rating=credibility_rating,
            requires_correction=requires_correction,
            verification_details=verification_details,
            raw_verification_response=verification_response,
            timestamp=datetime.now().isoformat()
        )

    def verify_data_file(self, data_file: Dict[str, Any]) -> List[ResponseVerificationResult]:
        """æ ¸éªŒå•ä¸ªæ•°æ®æ–‡ä»¶ä¸­çš„æ‰€æœ‰LLMå›ç­”"""
        results = []
        data = data_file["data"]
        file_name = data_file["file_name"]
        
        print(f"\nğŸ“‹ å¼€å§‹æ ¸éªŒæ–‡ä»¶: {file_name}")
        
        # æå–åŸºæœ¬ä¿¡æ¯
        metric = data.get("metric", "æœªçŸ¥æŒ‡æ ‡")
        target_year = data.get("target_year", 2024)
        university_data = data.get("university_data", {})
        
        print(f"   æŒ‡æ ‡: {metric}")
        print(f"   å¹´ä»½: {target_year}")
        print(f"   å­¦æ ¡æ•°é‡: {len(university_data)}")
        
        # é€ä¸ªæ ¸éªŒå­¦æ ¡çš„LLMå›ç­”
        for i, (university, info) in enumerate(university_data.items(), 1):
            print(f"\n   [{i}/{len(university_data)}] {university}")
            
            # è·å–åŸå§‹LLMå›ç­”
            original_response = info.get("llm_raw_response", "")
            
            # è·³è¿‡ç©ºå›ç­”
            if not original_response or original_response.strip() == "":
                print(f"   â­ï¸  è·³è¿‡ç©ºå›ç­”")
                continue
            
            # æ ¸éªŒå•ä¸ªå›ç­”
            result = self.verify_single_response(university, metric, original_response, target_year)
            results.append(result)
            
            # è¯·æ±‚é—´éš”
            if i < len(university_data):
                delay = random.uniform(*VERIFICATION_CONFIG["delay_between_requests"])
                time.sleep(delay)
        
        return results

    def verify_all_responses(self) -> Dict[str, Any]:
        """æ ¸éªŒæ‰€æœ‰æ•°æ®æ–‡ä»¶ä¸­çš„LLMå›ç­”"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡LLMå›ç­”è´¨é‡æ ¸éªŒ...")
        
        # åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶
        data_files = self.load_data_files()
        
        if not data_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶")
            return {}
        
        all_results = []
        start_time = time.time()
        
        # é€æ–‡ä»¶æ ¸éªŒ
        for i, data_file in enumerate(data_files, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“ [{i}/{len(data_files)}] æ ¸éªŒæ–‡ä»¶: {data_file['file_name']}")
            
            file_results = self.verify_data_file(data_file)
            all_results.extend(file_results)
            
            # æ–‡ä»¶é—´ä¼‘æ¯
            if i < len(data_files):
                time.sleep(random.uniform(8, 15))
        
        # ç”Ÿæˆæ ¸éªŒæŠ¥å‘Š
        report = self._generate_verification_report(all_results)
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_verification_report(report)
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ å›ç­”è´¨é‡æ ¸éªŒå®Œæˆ! æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        self._print_verification_summary(report)
        
        return report

    def _generate_verification_report(self, results: List[ResponseVerificationResult]) -> Dict[str, Any]:
        """ç”ŸæˆLLMå›ç­”è´¨é‡æ ¸éªŒæŠ¥å‘Š"""
        
        if not results:
            return {
                "summary": {"total": 0, "high_quality": 0, "needs_improvement": 0},
                "details": []
            }
        
        # ç»Ÿè®¡åˆ†æ
        total_count = len(results)
        high_quality = len([r for r in results if r.overall_quality_score >= 80])
        medium_quality = len([r for r in results if 60 <= r.overall_quality_score < 80])
        low_quality = len([r for r in results if r.overall_quality_score < 60])
        needs_correction = len([r for r in results if r.requires_correction])
        
        # å¹³å‡è´¨é‡è¯„åˆ†
        avg_score = sum(r.overall_quality_score for r in results) / total_count if total_count > 0 else 0
        
        # å¯ä¿¡åº¦åˆ†å¸ƒ
        credibility_dist = {}
        for result in results:
            rating = result.credibility_rating
            credibility_dist[rating] = credibility_dist.get(rating, 0) + 1
        
        # å¸¸è§é—®é¢˜åˆ†æ
        all_issues = []
        for result in results:
            all_issues.extend(result.identified_issues)
        
        issue_frequency = {}
        for issue in all_issues:
            # ç®€åŒ–é—®é¢˜åˆ†ç±»
            if "å­¦æ ¡åç§°" in issue or "åç§°é”™è¯¯" in issue:
                key = "å­¦æ ¡åç§°é”™è¯¯"
            elif "æ•°æ®æº" in issue or "æ¥æº" in issue:
                key = "æ•°æ®æºé—®é¢˜"
            elif "æ—¶é—´" in issue or "å¹´ä»½" in issue:
                key = "æ—¶é—´ä¿¡æ¯é—®é¢˜"
            elif "æ•°æ®" in issue or "æ•°å€¼" in issue:
                key = "æ•°æ®å‡†ç¡®æ€§é—®é¢˜"
            elif "å®Œæ•´" in issue or "ç¼ºå¤±" in issue:
                key = "ä¿¡æ¯å®Œæ•´æ€§é—®é¢˜"
            else:
                key = "å…¶ä»–é—®é¢˜"
            
            issue_frequency[key] = issue_frequency.get(key, 0) + 1
        
        # æŒ‰æŒ‡æ ‡åˆ†ç»„ç»Ÿè®¡
        metric_quality = {}
        for result in results:
            metric = result.metric
            if metric not in metric_quality:
                metric_quality[metric] = {"count": 0, "total_score": 0, "high_quality": 0}
            metric_quality[metric]["count"] += 1
            metric_quality[metric]["total_score"] += result.overall_quality_score
            if result.overall_quality_score >= 80:
                metric_quality[metric]["high_quality"] += 1
        
        # è®¡ç®—å„æŒ‡æ ‡å¹³å‡è´¨é‡
        for metric in metric_quality:
            stats = metric_quality[metric]
            stats["avg_score"] = stats["total_score"] / stats["count"]
            stats["quality_rate"] = f"{(stats['high_quality']/stats['count']*100):.1f}%"
        
        return {
            "verification_completed_at": datetime.now().isoformat(),
            "summary": {
                "total_responses_verified": total_count,
                "average_quality_score": f"{avg_score:.1f}",
                "quality_distribution": {
                    "high_quality_80_plus": high_quality,
                    "medium_quality_60_79": medium_quality,
                    "low_quality_below_60": low_quality
                },
                "credibility_distribution": credibility_dist,
                "responses_needing_correction": needs_correction,
                "correction_rate": f"{(needs_correction/total_count*100):.1f}%" if total_count > 0 else "0%"
            },
            "quality_analysis": {
                "metric_quality_breakdown": metric_quality,
                "common_issues_frequency": issue_frequency,
                "improvement_suggestions": self._generate_improvement_suggestions(results)
            },
            "detailed_results": [
                {
                    "university": r.university,
                    "metric": r.metric,
                    "response_length": r.response_length,
                    "quality_score": r.overall_quality_score,
                    "credibility_rating": r.credibility_rating,
                    "information_completeness": r.information_completeness,
                    "factual_accuracy": r.factual_accuracy,
                    "source_reliability": r.source_reliability,
                    "response_relevance": r.response_relevance,
                    "identified_issues_count": len(r.identified_issues),
                    "missing_information_count": len(r.missing_information),
                    "requires_correction": r.requires_correction,
                    "authoritative_sources_found": len(r.authoritative_sources),
                    "timestamp": r.timestamp
                }
                for r in results
            ],
            "raw_verification_data": [
                {
                    "university": r.university,
                    "metric": r.metric,
                    "original_response": r.original_response,
                    "verification_details": r.verification_details,
                    "identified_issues": r.identified_issues,
                    "missing_information": r.missing_information,
                    "contradictory_info": r.contradictory_info,
                    "outdated_info": r.outdated_info,
                    "verified_facts": r.verified_facts,
                    "disputed_facts": r.disputed_facts,
                    "authoritative_sources": r.authoritative_sources,
                    "raw_verification_response": r.raw_verification_response,
                    "timestamp": r.timestamp
                }
                for r in results
            ]
        }

    def _generate_improvement_suggestions(self, results: List[ResponseVerificationResult]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        # åˆ†æå¸¸è§é—®é¢˜å¹¶ç”Ÿæˆå»ºè®®
        all_issues = []
        for result in results:
            all_issues.extend(result.identified_issues)
        
        if any("å­¦æ ¡åç§°" in issue for issue in all_issues):
            suggestions.append("åŠ å¼ºå­¦æ ¡åç§°éªŒè¯æœºåˆ¶ï¼Œé¿å…æ··æ·†ä¸åŒå­¦æ ¡")
        
        if any("æ•°æ®æº" in issue for issue in all_issues):
            suggestions.append("ä¼˜å…ˆä½¿ç”¨å®˜æ–¹æƒå¨æ•°æ®æºï¼Œå¹¶æ˜ç¡®æ ‡æ³¨æ•°æ®æ¥æº")
        
        if any("æ—¶é—´" in issue for issue in all_issues):
            suggestions.append("å¼ºåŒ–æ—¶é—´ä¿¡æ¯çš„å‡†ç¡®æ€§ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°å¹´ä»½æ•°æ®")
        
        # æ ¹æ®è´¨é‡åˆ†å¸ƒç»™å‡ºå»ºè®®
        low_quality_count = len([r for r in results if r.overall_quality_score < 60])
        if low_quality_count > len(results) * 0.3:
            suggestions.append("æ€»ä½“å›ç­”è´¨é‡åä½ï¼Œå»ºè®®ä¼˜åŒ–æç¤ºè¯å’Œæœç´¢ç­–ç•¥")
        
        # æ ¹æ®å®Œæ•´æ€§é—®é¢˜ç»™å‡ºå»ºè®®
        incomplete_responses = len([r for r in results if "ä¸å®Œæ•´" in r.information_completeness])
        if incomplete_responses > len(results) * 0.2:
            suggestions.append("åŠ å¼ºä¿¡æ¯å®Œæ•´æ€§è¦æ±‚ï¼Œç¡®ä¿æä¾›å…·ä½“çš„æ•°é‡ã€åç§°ã€æ—¶é—´ç­‰è¯¦ç»†ä¿¡æ¯")
        
        return suggestions

    def _save_verification_report(self, report: Dict[str, Any]) -> str:
        """ä¿å­˜LLMå›ç­”è´¨é‡æ ¸éªŒæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = os.path.join(
            VERIFICATION_CONFIG["output_folder"], 
            f"llm_response_quality_report_{timestamp}.json"
        )
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ LLMå›ç­”è´¨é‡æ ¸éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file

    def _print_verification_summary(self, report: Dict[str, Any]):
        """æ‰“å°LLMå›ç­”è´¨é‡æ ¸éªŒæ‘˜è¦"""
        summary = report["summary"]
        quality_analysis = report["quality_analysis"]
        
        print(f"\n{'='*60}")
        print("ğŸ“Š LLMå›ç­”è´¨é‡æ ¸éªŒæ‘˜è¦")
        print(f"{'='*60}")
        print(f"æ€»æ ¸éªŒå›ç­”æ•°: {summary['total_responses_verified']}")
        print(f"å¹³å‡è´¨é‡è¯„åˆ†: {summary['average_quality_score']}/100")
        print(f"éœ€è¦çº æ­£çš„å›ç­”: {summary['responses_needing_correction']} ({summary['correction_rate']})")
        
        print(f"\nè´¨é‡åˆ†å¸ƒ:")
        quality_dist = summary['quality_distribution']
        print(f"  é«˜è´¨é‡(80+): {quality_dist['high_quality_80_plus']}")
        print(f"  ä¸­ç­‰è´¨é‡(60-79): {quality_dist['medium_quality_60_79']}")
        print(f"  ä½è´¨é‡(<60): {quality_dist['low_quality_below_60']}")
        
        print(f"\nå¯ä¿¡åº¦åˆ†å¸ƒ:")
        for rating, count in summary['credibility_distribution'].items():
            print(f"  {rating}: {count}")
        
        print(f"\nå¸¸è§é—®é¢˜:")
        for issue, freq in quality_analysis['common_issues_frequency'].items():
            print(f"  {issue}: {freq}æ¬¡")
        
        print(f"\nå„æŒ‡æ ‡è´¨é‡è¡¨ç°:")
        for metric, stats in quality_analysis['metric_quality_breakdown'].items():
            print(f"  {metric}: å¹³å‡{stats['avg_score']:.1f}åˆ†, é«˜è´¨é‡ç‡{stats['quality_rate']}")
        
        print(f"\næ”¹è¿›å»ºè®®:")
        for i, suggestion in enumerate(quality_analysis['improvement_suggestions'], 1):
            print(f"  {i}. {suggestion}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ LLMå›ç­”è´¨é‡æ ¸éªŒç¨‹åºå¯åŠ¨")
    print("-" * 60)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶å¤¹
    if not os.path.exists(VERIFICATION_CONFIG["data_folder"]):
        print(f"âŒ æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {VERIFICATION_CONFIG['data_folder']}")
        return
    
    try:
        # åˆ›å»ºæ ¸éªŒå™¨å¹¶å¼€å§‹æ ¸éªŒ
        verifier = LLMResponseVerifier()
        report = verifier.verify_all_responses()
        
        if report:
            print(f"\nâœ… LLMå›ç­”è´¨é‡æ ¸éªŒä»»åŠ¡å®Œæˆ")
        else:
            print(f"\nâŒ LLMå›ç­”è´¨é‡æ ¸éªŒä»»åŠ¡å¤±è´¥")
            
    except APIConnectionError:
        print("âŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
    except RateLimitError:
        print("âŒ è¯·æ±‚è¶…é™ï¼Œè¯·æ£€æŸ¥APIé…é¢æˆ–ç¨åé‡è¯•")
    except APIError as e:
        print(f"âŒ APIé”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()