"""çœçº§ä¸€æµè¯¾ç¨‹çˆ¬è™«ä¸»ç¨‹åº"""
import asyncio
import os
from pathlib import Path
from dotenv import load_dotenv

from config import *
from models.course_data import ProvincialCourseData
from utils.data_utils import save_provincial_course_data, generate_statistics
from utils.scraper_utils import crawl_provincial_course_data

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


async def main():
    """
    ä¸»ç¨‹åºï¼šçˆ¬å–çœçº§ä¸€æµè¯¾ç¨‹æ•°æ®
    """
    print("ğŸ¯ å¼€å§‹çˆ¬å–çœçº§ä¸€æµè¯¾ç¨‹æ•°æ®...")
    print(f"ğŸ“ ç›®æ ‡URL: {TARGET_URL}")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {LLM_MODEL}")
    print(f"ğŸ”¥ ä½¿ç”¨æŠ€æœ¯: Crawl4aiè‡ªåŠ¨åˆ†å— + LLMæ™ºèƒ½æå–")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent.parent.parent / "data" / OUTPUT_DIR
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        # çˆ¬å–æ•°æ®
        print(f"\n{'='*80}")
        print("ğŸš€ å¼€å§‹çˆ¬å–...")
        print(f"{'='*80}")
        
        courses = await crawl_provincial_course_data(TARGET_URL)
        
        if courses:
            print(f"\n{'='*80}")
            print("ğŸ“Š çˆ¬å–æˆåŠŸï¼Œå¼€å§‹ä¿å­˜æ•°æ®...")
            print(f"{'='*80}")
            
            # ä¿å­˜ä¸»æ•°æ®æ–‡ä»¶
            output_file = output_dir / OUTPUT_FILE
            save_provincial_course_data(courses, str(output_file), TARGET_URL)
            
            # ç”Ÿæˆå¹¶ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats = generate_statistics(courses)
            stats_file = output_dir / "statistics.json"
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                import json
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print(f"\n{'='*80}")
            print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
            print(f"{'='*80}")
            print(f"ğŸ« å­¦æ ¡æ€»æ•°: {stats['overview']['total_schools']}")
            print(f"ğŸ“š ç¬¬ä¸€æ‰¹è¯¾ç¨‹æ€»æ•°: {stats['overview']['total_first_batch']}")
            print(f"ğŸ“š ç¬¬äºŒæ‰¹è¯¾ç¨‹æ€»æ•°: {stats['overview']['total_second_batch']}")
            print(f"ğŸ“š ç¬¬ä¸‰æ‰¹è¯¾ç¨‹æ€»æ•°: {stats['overview']['total_third_batch']}")
            print(f"ğŸ“š è¯¾ç¨‹æ€»æ•°: {stats['overview']['total_all_batches']}")
            print(f"ğŸ“Š å¹³å‡æ¯æ ¡è¯¾ç¨‹æ•°: {stats['overview']['average_per_school']}")
            
            print(f"\nğŸ† è¯¾ç¨‹æ•°é‡å‰10çš„å­¦æ ¡:")
            for school in stats['top_10_schools'][:10]:
                print(f"  {school['rank']}. {school['school']}: {school['total']}é—¨")
            
            print(f"\nğŸ“ æ–‡ä»¶å·²ä¿å­˜:")
            print(f"  ğŸ“„ {OUTPUT_FILE} - å®Œæ•´æ•°æ® ({len(courses)} æ‰€å­¦æ ¡)")
            print(f"  ğŸ“„ statistics.json - ç»Ÿè®¡ä¿¡æ¯")
            
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. éªŒè¯ç›®æ ‡URLæ˜¯å¦æ­£ç¡®")
            print("   3. æ£€æŸ¥LLM APIé…ç½®")
            print("   4. å¢åŠ é¡µé¢ç­‰å¾…æ—¶é—´")
            
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())