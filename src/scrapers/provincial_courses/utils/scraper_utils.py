import asyncio
import os
import json
import re
import time
from typing import List

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

from models.course_data import ProvincialCourseData
from config import *


def get_browser_config() -> BrowserConfig:
    """
    è·å–æµè§ˆå™¨é…ç½®
    """
    print("ğŸ”§ é…ç½®æµè§ˆå™¨å‚æ•°...")
    config = BrowserConfig(
        browser_type="chromium",
        headless=False,
        verbose=True,
    )
    print("âœ… æµè§ˆå™¨é…ç½®å®Œæˆ")
    return config


def create_provincial_course_llm_strategy() -> LLMExtractionStrategy:
    """
    åˆ›å»ºçœçº§ä¸€æµè¯¾ç¨‹æ•°æ®æå–çš„LLMç­–ç•¥
    """
    print("ğŸ¤– åˆ›å»ºçœçº§ä¸€æµè¯¾ç¨‹æ•°æ®æå–ç­–ç•¥...")
    
    extraction_instruction = """ä½ æ˜¯ä¸“ä¸šçš„æ•™è‚²æ•°æ®æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹ç½‘é¡µå†…å®¹ä¸­æå–å„ä¸ªå­¦æ ¡çš„çœçº§ä¸€æµè¯¾ç¨‹æ•°æ®ã€‚

**ä¸¥æ ¼æå–è¦æ±‚ï¼š**
1. ä»”ç»†åˆ†ææ•´ä¸ªå†…å®¹ï¼Œæå–æ¯ä¸ªå­¦æ ¡çš„çœçº§ä¸€æµè¯¾ç¨‹ç»Ÿè®¡æ•°æ®
2. å­¦æ ¡åç§°ï¼šå®Œæ•´çš„å¤§å­¦åç§°ï¼ˆå¦‚"åŒ—äº¬å¤§å­¦"ã€"æ¸…åå¤§å­¦"ç­‰ï¼‰
3. ç¬¬ä¸€æ‰¹æ•°é‡ï¼šè¯¥å­¦æ ¡çœçº§ä¸€æµè¯¾ç¨‹ç¬¬ä¸€æ‰¹çš„æ•°é‡ï¼ˆæ•´æ•°ï¼‰
4. ç¬¬äºŒæ‰¹æ•°é‡ï¼šè¯¥å­¦æ ¡çœçº§ä¸€æµè¯¾ç¨‹ç¬¬äºŒæ‰¹çš„æ•°é‡ï¼ˆæ•´æ•°ï¼‰
5. ç¬¬ä¸‰æ‰¹æ•°é‡ï¼šè¯¥å­¦æ ¡çœçº§ä¸€æµè¯¾ç¨‹ç¬¬ä¸‰æ‰¹çš„æ•°é‡ï¼ˆæ•´æ•°ï¼‰
6. åˆè®¡æ•°é‡ï¼šè¯¥å­¦æ ¡çœçº§ä¸€æµè¯¾ç¨‹çš„æ€»æ•°é‡ï¼ˆæ•´æ•°ï¼‰
7. ç¡®ä¿ä¸é—æ¼ä»»ä½•å­¦æ ¡çš„æ•°æ®ï¼Œä½†é¿å…é‡å¤æå–
8. å¿½ç•¥å¯¼èˆªèœå•ã€å¹¿å‘Šã€é¡µè„šç­‰æ— å…³å†…å®¹

**è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONæ•°ç»„ï¼‰ï¼š**
[
  {"school": "å­¦æ ¡åç§°", "first": ç¬¬ä¸€æ‰¹æ•°é‡, "second": ç¬¬äºŒæ‰¹æ•°é‡, "third": ç¬¬ä¸‰æ‰¹æ•°é‡, "total": åˆè®¡æ•°é‡},
  {"school": "å­¦æ ¡åç§°", "first": ç¬¬ä¸€æ‰¹æ•°é‡, "second": ç¬¬äºŒæ‰¹æ•°é‡, "third": ç¬¬ä¸‰æ‰¹æ•°é‡, "total": åˆè®¡æ•°é‡}
]

**é‡è¦è¯´æ˜ï¼š**
- æ•°å­—å­—æ®µ(first, second, third, total)å¿…é¡»æ˜¯æ•´æ•°ç±»å‹ï¼Œä¸èƒ½æ˜¯å­—ç¬¦ä¸²
- å¦‚æœæŸæ‰¹æ¬¡æ•°é‡ä¸º0ï¼Œè¯·å¡«å†™0è€Œä¸æ˜¯çœç•¥
- ç¡®ä¿totalç­‰äºfirst + second + thirdçš„å’Œ
- å¦‚æœç¡®å®æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ•°æ®ï¼Œè¿”å›ç©ºæ•°ç»„ []
- ä¸è¦çŒœæµ‹æˆ–æ¨æ–­ä¿¡æ¯ï¼Œåªæå–æ˜ç¡®æ˜¾ç¤ºçš„å†…å®¹

è¯·å¼€å§‹è¯¦ç»†æå–ï¼š"""
    
    llm_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(
            provider=LLM_MODEL,
            api_token=os.getenv(API_KEY_ENV, os.getenv("DEEPSEEK_API_KEY")),
            base_url=LLM_BASE_URL,
        ),
        schema='[{"school": "str", "first": "int", "second": "int", "third": "int", "total": "int"}]',
        extraction_type="schema",
        instruction=extraction_instruction,
        
        # å¯ç”¨Crawl4aiè‡ªåŠ¨åˆ†å—
        apply_chunking=True,
        chunk_token_threshold=6000,
        overlap_rate=0.15,
        
        input_format="markdown",
        extra_args={
            "temperature": 0.0,
            "max_tokens": 8000,
            "top_p": 0.1,
        }
    )
    
    print("âœ… çœçº§ä¸€æµè¯¾ç¨‹LLMç­–ç•¥åˆ›å»ºå®Œæˆ")
    return llm_strategy


def validate_course_data(course_data: dict) -> bool:
    """
    éªŒè¯æå–çš„çœçº§è¯¾ç¨‹æ•°æ®è´¨é‡
    """
    school_name = course_data.get('school', '').strip()
    
    # åŸºæœ¬æ£€æŸ¥
    if len(school_name) < 2:
        return False
    
    # æ£€æŸ¥æ•°å­—å­—æ®µ
    required_fields = ['first', 'second', 'third', 'total']
    for field in required_fields:
        value = course_data.get(field)
        if not isinstance(value, int) or value < 0:
            print(f"âš ï¸ å­—æ®µ {field} éªŒè¯å¤±è´¥: {value}")
            return False
    
    # æ£€æŸ¥æ€»æ•°æ˜¯å¦åˆç†ï¼ˆå…è®¸ä¸€å®šè¯¯å·®ï¼‰
    calculated_total = course_data['first'] + course_data['second'] + course_data['third']
    actual_total = course_data['total']
    
    if abs(calculated_total - actual_total) > 5:  # å…è®¸5ä¸ªè¯¾ç¨‹çš„è¯¯å·®
        print(f"âš ï¸ {school_name} æ€»æ•°éªŒè¯å¤±è´¥: è®¡ç®—å€¼({calculated_total}) vs å®é™…å€¼({actual_total})")
        return False
    
    # è¿‡æ»¤æ˜æ˜¾é”™è¯¯çš„å­¦æ ¡åç§°
    invalid_patterns = [
        r'^(è¯¦æƒ…|æ›´å¤š|æŸ¥çœ‹|ç‚¹å‡»|é“¾æ¥|æŒ‰é’®|ç™»å½•|æ³¨å†Œ|é¦–é¡µ|å¯¼èˆª|èœå•|åˆè®¡|æ€»è®¡|å°è®¡).*',
        r'.*\.(com|cn|org|net).*',
        r'^\d+$',  # çº¯æ•°å­—
    ]
    
    for pattern in invalid_patterns:
        if re.match(pattern, school_name, re.IGNORECASE):
            return False
    
    return True


async def crawl_provincial_course_data(target_url: str) -> List[ProvincialCourseData]:
    """
    çˆ¬å–çœçº§ä¸€æµè¯¾ç¨‹æ•°æ®
    """
    print(f"\n{'='*80}")
    print(f"ğŸ« å¼€å§‹çˆ¬å–çœçº§ä¸€æµè¯¾ç¨‹æ•°æ®")
    print(f"ğŸ¯ ç›®æ ‡URL: {target_url}")
    print(f"ğŸ¤– ç­–ç•¥ï¼šç®€å•é¡µé¢åŠ è½½ + Crawl4aiè‡ªåŠ¨åˆ†å— + LLMåˆ†æ")
    print(f"{'='*80}")
    
    # åˆ›å»ºsession ID
    unique_timestamp = int(time.time() * 1000)
    session_id = f"provincial_courses_{unique_timestamp}"
    print(f"ğŸ†” Session ID: {session_id}")
    
    async with AsyncWebCrawler(config=get_browser_config()) as crawler:
        try:
            print("ğŸš€ åŠ è½½é¡µé¢...")
            
            # ç®€å•çš„ç­‰å¾…JSä»£ç 
            simple_wait_js = """
            (async function() {
                console.log('ğŸ”„ å¼€å§‹ç­‰å¾…é¡µé¢å†…å®¹åŠ è½½...');
                console.log('ğŸ“ å½“å‰URL:', window.location.href);
                console.log('ğŸ“„ é¡µé¢æ ‡é¢˜:', document.title);
                
                // ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                await new Promise(resolve => setTimeout(resolve, 8000));
                
                const divCount = document.querySelectorAll('div').length;
                console.log(`ğŸ“Š é¡µé¢åŒ…å« ${divCount} ä¸ªdivå…ƒç´ `);
                
                // æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨ç¡®ä¿æ‰€æœ‰å†…å®¹å¯è§
                window.scrollTo(0, document.body.scrollHeight);
                await new Promise(resolve => setTimeout(resolve, 3000));
                
                console.log('âœ… é¡µé¢å†…å®¹å‡†å¤‡å®Œæˆ');
                
                return {
                    success: true,
                    elements: divCount,
                    message: 'çœçº§è¯¾ç¨‹æ•°æ®é¡µé¢å·²å‡†å¤‡å°±ç»ª'
                };
            })();
            """
            
            # åˆ›å»ºLLMç­–ç•¥
            print("ğŸ¤– åˆ›å»ºLLMæå–ç­–ç•¥...")
            llm_strategy = create_provincial_course_llm_strategy()
            
            # æ‰§è¡Œçˆ¬å–
            print("âš¡ å¼€å§‹LLMåˆ†æ...")
            
            result = await crawler.arun(
                url=target_url,
                config=CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    session_id=session_id,
                    js_code=simple_wait_js,
                    extraction_strategy=llm_strategy,
                    delay_before_return_html=15.0,  # ç®€åŒ–ç­‰å¾…æ—¶é—´
                    wait_for_images=False,
                    page_timeout=60000,  # 1åˆ†é’Ÿè¶…æ—¶
                    override_navigator=True,
                ),
            )
            
            if not result.success:
                print(f"âŒ é¡µé¢åŠ è½½å¤±è´¥: {result.error_message}")
                return []
            
            print(f"âœ… é¡µé¢åŠ è½½æˆåŠŸï¼ŒHTMLé•¿åº¦: {len(result.cleaned_html):,} å­—ç¬¦")
            
            # å¤„ç†LLMæå–ç»“æœ
            print("ğŸ“Š å¤„ç†æå–ç»“æœ...")
            
            if result.extracted_content:
                try:
                    print(f"ğŸ” LLMæå–ç»“æœé•¿åº¦: {len(result.extracted_content):,} å­—ç¬¦")
                    print(f"ğŸ” LLMæå–å†…å®¹é¢„è§ˆ: {result.extracted_content[:200]}...")
                    
                    # è§£æLLMæå–çš„ç»“æœ
                    extracted_data = json.loads(result.extracted_content)
                    
                    # ç¡®ä¿ç»“æœæ˜¯åˆ—è¡¨æ ¼å¼
                    if isinstance(extracted_data, dict):
                        if 'data' in extracted_data:
                            course_list = extracted_data['data']
                        else:
                            for value in extracted_data.values():
                                if isinstance(value, list):
                                    course_list = value
                                    break
                            else:
                                course_list = []
                    elif isinstance(extracted_data, list):
                        course_list = extracted_data
                    else:
                        print(f"âš ï¸ æ„å¤–çš„LLMè¿”å›æ ¼å¼: {type(extracted_data)}")
                        course_list = []
                    
                    print(f"ğŸ“‹ è§£æå¾—åˆ° {len(course_list)} ä¸ªå­¦æ ¡æ•°æ®æ¡ç›®")
                    
                    # å¤„ç†æå–çš„è¯¾ç¨‹æ•°æ®
                    all_courses = []
                    for i, course_data in enumerate(course_list):
                        if isinstance(course_data, dict):
                            school_name = course_data.get('school', '').strip()
                            first = course_data.get('first', 0)
                            second = course_data.get('second', 0)
                            third = course_data.get('third', 0)
                            total = course_data.get('total', 0)
                            
                            # éªŒè¯æ•°æ®è´¨é‡
                            if school_name:
                                if validate_course_data(course_data):
                                    course = ProvincialCourseData(
                                        school=school_name,
                                        first=first,
                                        second=second,
                                        third=third,
                                        total=total
                                    )
                                    all_courses.append(course)
                                    print(f"  âœ… æå– {i+1}: {school_name} - ç¬¬ä¸€æ‰¹({first}) ç¬¬äºŒæ‰¹({second}) ç¬¬ä¸‰æ‰¹({third}) åˆè®¡({total})")
                                else:
                                    print(f"  âŒ éªŒè¯å¤±è´¥ {i+1}: {school_name}")
                            else:
                                print(f"  âš ï¸ æ•°æ®ä¸å®Œæ•´ {i+1}: {course_data}")
                    
                    # å»é‡å¤„ç†
                    unique_courses = []
                    seen_schools = set()
                    
                    for course in all_courses:
                        school_key = course.school.strip().lower()
                        if school_key not in seen_schools:
                            seen_schools.add(school_key)
                            unique_courses.append(course)
                        else:
                            print(f"ğŸ”„ å»é‡: {course.school}")
                    
                    print(f"ğŸ‰ çœçº§è¯¾ç¨‹æ•°æ®æå–å®Œæˆ!")
                    print(f"  ğŸ« æå–å­¦æ ¡æ€»æ•°: {len(all_courses)}")
                    print(f"  âœ¨ å»é‡åå­¦æ ¡æ•°: {len(unique_courses)}")
                    
                    # æ˜¾ç¤ºæ ·ä¾‹
                    if unique_courses:
                        print("ğŸ« æå–çš„å­¦æ ¡æ•°æ®æ ·ä¾‹:")
                        for i, course in enumerate(unique_courses[:10], 1):
                            print(f"  {i}. {course}")
                        if len(unique_courses) > 10:
                            print(f"  ... è¿˜æœ‰ {len(unique_courses) - 10} æ‰€å­¦æ ¡")
                    
                    return unique_courses
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                    print(f"åŸå§‹æå–å†…å®¹: {result.extracted_content[:1000]}...")
                    return []
                    
            else:
                print("âŒ LLMæœªæå–åˆ°ä»»ä½•å†…å®¹")
                return []
            
        except Exception as e:
            print(f"âŒ çœçº§è¯¾ç¨‹æ•°æ®çˆ¬å–å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return []