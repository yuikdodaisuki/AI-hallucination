"""æ™ºæ…§æ•™è‚²å¹³å°è¯¾ç¨‹çˆ¬è™«ä¸»ç¨‹åº"""
import asyncio
import os
import json
from pathlib import Path
from dotenv import load_dotenv

from crawl4ai import AsyncWebCrawler

from config import *
from models.course import Course, SchoolCourseSummary
from utils.data_utils import (
    read_school_list_from_csv,
    save_courses_to_json,
    save_school_summary_to_json,
    generate_statistics
)
from utils.scraper_utils import get_browser_config, crawl_school_courses

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


async def main():
    """
    ä¸»ç¨‹åºï¼šçˆ¬å–å›½å®¶é«˜ç­‰æ•™è‚²æ™ºæ…§æ•™è‚²å¹³å°è¯¾ç¨‹æ•°æ® - ä½¿ç”¨è‡ªåŠ¨åˆ†å—
    """
    print("ğŸ¯ å¼€å§‹çˆ¬å–å›½å®¶é«˜ç­‰æ•™è‚²æ™ºæ…§æ•™è‚²å¹³å°è¯¾ç¨‹æ•°æ®...")
    print(f"ğŸ“ åŸºç¡€URL: {BASE_URL}")
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: {'å¼€å¯' if TEST_MODE else 'å…³é—­'}")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {LLM_MODEL}")
    print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {LLM_BATCH_SIZE}")
    print(f"ğŸ”¥ æ–°ç‰¹æ€§: ä½¿ç”¨Crawl4aiè‡ªåŠ¨åˆ†å—æŠ€æœ¯")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent.parent.parent / "data" / OUTPUT_DIR
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # è¯»å–å­¦æ ¡åˆ—è¡¨
    school_list_file = Path(__file__).parent.parent.parent.parent / SCHOOL_LIST_FILE
    if not school_list_file.exists():
        print(f"âŒ å­¦æ ¡åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {school_list_file}")
        return
    
    schools = read_school_list_from_csv(str(school_list_file))
    if not schools:
        print("âŒ æœªè¯»å–åˆ°å­¦æ ¡æ•°æ®")
        return
    
    # æµ‹è¯•æ¨¡å¼é™åˆ¶å­¦æ ¡æ•°é‡
    if TEST_MODE and len(schools) > MAX_SCHOOLS:
        schools = schools[:MAX_SCHOOLS]
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰ {MAX_SCHOOLS} æ‰€å­¦æ ¡")
    
    print(f"ğŸ“‹ å…±éœ€çˆ¬å– {len(schools)} æ‰€å­¦æ ¡")
    
    # ğŸ”¥ ä½¿ç”¨è‡ªåŠ¨åˆ†å—çˆ¬å–ç­–ç•¥
    print(f"ğŸ¤– ä½¿ç”¨ç­–ç•¥: Crawl4aiè‡ªåŠ¨åˆ†å— + LLMæ™ºèƒ½æå–")
    print(f"ğŸ“¦ åˆ†å—å¤§å°: 6000 tokens")
    print(f"ğŸ”„ é‡å ç‡: 15%")
    
    # åˆå§‹åŒ–æµè§ˆå™¨é…ç½®
    browser_config = get_browser_config()
    session_id = "smartedu_auto_chunking_crawl"
    
    # å­˜å‚¨æ‰€æœ‰æ•°æ®
    all_courses = []
    school_summaries = []
    successful_schools = 0
    failed_schools = []
    
    try:
        # ğŸ”¥ ä¸å†åˆ›å»ºå•ç‹¬çš„crawlerï¼Œç›´æ¥è°ƒç”¨crawl_school_courses
        # å› ä¸ºæ–°çš„å‡½æ•°å†…éƒ¨å·²ç»ç®¡ç†äº†crawlerçš„ç”Ÿå‘½å‘¨æœŸ
        
        for index, school_name in enumerate(schools):
            print(f"\nğŸ”„ è¿›åº¦: {index + 1}/{len(schools)} - {school_name}")
            
            try:
                # ğŸ”¥ ä½¿ç”¨æ–°çš„è‡ªåŠ¨åˆ†å—çˆ¬å–å‡½æ•°
                # æ¯ä¸ªå­¦æ ¡ä½¿ç”¨ç‹¬ç«‹çš„session
                school_session_id = f"{session_id}_{index}_{school_name.replace(' ', '_')}"
                
                school_courses = await crawl_school_courses(
                    school_name, school_session_id
                )
                
                if school_courses:
                    # æ·»åŠ åˆ°æ€»è¯¾ç¨‹åˆ—è¡¨
                    all_courses.extend(school_courses)
                    
                    # åˆ›å»ºå­¦æ ¡æ±‡æ€»
                    school_summary = SchoolCourseSummary(
                        school=school_name,
                        total_courses=len(school_courses),
                        courses=school_courses
                    )
                    school_summaries.append(school_summary)
                    
                    successful_schools += 1
                    print(f"âœ… {school_name}: {len(school_courses)} é—¨è¯¾ç¨‹")
                    
                    # æ˜¾ç¤ºè¯¾ç¨‹æ ·ä¾‹
                    if len(school_courses) > 0:
                        print(f"   ğŸ“š æ ·ä¾‹è¯¾ç¨‹:")
                        for i, course in enumerate(school_courses[:3], 1):  # æ˜¾ç¤ºå‰3ä¸ª
                            print(f"     {i}. {course.course_name} - {course.teacher}")
                        if len(school_courses) > 3:
                            print(f"     ... è¿˜æœ‰ {len(school_courses) - 3} é—¨è¯¾ç¨‹")
                else:
                    failed_schools.append(school_name)
                    print(f"âŒ {school_name}: æœªè·å–åˆ°è¯¾ç¨‹æ•°æ®")
                
                # ğŸ”¥ è‡ªåŠ¨åˆ†å—ç­–ç•¥ä¸‹çš„è¯·æ±‚é—´éš”
                # ç”±äºè‡ªåŠ¨åˆ†å—å¯èƒ½ä¼šå¢åŠ å¤„ç†æ—¶é—´ï¼Œé€‚å½“è°ƒæ•´é—´éš”
                if index < len(schools) - 1:
                    interval = max(REQUEST_DELAY, 5)  # æœ€å°‘5ç§’é—´éš”
                    print(f"â±ï¸ ç­‰å¾… {interval} ç§’...")
                    await asyncio.sleep(interval)
                    
            except Exception as e:
                failed_schools.append(school_name)
                print(f"âŒ çˆ¬å– {school_name} å¼‚å¸¸: {e}")
                # ğŸ”¥ å¼‚å¸¸æ—¶ä¹Ÿè¦ç­‰å¾…ï¼Œé¿å…é¢‘ç¹é‡è¯•
                if index < len(schools) - 1:
                    await asyncio.sleep(3)
                continue
        
        # ä¿å­˜æ•°æ®
        if all_courses:
            print(f"\n{'='*100}")
            print(f"ğŸ“Š å¼€å§‹ä¿å­˜æ•°æ®...")
            print(f"{'='*100}")
            
            # ä¿å­˜æ‰€æœ‰è¯¾ç¨‹æ•°æ®
            all_courses_file = output_dir / OUTPUT_FILES["all_courses"]
            save_courses_to_json(all_courses, str(all_courses_file))
            
            # ä¿å­˜å­¦æ ¡æ±‡æ€»æ•°æ®
            school_summary_file = output_dir / OUTPUT_FILES["school_summary"]
            save_school_summary_to_json(school_summaries, str(school_summary_file))
            
            # ç”Ÿæˆå¹¶ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats = generate_statistics(school_summaries)
            stats_file = output_dir / OUTPUT_FILES["statistics"]
            
            # ğŸ”¥ å¢åŠ è‡ªåŠ¨åˆ†å—ç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯
            enhanced_stats = {
                **stats,
                "extraction_method": "Crawl4aiè‡ªåŠ¨åˆ†å— + LLMæ™ºèƒ½æå–",
                "chunk_size": "6000 tokens",
                "overlap_rate": "15%",
                "total_extraction_time": "å®é™…è¿è¡Œæ—¶é—´",
                "failed_schools_list": failed_schools,
                "success_rate": f"{successful_schools}/{len(schools)} ({successful_schools/len(schools)*100:.1f}%)"
            }
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_stats, f, ensure_ascii=False, indent=2)
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print(f"\n{'='*100}")
            print(f"ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡ - è‡ªåŠ¨åˆ†å—ç‰ˆæœ¬")
            print(f"{'='*100}")
            print(f"ğŸ¯ çˆ¬å–ç­–ç•¥: Crawl4aiè‡ªåŠ¨åˆ†å— + LLMæ™ºèƒ½æå–")
            print(f"âœ… æˆåŠŸçˆ¬å–å­¦æ ¡: {successful_schools}/{len(schools)} ({successful_schools/len(schools)*100:.1f}%)")
            print(f"ğŸ“š æ€»è¯¾ç¨‹æ•°é‡: {len(all_courses)}")
            print(f"ğŸ“Š å¹³å‡æ¯æ ¡è¯¾ç¨‹æ•°: {enhanced_stats.get('average_courses_per_school', 0)}")
            
            # ğŸ”¥ æ•°æ®è´¨é‡åˆ†æ
            if len(all_courses) > 0:
                # åˆ†ææ•™å¸ˆåç§°é•¿åº¦åˆ†å¸ƒ
                teacher_lengths = [len(course.teacher) for course in all_courses]
                avg_teacher_length = sum(teacher_lengths) / len(teacher_lengths)
                
                # åˆ†æè¯¾ç¨‹åç§°é•¿åº¦åˆ†å¸ƒ
                course_lengths = [len(course.course_name) for course in all_courses]
                avg_course_length = sum(course_lengths) / len(course_lengths)
                
                print(f"\nğŸ” æ•°æ®è´¨é‡åˆ†æ:")
                print(f"   æ•™å¸ˆåç§°å¹³å‡é•¿åº¦: {avg_teacher_length:.1f} å­—ç¬¦")
                print(f"   è¯¾ç¨‹åç§°å¹³å‡é•¿åº¦: {avg_course_length:.1f} å­—ç¬¦")
                
                # æ£€æŸ¥å¯èƒ½çš„æ•°æ®è´¨é‡é—®é¢˜
                short_teachers = [c for c in all_courses if len(c.teacher) < 2]
                long_teachers = [c for c in all_courses if len(c.teacher) > 10]
                
                if short_teachers:
                    print(f"   âš ï¸ ç–‘ä¼¼æ— æ•ˆæ•™å¸ˆåç§°: {len(short_teachers)} ä¸ª")
                if long_teachers:
                    print(f"   âš ï¸ ç–‘ä¼¼è¿‡é•¿æ•™å¸ˆåç§°: {len(long_teachers)} ä¸ª")
                
                print(f"   âœ… æ•°æ®è´¨é‡: {(len(all_courses)-len(short_teachers)-len(long_teachers))/len(all_courses)*100:.1f}%")
            
            if enhanced_stats.get('top_5_schools'):
                print(f"\nğŸ† è¯¾ç¨‹æ•°é‡å‰5åå­¦æ ¡:")
                for i, school_info in enumerate(enhanced_stats['top_5_schools'], 1):
                    print(f"   {i}. {school_info['name']}: {school_info['course_count']} é—¨")
            
            if failed_schools:
                print(f"\nâŒ å¤±è´¥å­¦æ ¡ ({len(failed_schools)}):")
                for i, school in enumerate(failed_schools, 1):
                    print(f"   {i}. {school}")
                
                # ğŸ”¥ å¤±è´¥åŸå› åˆ†ææç¤º
                print(f"\nğŸ’¡ å¤±è´¥å¯èƒ½åŸå› :")
                print(f"   1. å­¦æ ¡åç§°åœ¨ç½‘ç«™ä¸­ä¸å­˜åœ¨")
                print(f"   2. ç½‘ç»œè¿æ¥é—®é¢˜æˆ–é¡µé¢åŠ è½½è¶…æ—¶")
                print(f"   3. ç½‘ç«™ç»“æ„å‘ç”Ÿå˜åŒ–")
                print(f"   4. LLMæå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
                print(f"   5. è‡ªåŠ¨åˆ†å—é…ç½®éœ€è¦è°ƒæ•´")
            
            print(f"\nğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")
            print(f"   ğŸ“„ {OUTPUT_FILES['all_courses']} - æ‰€æœ‰è¯¾ç¨‹è¯¦ç»†æ•°æ® ({len(all_courses)} æ¡)")
            print(f"   ğŸ“„ {OUTPUT_FILES['school_summary']} - å­¦æ ¡è¯¾ç¨‹æ±‡æ€» ({len(school_summaries)} æ‰€)")
            print(f"   ğŸ“„ {OUTPUT_FILES['statistics']} - ç»Ÿè®¡ä¿¡æ¯ (å«è‡ªåŠ¨åˆ†å—æ•°æ®)")
            
            # ğŸ”¥ ä¿å­˜æˆåŠŸå­¦æ ¡åˆ—è¡¨ï¼ˆç”¨äºåç»­åˆ†æï¼‰
            if successful_schools > 0:
                successful_schools_list = [summary.school for summary in school_summaries]
                success_file = output_dir / "successful_schools.json"
                with open(success_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "successful_schools": successful_schools_list,
                        "total_count": len(successful_schools_list),
                        "extraction_method": "Crawl4aiè‡ªåŠ¨åˆ†å—"
                    }, f, ensure_ascii=False, indent=2)
                print(f"   ğŸ“„ successful_schools.json - æˆåŠŸå­¦æ ¡åˆ—è¡¨ ({len(successful_schools_list)} æ‰€)")
            
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•è¯¾ç¨‹æ•°æ®")
            print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. éªŒè¯å­¦æ ¡åˆ—è¡¨æ–‡ä»¶ä¸­çš„å­¦æ ¡åç§°")
            print("   3. è°ƒæ•´LLMé…ç½®å‚æ•°")
            print("   4. å¢åŠ é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´")
            print("   5. æ£€æŸ¥è‡ªåŠ¨åˆ†å—é…ç½®æ˜¯å¦åˆé€‚")
            
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        print(f"ğŸ“Š å·²å¤„ç†: {len(school_summaries)} æ‰€å­¦æ ¡")
        if all_courses:
            print(f"ğŸ“š å·²è·å–: {len(all_courses)} é—¨è¯¾ç¨‹")
            # å³ä½¿è¢«ä¸­æ–­ï¼Œä¹Ÿä¿å­˜å·²è·å–çš„æ•°æ®
            emergency_file = output_dir / f"emergency_save_{len(all_courses)}_courses.json"
            save_courses_to_json(all_courses, str(emergency_file))
            print(f"ğŸ’¾ åº”æ€¥ä¿å­˜: {emergency_file}")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        print(f"ğŸ“Š å·²å¤„ç†: {len(school_summaries)} æ‰€å­¦æ ¡")
        if all_courses:
            print(f"ğŸ“š å·²è·å–: {len(all_courses)} é—¨è¯¾ç¨‹")
            # å¼‚å¸¸æ—¶ä¹Ÿä¿å­˜å·²è·å–çš„æ•°æ®
            emergency_file = output_dir / f"error_save_{len(all_courses)}_courses.json"
            save_courses_to_json(all_courses, str(emergency_file))
            print(f"ğŸ’¾ å¼‚å¸¸ä¿å­˜: {emergency_file}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())