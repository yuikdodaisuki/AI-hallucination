"""ä¿®æ”¹main.pyæ”¯æŒå¤šå¹´ä»½çˆ¬å–"""
# filepath: c:\Users\83789\PycharmProjects\scrapetest\src\scrapers\deepseek-ruanke-scraper\main.py

import asyncio
import os
from pathlib import Path
from dotenv import load_dotenv

from config import BASE_URL, SUBJECT_LINK_SELECTOR, CSS_SELECTOR, REQUIRED_KEYS, YEARS_TO_CRAWL
from utils.data_utils import save_venues_to_csv
from utils.subject_crawler import crawl_all_subject_rankings
from utils.data_utils import save_venues_to_csv, save_venues_to_json, save_venues_to_both_formats

load_dotenv()


async def crawl_single_year(year: int, output_base_dir: Path) -> dict:
    """
    çˆ¬å–å•ä¸ªå¹´ä»½çš„æ•°æ®
    
    Args:
        year: è¦çˆ¬å–çš„å¹´ä»½
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        
    Returns:
        dict: åŒ…å«çˆ¬å–ç»“æœçš„å­—å…¸
    """
    print(f"\n{'='*100}")
    print(f"ğŸ—“ï¸  å¼€å§‹çˆ¬å– {year} å¹´æ•°æ®")
    print(f"{'='*100}")
    
    # ğŸ”¥ æ„å»ºè¯¥å¹´ä»½çš„å®Œæ•´URL ğŸ”¥
    year_url = f"{BASE_URL}/{year}"
    
    # ğŸ”¥ ä¸ºè¯¥å¹´ä»½åˆ›å»ºä¸“é—¨çš„è¾“å‡ºç›®å½• ğŸ”¥
    year_output_dir = output_base_dir / str(year)
    year_output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ ç´¢å¼•é¡µé¢: {year_url}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {year_output_dir}")
    print(f"ğŸ” å­¦ç§‘é“¾æ¥é€‰æ‹©å™¨: {SUBJECT_LINK_SELECTOR}")
    print(f"ğŸ“Š æ’åè¡¨æ ¼é€‰æ‹©å™¨: {CSS_SELECTOR}")
    
    try:
        # ğŸ”¥ æ‰§è¡Œè¯¥å¹´ä»½çš„åŒå±‚çˆ¬å– ğŸ”¥
        all_data = await crawl_all_subject_rankings(
            index_url=year_url,
            subject_link_selector=SUBJECT_LINK_SELECTOR,
            ranking_css_selector=CSS_SELECTOR,
            required_keys=REQUIRED_KEYS
        )
        
        if all_data:
            # ğŸ”¥ ä¸ºæ¯æ¡æ•°æ®æ·»åŠ å¹´ä»½ä¿¡æ¯ ğŸ”¥
            for item in all_data:
                item["year"] = year
            
            # æŒ‰å­¦ç§‘åˆ†ç±»ç»Ÿè®¡
            subjects = {}
            for item in all_data:
                subject = item.get("subject", "unknown")
                if subject not in subjects:
                    subjects[subject] = []
                subjects[subject].append(item)
            
            # ğŸ”¥ ä¿å­˜è¯¥å¹´ä»½çš„æ€»æ•°æ®ï¼Œæ–‡ä»¶ååŒ…å«å¹´ä»½ ğŸ”¥
            output_file_base = year_output_dir / f"all_subject_layers_{year}"
            save_venues_to_both_formats(all_data, str(output_file_base))
            output_file = output_file_base.with_suffix('.json')  # æ›´æ–°è¿”å›çš„æ–‡ä»¶è·¯å¾„
            print(f"\nğŸ’¾ ä¿å­˜ {year} å¹´æ•°æ®: {len(all_data)} æ¡åˆ° {output_file}")
            
            # ğŸ”¥ åˆ†å­¦ç§‘ä¿å­˜ï¼ˆå¯é€‰ï¼ŒåŒ…å«å¹´ä»½ï¼‰ ğŸ”¥
            # print(f"\nğŸ“‚ æŒ‰å­¦ç§‘åˆ†ç±»ä¿å­˜ {year} å¹´æ•°æ®:")
            # for subject_name, subject_data in subjects.items():
            #     safe_filename = subject_name.replace('/', '_').replace('\\', '_').replace(':', '_')
            #     filename = f"ranking_{safe_filename}_{year}.csv"
            #     subject_file = year_output_dir / filename
            #     save_venues_to_csv(subject_data, str(subject_file))
            #     print(f"   ğŸ“„ {subject_name}: {len(subject_data)} æ¡ -> {filename}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ‰ {year} å¹´çˆ¬å–å®Œæˆ!")
            print(f"ğŸ“Š {year} å¹´æ€»è®¡: {len(all_data)} æ¡æ’åæ•°æ®")
            print(f"ğŸ« {year} å¹´æ¶µç›–å­¦ç§‘: {len(subjects)} ä¸ª")
            
            return {
                "year": year,
                "success": True,
                "data_count": len(all_data),
                "subjects_count": len(subjects),
                "output_file": str(output_file)
            }
            
        else:
            print(f"âŒ {year} å¹´æœªè·å–åˆ°ä»»ä½•å­¦ç§‘æ’åæ•°æ®")
            return {
                "year": year,
                "success": False,
                "data_count": 0,
                "subjects_count": 0,
                "error": "æ— æ•°æ®"
            }
            
    except Exception as e:
        print(f"âŒ {year} å¹´çˆ¬å–å¼‚å¸¸: {e}")
        return {
            "year": year,
            "success": False,
            "data_count": 0,
            "subjects_count": 0,
            "error": str(e)
        }


async def main():
    """
    ä¸»ç¨‹åº - å¤šå¹´ä»½å­¦ç§‘æ’åçˆ¬å–
    """
    print("ğŸ¯ å¼€å§‹å¤šå¹´ä»½å­¦ç§‘æ’åçˆ¬å–...")
    print(f"ğŸ“ åŸºç¡€URL: {BASE_URL}")
    print(f"ğŸ—“ï¸  è¦çˆ¬å–çš„å¹´ä»½: {', '.join(map(str, YEARS_TO_CRAWL))}")
    print("="*80)

    # ğŸ”¥ è®¾ç½®è¾“å‡ºåŸºç¡€ç›®å½• ğŸ”¥
    output_base_dir = Path(__file__).parent.parent.parent / "data" / "ruanke_subjects"
    output_base_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºåŸºç¡€ç›®å½•: {output_base_dir}")
    
    # ğŸ”¥ å­˜å‚¨æ‰€æœ‰å¹´ä»½çš„çˆ¬å–ç»“æœ ğŸ”¥
    all_years_results = []
    all_years_data = []  # å­˜å‚¨æ‰€æœ‰å¹´ä»½çš„æ•°æ®ï¼Œç”¨äºåˆå¹¶ä¿å­˜
    
    try:
        # ğŸ”¥ é€å¹´çˆ¬å– ğŸ”¥
        for index, year in enumerate(YEARS_TO_CRAWL):
            print(f"\nğŸ”„ å¹´ä»½è¿›åº¦: {index + 1}/{len(YEARS_TO_CRAWL)}")
            
            # çˆ¬å–å•ä¸ªå¹´ä»½
            year_result = await crawl_single_year(year, output_base_dir)
            all_years_results.append(year_result)
            
            # å¦‚æœæˆåŠŸï¼Œè¯»å–æ•°æ®ç”¨äºåˆå¹¶ï¼ˆå¯é€‰ï¼‰
            if year_result["success"]:
                # è¿™é‡Œå¯ä»¥æ·»åŠ è¯»å–è¯¥å¹´ä»½æ•°æ®çš„é€»è¾‘ï¼Œç”¨äºåç»­åˆå¹¶
                pass
            
            # ğŸ”¥ å¹´ä»½é—´å»¶è¿Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚ ğŸ”¥
            if index < len(YEARS_TO_CRAWL) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªå¹´ä»½
                print(f"â±ï¸ ç­‰å¾… 30 ç§’åçˆ¬å–ä¸‹ä¸€å¹´ä»½...")
                await asyncio.sleep(30)
        
        # ğŸ”¥ ç”Ÿæˆæ€»ä½“ç»Ÿè®¡æŠ¥å‘Š ğŸ”¥
        print(f"\n{'='*100}")
        print(f"ğŸ“Š å¤šå¹´ä»½çˆ¬å–å®Œæˆç»Ÿè®¡")
        print(f"{'='*100}")
        
        successful_years = [r for r in all_years_results if r["success"]]
        failed_years = [r for r in all_years_results if not r["success"]]
        
        print(f"âœ… æˆåŠŸçˆ¬å–å¹´ä»½: {len(successful_years)}/{len(YEARS_TO_CRAWL)}")
        for result in successful_years:
            print(f"   ğŸ—“ï¸  {result['year']}: {result['data_count']} æ¡æ•°æ®, {result['subjects_count']} ä¸ªå­¦ç§‘")
        
        if failed_years:
            print(f"\nâŒ å¤±è´¥å¹´ä»½: {len(failed_years)}")
            for result in failed_years:
                print(f"   ğŸ—“ï¸  {result['year']}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        print(f"\nğŸ“ æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {output_base_dir}")
        print(f"ğŸ“‚ ç›®å½•ç»“æ„:")
        for year in YEARS_TO_CRAWL:
            year_dir = output_base_dir / str(year)
            if year_dir.exists():
                print(f"   ğŸ“ {year}/")
                print(f"      ğŸ“„ all_subject_layers_{year}.csv")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")


if __name__ == "__main__":
    asyncio.run(main())