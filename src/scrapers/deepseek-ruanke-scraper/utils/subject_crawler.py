"""æ”¹è¿›çš„å­¦ç§‘çˆ¬è™« - æå–å®Œæ•´ä¸“ä¸šå"""
# filepath: c:\Users\83789\PycharmProjects\scrapetest\deepseek-ai-web-crawler\utils\subject_crawler.py

import asyncio
from typing import List, Dict, Set
from urllib.parse import urljoin

from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from bs4 import BeautifulSoup

from utils.scraper_utils import fetch_and_process_page, get_browser_config, get_llm_strategy
from utils.data_utils import save_venues_to_csv
from config import TEST_MODE, MAX_SUBJECTS, MAX_PAGES_PER_SUBJECT


async def extract_subject_links(
    crawler: AsyncWebCrawler, 
    index_url: str,
    link_selector: str,
    session_id: str
) -> List[Dict[str, str]]:
    """
    ä»å­¦ç§‘ç´¢å¼•é¡µé¢æå–æ‰€æœ‰å­¦ç§‘é“¾æ¥å’Œå®Œæ•´å­¦ç§‘å
    """
    print(f"ğŸ” å¼€å§‹æå–å­¦ç§‘é“¾æ¥ä»: {index_url}")
    
    try:
        result = await crawler.arun(
            url=index_url,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                session_id=session_id,
                delay_before_return_html=5.0,
            ),
        )
        
        if result.success:
            soup = BeautifulSoup(result.html, 'html.parser')
            subject_links = []
            
            # ğŸ”¥ æŸ¥æ‰¾æ‰€æœ‰å­¦ç§‘é“¾æ¥ ğŸ”¥
            links = soup.select(link_selector)
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(links)} ä¸ªæ½œåœ¨å­¦ç§‘é“¾æ¥")
            
            for index, link in enumerate(links):
                href = link.get('href')
                
                # ğŸ”¥ æ”¹è¿›ï¼šæå–å®Œæ•´çš„ä¸“ä¸šåï¼ˆåŒ…å«æ‰€æœ‰spanå†…å®¹ï¼‰ ğŸ”¥
                spans = link.find_all('span')
                
                if spans:
                    # å°†æ‰€æœ‰spançš„æ–‡æœ¬è¿æ¥èµ·æ¥
                    subject_parts = []
                    for span in spans:
                        text = span.get_text(strip=True)
                        if text:
                            subject_parts.append(text)
                    
                    # ç”¨ç©ºæ ¼è¿æ¥æ‰€æœ‰éƒ¨åˆ†ï¼Œå½¢æˆå®Œæ•´ä¸“ä¸šå
                    subject_name = ' '.join(subject_parts)
                else:
                    # å¦‚æœæ²¡æœ‰spanï¼Œä½¿ç”¨æ•´ä¸ªé“¾æ¥çš„æ–‡æœ¬
                    subject_name = link.get_text(strip=True)
                
                if href and subject_name:
                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                    full_url = urljoin(index_url, href)
                    
                    subject_info = {
                        "subject_name": subject_name,
                        "url": full_url
                    }
                    
                    subject_links.append(subject_info)
                    print(f"å­¦ç§‘ {index + 1}: {subject_name} -> {full_url}")
                    
                    # ğŸ”¥ æµ‹è¯•æ¨¡å¼ï¼šåªæå–æŒ‡å®šæ•°é‡çš„é“¾æ¥ ğŸ”¥
                    if TEST_MODE and len(subject_links) >= MAX_SUBJECTS:
                        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå·²æå– {MAX_SUBJECTS} ä¸ªå­¦ç§‘é“¾æ¥ï¼Œåœæ­¢æå–")
                        break
            
            print(f"âœ… æˆåŠŸæå– {len(subject_links)} ä¸ªå­¦ç§‘é“¾æ¥")
            return subject_links
            
        else:
            print(f"âŒ æå–å­¦ç§‘é“¾æ¥å¤±è´¥: {result.error_message}")
            return []
            
    except Exception as e:
        print(f"âŒ æå–å­¦ç§‘é“¾æ¥å¼‚å¸¸: {e}")
        return []


async def crawl_single_subject_ranking(
    crawler: AsyncWebCrawler,
    subject_info: Dict[str, str],
    css_selector: str,
    llm_strategy,
    session_id: str,
    required_keys: List[str]
) -> List[Dict]:
    """
    çˆ¬å–å•ä¸ªå­¦ç§‘çš„æ’åæ•°æ®ï¼ˆæµ‹è¯•æ¨¡å¼é™åˆ¶é¡µæ•°ï¼‰
    """
    subject_name = subject_info["subject_name"]
    subject_url = subject_info["url"]
    
    print(f"\n{'='*60}")
    print(f"ğŸ¯ å¼€å§‹çˆ¬å–å­¦ç§‘: {subject_name}")
    print(f"ğŸ”— URL: {subject_url}")
    print(f"{'='*60}")
    
    all_venues = []
    seen_names = set()
    page_number = 1
    
    # ğŸ”¥ æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶é¡µæ•° ğŸ”¥
    max_pages = MAX_PAGES_PER_SUBJECT if TEST_MODE else 20
    print(f"ğŸ“„ {'æµ‹è¯•æ¨¡å¼' if TEST_MODE else 'æ­£å¸¸æ¨¡å¼'}ï¼šæœ€å¤šçˆ¬å– {max_pages} é¡µ")
    
    consecutive_empty_pages = 0
    
    while page_number <= max_pages:
        try:
            venues, should_stop = await fetch_and_process_page(
                crawler,
                page_number,
                subject_url,
                subject_name,
                css_selector,
                llm_strategy,
                f"{session_id}_{subject_name.replace(' ', '_')}",
                required_keys,
                seen_names,
            )
            
            if should_stop:
                print(f"ğŸ”š {subject_name} æ£€æµ‹åˆ°åœæ­¢æ¡ä»¶")
                break
                
            if not venues:
                consecutive_empty_pages += 1
                print(f"âš ï¸ {subject_name} ç¬¬ {page_number} é¡µæ— æ•°æ® (è¿ç»­ç©ºé¡µ: {consecutive_empty_pages})")
                
                if consecutive_empty_pages >= 2:  # æµ‹è¯•æ¨¡å¼ä¸‹æ›´å¿«åœæ­¢
                    print(f"ğŸ”š {subject_name} è¿ç»­2é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                    break
            else:
                consecutive_empty_pages = 0
                
                # ğŸ”¥ ä¸ºæ¯æ¡æ•°æ®æ·»åŠ å­¦ç§‘ä¿¡æ¯ ğŸ”¥
                for venue in venues:
                    venue["subject"] = subject_name
                
                all_venues.extend(venues)
                print(f"ğŸ“Š {subject_name} ç´¯è®¡æ•°æ®: {len(all_venues)} æ¡")
            
            page_number += 1
            
            # é¡µé¢é—´å»¶è¿Ÿï¼ˆæµ‹è¯•æ¨¡å¼ä¸‹å‡å°‘å»¶è¿Ÿï¼‰
            if page_number <= max_pages:
                delay = 2 if TEST_MODE else 3
                await asyncio.sleep(delay)
            
        except Exception as e:
            print(f"âŒ {subject_name} ç¬¬ {page_number} é¡µå¼‚å¸¸: {e}")
            consecutive_empty_pages += 1
            if consecutive_empty_pages >= 2:
                break
            page_number += 1
    
    print(f"ğŸ‰ {subject_name} çˆ¬å–å®Œæˆï¼Œå…± {len(all_venues)} æ¡æ•°æ®")
    return all_venues


async def crawl_all_subject_rankings(
    index_url: str,
    subject_link_selector: str,
    ranking_css_selector: str,
    required_keys: List[str]
) -> List[Dict]:
    """
    çˆ¬å–æ‰€æœ‰å­¦ç§‘çš„æ’åæ•°æ®ï¼ˆæ”¯æŒæµ‹è¯•æ¨¡å¼ï¼‰
    """
    browser_config = get_browser_config()
    llm_strategy = get_llm_strategy()
    session_id = "subject_ranking_crawl"
    
    all_subject_data = []
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        # ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šæå–å­¦ç§‘é“¾æ¥ ğŸ”¥
        subject_links = await extract_subject_links(
            crawler,
            index_url,
            subject_link_selector,
            session_id
        )
        
        if not subject_links:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å­¦ç§‘é“¾æ¥")
            return []
        
        mode_info = f"{'ğŸ§ª æµ‹è¯•æ¨¡å¼' if TEST_MODE else 'ğŸš€ æ­£å¸¸æ¨¡å¼'}"
        print(f"\nğŸ“‹ {mode_info} - æ‰¾åˆ° {len(subject_links)} ä¸ªå­¦ç§‘ï¼Œå¼€å§‹é€ä¸ªçˆ¬å–...")
        
        # ğŸ”¥ ç¬¬äºŒæ­¥ï¼šé€ä¸ªçˆ¬å–æ¯ä¸ªå­¦ç§‘çš„æ’å ğŸ”¥
        for index, subject_info in enumerate(subject_links):
            print(f"\nğŸ”„ è¿›åº¦: {index + 1}/{len(subject_links)}")
            
            try:
                subject_data = await crawl_single_subject_ranking(
                    crawler,
                    subject_info,
                    ranking_css_selector,
                    llm_strategy,
                    session_id,
                    required_keys
                )
                
                all_subject_data.extend(subject_data)
                print(f"ğŸ“Š æ€»ç´¯è®¡æ•°æ®: {len(all_subject_data)} æ¡")
                
                # å­¦ç§‘é—´å»¶è¿Ÿï¼ˆæµ‹è¯•æ¨¡å¼ä¸‹å‡å°‘å»¶è¿Ÿï¼‰
                if index < len(subject_links) - 1:
                    delay = 5 if TEST_MODE else 15
                    print(f"â±ï¸ ç­‰å¾… {delay} ç§’åçˆ¬å–ä¸‹ä¸€ä¸ªå­¦ç§‘...")
                    await asyncio.sleep(delay)
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–å­¦ç§‘ {subject_info['subject_name']} å¤±è´¥: {e}")
                continue
    
    return all_subject_data