"""
ä½¿ç”¨crawl4aiè¿›è¡Œç½‘é¡µçˆ¬å–å’Œå¿«ç…§æˆªå–
"""
import base64
import asyncio
import os
from datetime import datetime
from crawl4ai import *
current_dir = os.path.dirname(os.path.abspath(__file__))

async def demo_screenshot():
    async with AsyncWebCrawler() as crawler:
        result : CrawlResult = await crawler.arun(
                url="https://www.eol.cn/news/yaowen/202408/t20240801_2627275.shtml",
                config=CrawlerRunConfig(screenshot=True)
        )

        if result.screenshot:
            screenshot_path = f"{current_dir}/snapshots/screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            with open(screenshot_path, 'wb') as f:
                f.write(base64.b64decode(result.screenshot))
            print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {screenshot_path}")

async def main():
    await demo_screenshot()

if __name__ == "__main__":
    asyncio.run(main())
    from PIL import Image

    # æ‰“å¼€åŸå§‹å›¾ç‰‡
    img = Image.open("snapshots\screenshot_20250619_213839.png")
    width, height = img.size

    # æ£€æŸ¥é«˜åº¦æ˜¯å¦ç¬¦åˆè¦æ±‚
    if height == 40000:
        # åˆ†å‰²å›¾ç‰‡
        for i in range(10):
            # è®¡ç®—è£å‰ªåŒºåŸŸ (å·¦, ä¸Š, å³, ä¸‹)
            box = (0, i*4000, width, (i+1)*4000)
            chunk = img.crop(box)
            chunk.save(f"åˆ†å‰²å›¾_{i+1}.png")  # ä¿å­˜ä¸ºPNGä¿è¯è´¨é‡
        print("åˆ†å‰²å®Œæˆï¼å…±ç”Ÿæˆ10å¼ å›¾ç‰‡")
    else:
        print(f"é”™è¯¯ï¼šå›¾ç‰‡é«˜åº¦åº”ä¸º40000åƒç´ ï¼Œå½“å‰ä¸º{height}åƒç´ ")