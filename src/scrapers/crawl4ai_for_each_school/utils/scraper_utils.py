import asyncio
import os
import json
import re
import time
from typing import List, Optional, Dict
from datetime import datetime

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

from models.school_intro_data import SchoolIntroData, CrawlResult
from config import *


def get_browser_config() -> BrowserConfig:
    """
    è·å–æµè§ˆå™¨é…ç½®
    """
    print("ğŸ”§ é…ç½®æµè§ˆå™¨å‚æ•°...")
    config = BrowserConfig(
        browser_type="chromium",
        headless=ENABLE_HEADLESS,
        verbose=True,
    )
    print("âœ… æµè§ˆå™¨é…ç½®å®Œæˆ")
    return config


def create_school_intro_llm_strategy() -> LLMExtractionStrategy:
    """
    åˆ›å»ºå­¦æ ¡ç®€ä»‹ä¿¡æ¯æå–çš„LLMç­–ç•¥
    """
    print("ğŸ¤– åˆ›å»ºå­¦æ ¡ç®€ä»‹ä¿¡æ¯æå–ç­–ç•¥...")
    
    extraction_instruction = """ä½ æ˜¯ä¸“ä¸šçš„æ•™è‚²æ•°æ®æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å­¦æ ¡ç®€ä»‹æˆ–å­¦æ ¡æ¦‚å†µç½‘é¡µå†…å®¹ä¸­æå–æ ¸å¿ƒæ•™å­¦æ•°æ®ã€‚

**æå–ç›®æ ‡æ•°æ®ï¼š**
1. å­¦æ ¡åç§°ï¼šå®Œæ•´çš„å­¦æ ¡åç§°
2. æœ¬ç§‘ä¸“ä¸šæ€»æ•°ï¼šå­¦æ ¡å¼€è®¾çš„æœ¬ç§‘ä¸“ä¸šæ•°é‡
3. å›½å®¶çº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹ï¼šè·å¾—å›½å®¶çº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹çš„æ•°é‡
4. çœçº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹ï¼šè·å¾—çœçº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹çš„æ•°é‡  

**æ•°æ®è¯†åˆ«å…³é”®è¯å‚è€ƒï¼š**
- æœ¬ç§‘ä¸“ä¸šï¼šå¯èƒ½è¡¨è¿°ä¸º"æœ¬ç§‘ä¸“ä¸š"ã€"ä¸“ä¸š"ã€"å­¦ç§‘ä¸“ä¸š"ã€"æ‹›ç”Ÿä¸“ä¸š"ã€"å¼€è®¾ä¸“ä¸š"ç­‰
- ä¸€æµä¸“ä¸šï¼šå¯èƒ½è¡¨è¿°ä¸º"ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹"ã€"ä¸€æµä¸“ä¸š"ã€"é‡‘ä¸“"ã€"å›½å®¶çº§ä¸“ä¸š"ã€"çœçº§ä¸“ä¸š"ã€"ä¸“ä¸šå»ºè®¾ç‚¹"ç­‰

**æå–è¦æ±‚ï¼š**
- ä»”ç»†åˆ†ææ•´ä¸ªç½‘é¡µå†…å®¹ï¼Œé‡ç‚¹å…³æ³¨å­¦æ ¡ç®€ä»‹ã€å­¦æ ¡æ¦‚å†µã€ä¸“ä¸šè®¾ç½®ã€æ•™å­¦æˆæœç­‰éƒ¨åˆ†
- å¦‚æœæŸé¡¹æ•°æ®æœªæ˜ç¡®æåŠï¼Œè¯·å¡«å†™0
- ç¡®ä¿æå–çš„æ•°æ®å‡†ç¡®å¯é ï¼Œä¸è¦æ¨æµ‹æˆ–çŒœæµ‹
- ä¼˜å…ˆæå–æ˜ç¡®çš„æ•°å­—ï¼Œé¿å…æ¨¡ç³Šè¡¨è¿°
- æ³¨æ„åŒºåˆ†å›½å®¶çº§å’Œçœçº§çš„ä¸åŒ

**è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONæ•°ç»„ï¼‰ï¼š**
[
    {
    "school_name": "å­¦æ ¡åç§°",
    "undergraduate_majors": æœ¬ç§‘ä¸“ä¸šæ€»æ•°,
    "national_first_class_majors": å›½å®¶çº§ä¸€æµä¸“ä¸šæ•°,
    "provincial_first_class_majors": çœçº§ä¸€æµä¸“ä¸šæ•°,
    },
    {
    "school_name": "å­¦æ ¡åç§°",
    "undergraduate_majors": æœ¬ç§‘ä¸“ä¸šæ€»æ•°,
    "national_first_class_majors": å›½å®¶çº§ä¸€æµä¸“ä¸šæ•°,
    "provincial_first_class_majors": çœçº§ä¸€æµä¸“ä¸šæ•°,
    }
]

**é‡è¦è¯´æ˜ï¼š**
- æ‰€æœ‰æ•°å­—å­—æ®µå¿…é¡»æ˜¯æ•´æ•°ç±»å‹ï¼Œä¸èƒ½æ˜¯å­—ç¬¦ä¸²
- å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ï¼Œå¯¹åº”å­—æ®µå¡«å†™0
- ä¸è¦çŒœæµ‹æˆ–æ¨æ–­ä¿¡æ¯ï¼Œåªæå–æ˜ç¡®æ˜¾ç¤ºçš„å†…å®¹
- å­¦æ ¡åç§°å¿…é¡»å®Œæ•´å‡†ç¡®
- å¦‚æœç¡®å®æ— æ³•æå–åˆ°å­¦æ ¡ä¿¡æ¯ï¼Œè¿”å›null

è¯·å¼€å§‹è¯¦ç»†æå–ï¼š"""
    
    llm_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(
            provider=LLM_MODEL,
            api_token=os.getenv(API_KEY_ENV, os.getenv("DEEPSEEK_API_KEY")),
            base_url=LLM_BASE_URL,
        ),
        schema='[{"school_name": "str", "undergraduate_majors": "int", "national_first_class_majors": "int", "provincial_first_class_majors": "int"}]',
        extraction_type="schema",
        instruction=extraction_instruction,
        
        # å¯ç”¨Crawl4aiè‡ªåŠ¨åˆ†å—
        apply_chunking=True,
        chunk_token_threshold=6000,
        overlap_rate=0.15,
        
        input_format="markdown",
        extra_args={
            "temperature": 0.0,
            "max_tokens": 8000,
            "top_p": 0.1,
        }
    )
    print("âœ… å­¦æ ¡ç®€ä»‹ä¿¡æ¯æå–ç­–ç•¥åˆ›å»ºå®Œæˆ")
    return llm_strategy


def validate_school_intro_data(school_data: dict, expected_school: str) -> bool:
    """
    éªŒè¯æå–çš„å­¦æ ¡ç®€ä»‹æ•°æ®è´¨é‡
    """
    if not school_data:
        return False
        
    school_name = school_data.get('school_name', '').strip()
    
    # åŸºæœ¬æ£€æŸ¥
    if len(school_name) < 2:
        print(f"âš ï¸ å­¦æ ¡åç§°è¿‡çŸ­: {school_name}")
        return False
    
    # æ£€æŸ¥æ•°å­—å­—æ®µ
    required_fields = ['undergraduate_majors', 'national_first_class_majors', 
                      'provincial_first_class_majors']
    
    for field in required_fields:
        value = school_data.get(field)
        if value is None:
            print(f"âš ï¸ ç¼ºå°‘å­—æ®µ: {field}")
            return False
            
        # å°è¯•è½¬æ¢ä¸ºæ•´æ•°
        try:
            int_value = int(value)
            if int_value < 0:
                print(f"âš ï¸ å­—æ®µ {field} ä¸èƒ½ä¸ºè´Ÿæ•°: {int_value}")
                return False
            school_data[field] = int_value  # ç¡®ä¿æ˜¯æ•´æ•°ç±»å‹
        except (ValueError, TypeError):
            print(f"âš ï¸ å­—æ®µ {field} ä¸æ˜¯æœ‰æ•ˆæ•°å­—: {value} (ç±»å‹: {type(value)})")
            return False
    
    # è¿‡æ»¤æ˜æ˜¾é”™è¯¯çš„å­¦æ ¡åç§°
    invalid_patterns = [
        r'^(è¯¦æƒ…|æ›´å¤š|æŸ¥çœ‹|ç‚¹å‡»|é“¾æ¥|æŒ‰é’®|ç™»å½•|æ³¨å†Œ|é¦–é¡µ|å¯¼èˆª|èœå•|å…³äºæˆ‘ä»¬|è”ç³»æˆ‘ä»¬).*',
        r'.*\.(com|cn|org|net).*',
        r'^\d+$',  # çº¯æ•°å­—
        r'^(æ–°é—»|é€šçŸ¥|å…¬å‘Š|èµ„è®¯|ç½‘ç«™|å®˜ç½‘).*',
        r'^(å­¦é™¢|ä¸“ä¸š|è¯¾ç¨‹|æ•™å¸ˆ|å­¦ç”Ÿ)$',  # å•ä¸ªé€šç”¨è¯
    ]
    
    for pattern in invalid_patterns:
        if re.match(pattern, school_name, re.IGNORECASE):
            print(f"âš ï¸ å­¦æ ¡åç§°æ ¼å¼æ— æ•ˆ: {school_name}")
            return False
    
    return True


async def crawl_single_school_intro(school_name: str, url: str) -> CrawlResult:
    """
    çˆ¬å–å•ä¸ªå­¦æ ¡çš„ç®€ä»‹ä¿¡æ¯
    """
    print(f"\nğŸ« [{school_name}] å¼€å§‹çˆ¬å–: {url}")
    
    # åˆ›å»ºsession ID
    unique_timestamp = int(time.time() * 1000)
    session_id = f"school_intro_{school_name}_{unique_timestamp}"
    
    async with AsyncWebCrawler(config=get_browser_config()) as crawler:
        try:
            # ç®€åŒ–çš„ç­‰å¾…JSä»£ç 
            simple_wait_js = """
            (async function() {
                console.log('ğŸ”„ å¼€å§‹ç­‰å¾…å­¦æ ¡ç®€ä»‹é¡µé¢åŠ è½½...');
                
                // ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                await new Promise(resolve => setTimeout(resolve, 3000));
                
                // æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨ç¡®ä¿æ‰€æœ‰å†…å®¹å¯è§
                window.scrollTo(0, document.body.scrollHeight);
                await new Promise(resolve => setTimeout(resolve, 1000));
                
                console.log('âœ… å­¦æ ¡ç®€ä»‹é¡µé¢å‡†å¤‡å®Œæˆ');
                return { success: true };
            })();
            """
            
            # åˆ›å»ºLLMç­–ç•¥
            print("ğŸ¤– åˆ›å»ºLLMæå–ç­–ç•¥...")
            llm_strategy = create_school_intro_llm_strategy()
            
            # æ‰§è¡Œçˆ¬å–
            print("âš¡ å¼€å§‹LLMåˆ†æ...")
            
            result = await crawler.arun(
                url=url,
                config=CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    session_id=session_id,
                    js_code=simple_wait_js,
                    extraction_strategy=llm_strategy,
                    delay_before_return_html=5.0,
                    wait_for_images=False,
                    page_timeout=30000,
                    override_navigator=True,
                ),
            )
            
            if not result.success:
                error_msg = f"é¡µé¢åŠ è½½å¤±è´¥: {result.error_message}"
                print(f"âŒ [{school_name}] {error_msg}")
                return CrawlResult(
                    success=False,
                    school_name=school_name,
                    error_message=error_msg,
                    url=url
                )
            
            print(f"âœ… [{school_name}] é¡µé¢åŠ è½½æˆåŠŸï¼ŒHTMLé•¿åº¦: {len(result.cleaned_html):,} å­—ç¬¦")
            print(f"ğŸ”„  {result.extracted_content}")
            
            # å¤„ç†LLMæå–ç»“æœ
            print("ğŸ“Š å¤„ç†æå–ç»“æœ...")
            
            if result.extracted_content:
                try:
                    print(f"ğŸ” [{school_name}] LLMæå–ç»“æœé•¿åº¦: {len(result.extracted_content):,} å­—ç¬¦")
                    print(f"ğŸ” [{school_name}] LLMæå–å†…å®¹é¢„è§ˆ: {result.extracted_content[:300]}...")
                    
                    # è§£æLLMæå–çš„ç»“æœ
                    extracted_data = json.loads(result.extracted_content)
                    
                    # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
                    school_data = None
                    
                    if isinstance(extracted_data, dict):
                        # ç›´æ¥æ˜¯å­—å…¸æ ¼å¼
                        if 'school_name' in extracted_data:
                            school_data = extracted_data
                        else:
                            # å¯èƒ½åŒ…è£…åœ¨å…¶ä»–é”®ä¸­
                            for key, value in extracted_data.items():
                                if isinstance(value, dict) and 'school_name' in value:
                                    school_data = value
                                    break
                    
                    elif isinstance(extracted_data, list):
                        # æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå¯»æ‰¾æœ‰æ•ˆçš„å­—å…¸é¡¹
                        print(f"ğŸ” [{school_name}] æ”¶åˆ°åˆ—è¡¨æ ¼å¼ç»“æœï¼Œå°è¯•è§£æ...")
                        
                        for item in extracted_data:
                            if isinstance(item, dict):
                                # è·³è¿‡é”™è¯¯é¡¹
                                if item.get('error') == True:
                                    print(f"âŒ [{school_name}] LLMè¿”å›é”™è¯¯: {item.get('content', 'Unknown error')}")
                                    continue
                                
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
                                if 'school_name' in item:
                                    school_data = item
                                    print(f"âœ… [{school_name}] ä»åˆ—è¡¨ä¸­æ‰¾åˆ°æœ‰æ•ˆæ•°æ®é¡¹")
                                    break
                    
                    if school_data and validate_school_intro_data(school_data, school_name):
                        school_intro = SchoolIntroData(
                            school_name=school_data['school_name'].strip(),
                            undergraduate_majors=school_data['undergraduate_majors'],
                            national_first_class_majors=school_data['national_first_class_majors'],
                            provincial_first_class_majors=school_data['provincial_first_class_majors'],
                            source_url=url,
                            crawl_timestamp=datetime.now().isoformat()
                        )
                        
                        print(f"âœ… [{school_name}] æ•°æ®æå–æˆåŠŸ: {school_intro.school_name}")
                        print(f"  ğŸ“š æœ¬ç§‘ä¸“ä¸š({school_intro.undergraduate_majors}) å›½å®¶çº§ä¸€æµä¸“ä¸š({school_intro.national_first_class_majors}) çœçº§ä¸€æµä¸“ä¸š({school_intro.provincial_first_class_majors}) ")
                        
                        return CrawlResult(
                            success=True,
                            school_name=school_name,
                            data=school_intro,
                            url=url
                        )
                    else:
                        error_msg = "æ•°æ®éªŒè¯å¤±è´¥æˆ–æ— æ³•è§£æ"
                        print(f"âŒ [{school_name}] {error_msg}")
                        if school_data:
                            print(f"æå–çš„æ•°æ®: {school_data}")
                        return CrawlResult(
                            success=False,
                            school_name=school_name,
                            error_message=error_msg,
                            url=url
                        )
                        
                except json.JSONDecodeError as e:
                    error_msg = f"JSONè§£æå¤±è´¥: {e}"
                    print(f"âŒ [{school_name}] {error_msg}")
                    print(f"åŸå§‹æå–å†…å®¹: {result.extracted_content[:500]}...")
                    return CrawlResult(
                        success=False,
                        school_name=school_name,
                        error_message=error_msg,
                        url=url
                    )
                    
            else:
                error_msg = "LLMæœªæå–åˆ°ä»»ä½•å†…å®¹"
                print(f"âŒ [{school_name}] {error_msg}")
                return CrawlResult(
                    success=False,
                    school_name=school_name,
                    error_message=error_msg,
                    url=url
                )
            
        except Exception as e:
            error_msg = f"çˆ¬å–å¼‚å¸¸: {str(e)}"
            print(f"âŒ [{school_name}] {error_msg}")
            import traceback
            traceback.print_exc()
            return CrawlResult(
                success=False,
                school_name=school_name,
                error_message=error_msg,
                url=url
            )


async def crawl_multiple_school_intros(school_websites: Dict[str, str] = None) -> List[CrawlResult]:
    """
    æ‰¹é‡çˆ¬å–å­¦æ ¡ç®€ä»‹ä¿¡æ¯
    """
    if school_websites is None:
        school_websites = SCHOOL_WEBSITES
    
    print(f"\n{'='*80}")
    print(f"ğŸ« å¼€å§‹æ‰¹é‡çˆ¬å–å­¦æ ¡ç®€ä»‹ä¿¡æ¯")
    print(f"ğŸ¯ ç›®æ ‡å­¦æ ¡æ•°é‡: {len(school_websites)}")
    print(f"ğŸ¤– ç­–ç•¥ï¼šé¡µé¢åŠ è½½ + LLMåˆ†æ")
    print(f"ğŸ”„ çˆ¬å–é—´éš”: {CRAWL_INTERVAL}ç§’")
    print(f"{'='*80}")
    
    if not school_websites:
        print("âŒ æ²¡æœ‰é…ç½®è¦çˆ¬å–çš„å­¦æ ¡ç½‘å€")
        return []
    
    results = []
    
    for i, (school_name, url) in enumerate(school_websites.items(), 1):
        print(f"\nğŸ“ [{i}/{len(school_websites)}] å¤„ç†å­¦æ ¡: {school_name}")
        
        # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
        if not url.startswith('http'):
            url = f"https://{url}"
        
        # çˆ¬å–å•ä¸ªå­¦æ ¡
        result = await crawl_single_school_intro(school_name, url)
        results.append(result)
        
        # æ˜¾ç¤ºè¿›åº¦
        success_count = sum(1 for r in results if r.success)
        print(f"ğŸ“Š [{i}/{len(school_websites)}] å½“å‰æˆåŠŸç‡: {success_count}/{i} ({success_count/i*100:.1f}%)")
        
        # æ·»åŠ é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        if i < len(school_websites):
            print(f"â³ ç­‰å¾… {CRAWL_INTERVAL} ç§’...")
            await asyncio.sleep(CRAWL_INTERVAL)
    
    # ç»“æœç»Ÿè®¡
    total_count = len(results)
    success_count = sum(1 for r in results if r.success)
    failed_count = total_count - success_count
    
    print(f"\nğŸ‰ å­¦æ ¡ç®€ä»‹ä¿¡æ¯çˆ¬å–å®Œæˆ!")
    print(f"{'='*60}")
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  ğŸ« ç›®æ ‡å­¦æ ¡æ•°é‡: {total_count}")
    print(f"  âœ… æˆåŠŸæå–æ•°é‡: {success_count}")
    print(f"  âŒ å¤±è´¥æ•°é‡: {failed_count}")
    if total_count > 0:
        print(f"  ğŸ“ˆ æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
    
    # æˆåŠŸçš„å­¦æ ¡
    successful_schools = [r for r in results if r.success]
    if successful_schools:
        print(f"\nâœ… æˆåŠŸæå–çš„å­¦æ ¡:")
        for result in successful_schools:
            print(f"  ğŸ« {result.data.school_name if result.data else result.school_name}")
    
    # å¤±è´¥çš„å­¦æ ¡
    failed_schools = [r for r in results if not r.success]
    if failed_schools:
        print(f"\nâŒ æå–å¤±è´¥çš„å­¦æ ¡:")
        for result in failed_schools:
            print(f"  ğŸ« {result.school_name}: {result.error_message}")
    
    return results


# ä¾¿æ·å‡½æ•°ï¼šä»é…ç½®æ–‡ä»¶è¯»å–URLåˆ—è¡¨å¹¶çˆ¬å–
async def crawl_configured_school_intros() -> List[CrawlResult]:
    """
    ä»é…ç½®æ–‡ä»¶è¯»å–URLåˆ—è¡¨å¹¶çˆ¬å–å­¦æ ¡ç®€ä»‹ä¿¡æ¯
    """
    return await crawl_multiple_school_intros(SCHOOL_WEBSITES)