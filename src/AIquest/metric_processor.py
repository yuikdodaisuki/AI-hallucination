"""æŒ‡æ ‡æ•°æ®å¤„ç†å™¨"""
import csv
import os
from src.AIquest.utils.llm_client import LLMClient
from src.AIquest.utils.data_reader import DataReader
from src.AIquest.utils.question_processor import QuestionProcessor
from src.AIquest.utils.file_utils import FileUtils
from src.AIquest.config import METRIC_DATA_MAPPING, OUTPUT_CONFIG, METRIC_CATEGORIES


class MetricDataProcessor:
    """æ ¹æ®æŒ‡æ ‡ç±»å‹å¤„ç†æ•°æ®å’Œé—®é¢˜çš„ä¸»ç±»"""
    
    def __init__(self, config_path=None):
        self.llm_client = LLMClient(config_path)
        self.data_reader = DataReader()
        self.question_processor = QuestionProcessor(self.llm_client, self.data_reader)
        self.file_utils = FileUtils()
    
    def process_metric_questions(self, metric_name, questions_csv_path, output_csv_path):
        """å¤„ç†ç‰¹å®šæŒ‡æ ‡çš„é—®é¢˜"""
        print(f"\nå¼€å§‹å¤„ç†æŒ‡æ ‡: {metric_name}")
        
        # éªŒè¯æŒ‡æ ‡æ˜¯å¦åœ¨æ”¯æŒçš„åˆ—è¡¨ä¸­
        if not self._validate_metric(metric_name):
            return False
        
        # 1. æ ¹æ®æŒ‡æ ‡è·å–éœ€è¦çš„æ•°æ®æº
        data_sources = METRIC_DATA_MAPPING.get(metric_name, [])
        if not data_sources:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æŒ‡æ ‡ '{metric_name}' å¯¹åº”çš„æ•°æ®æºé…ç½®")
            return False
        
        # 2. æ•´åˆå¯¹åº”çš„æ•°æ®
        consolidated_data_path = self.data_reader.consolidate_data_for_metric(metric_name, data_sources)
        if not consolidated_data_path:
            print(f"é”™è¯¯: æœªèƒ½ä¸ºæŒ‡æ ‡ '{metric_name}' æ•´åˆæ•°æ®")
            return False
        
        # 3. ç­›é€‰å‡ºè¯¥æŒ‡æ ‡çš„é—®é¢˜
        metric_questions = self.question_processor.filter_questions_by_metric(questions_csv_path, metric_name)
        if not metric_questions:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æŒ‡æ ‡ '{metric_name}' çš„ç›¸å…³é—®é¢˜")
            return False
        
        # 4. å¤„ç†é—®é¢˜å¹¶è·å–ç­”æ¡ˆ
        return self.question_processor.process_metric_questions(
            metric_questions, consolidated_data_path, output_csv_path, metric_name
        )
    
    def _validate_metric(self, metric_name):
        """éªŒè¯æŒ‡æ ‡æ˜¯å¦åœ¨æ–°çš„æ”¯æŒåˆ—è¡¨ä¸­"""
        all_supported_metrics = (
            METRIC_CATEGORIES['subject_metrics'] + 
            METRIC_CATEGORIES['major_metrics']
        )
        
        if metric_name not in all_supported_metrics:
            print(f"âŒ ä¸æ”¯æŒçš„æŒ‡æ ‡: {metric_name}")
            print("âœ… æ”¯æŒçš„æŒ‡æ ‡åˆ—è¡¨:")
            print("  ğŸ“š å­¦ç§‘æŒ‡æ ‡:")
            for metric in METRIC_CATEGORIES['subject_metrics']:
                print(f"    - {metric}")
            print("  ğŸ“ ä¸“ä¸šæŒ‡æ ‡:")
            for metric in METRIC_CATEGORIES['major_metrics']:
                print(f"    - {metric}")
            return False
        
        return True
    
    def process_all_metrics(self, questions_csv_path, output_base_path):
        """å¤„ç†æ‰€æœ‰æŒ‡æ ‡çš„é—®é¢˜"""
        # è·å–æ‰€æœ‰å”¯ä¸€çš„æŒ‡æ ‡
        all_metrics = set()
        try:
            with open(questions_csv_path, mode='r', encoding=OUTPUT_CONFIG['file_encoding']) as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    metric = row.get('æŒ‡æ ‡åç§°')
                    if metric and metric != 'å¾…å¡«å……':
                        all_metrics.add(metric)
        except Exception as e:
            print(f"è¯»å–é—®é¢˜æ–‡ä»¶å¤±è´¥: {e}")
            return False
        
        print(f"å‘ç° {len(all_metrics)} ä¸ªä¸åŒçš„æŒ‡æ ‡éœ€è¦å¤„ç†")
        
        # éªŒè¯æ‰€æœ‰æŒ‡æ ‡éƒ½è¢«æ”¯æŒ
        unsupported_metrics = []
        for metric in all_metrics:
            if not self._validate_metric_silent(metric):
                unsupported_metrics.append(metric)
        
        if unsupported_metrics:
            print(f"âš ï¸  å‘ç° {len(unsupported_metrics)} ä¸ªä¸æ”¯æŒçš„æŒ‡æ ‡:")
            for metric in unsupported_metrics:
                print(f"    - {metric}")
            print("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æˆ–æ›´æ–°æŒ‡æ ‡æ˜ å°„")
        
        # åªå¤„ç†æ”¯æŒçš„æŒ‡æ ‡
        supported_metrics = [m for m in all_metrics if self._validate_metric_silent(m)]
        print(f"âœ… å°†å¤„ç† {len(supported_metrics)} ä¸ªæ”¯æŒçš„æŒ‡æ ‡")
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡å•ç‹¬å¤„ç†
        all_results = []
        for metric in supported_metrics:
            output_path = f"{output_base_path}_{metric}_answers.csv"
            success = self.process_metric_questions(metric, questions_csv_path, output_path)
            if success:
                all_results.append(output_path)
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        if all_results:
            final_output_path = f"{output_base_path}_all_answers.csv"
            self.file_utils.merge_csv_files(all_results, final_output_path)
        
        return True
    
    def _validate_metric_silent(self, metric_name):
        """é™é»˜éªŒè¯æŒ‡æ ‡ï¼ˆä¸æ‰“å°é”™è¯¯ä¿¡æ¯ï¼‰"""
        all_supported_metrics = (
            METRIC_CATEGORIES['subject_metrics'] + 
            METRIC_CATEGORIES['major_metrics']
        )
        return metric_name in all_supported_metrics
    
    def get_available_metrics(self):
        """è·å–æ‰€æœ‰å¯ç”¨çš„æŒ‡æ ‡ï¼ˆæŒ‰ç±»åˆ«åˆ†ç»„ï¼‰"""
        return {
            'subject_metrics': METRIC_CATEGORIES['subject_metrics'],
            'major_metrics': METRIC_CATEGORIES['major_metrics'],
            'all_metrics': METRIC_CATEGORIES['subject_metrics'] + METRIC_CATEGORIES['major_metrics']
        }
    
    def validate_data_sources(self):
        """éªŒè¯æ•°æ®æºæ˜¯å¦å­˜åœ¨"""
        print("ğŸ” éªŒè¯æ•°æ®æº...")
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        for metric, sources in METRIC_DATA_MAPPING.items():
            print(f"  ğŸ“Š æŒ‡æ ‡: {metric}")
            for source in sources:
                source_path = os.path.join(current_dir, '../../data', 
                                         DATA_SOURCES.get(source, source).replace('../../data/', ''))
                exists = os.path.exists(source_path)
                status = 'âœ…' if exists else 'âŒ'
                print(f"    {status} æ•°æ®æº {source}: {source_path}")
    
    def get_metric_statistics(self, questions_csv_path):
        """è·å–æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯"""
        metric_stats = {}
        school_count = 0
        
        try:
            with open(questions_csv_path, mode='r', encoding=OUTPUT_CONFIG['file_encoding']) as csvfile:
                reader = csv.DictReader(csvfile)
                schools = set()
                
                for row in reader:
                    metric = row.get('æŒ‡æ ‡åç§°')
                    school = row.get('å­¦æ ¡åç§°')
                    
                    if metric and metric != 'å¾…å¡«å……':
                        if metric not in metric_stats:
                            metric_stats[metric] = 0
                        metric_stats[metric] += 1
                    
                    if school:
                        schools.add(school)
                
                school_count = len(schools)
        
        except Exception as e:
            print(f"ç»Ÿè®¡æŒ‡æ ‡ä¿¡æ¯å¤±è´¥: {e}")
            return None
        
        return {
            'total_schools': school_count,
            'total_questions': sum(metric_stats.values()),
            'metrics_distribution': metric_stats,
            'supported_metrics': {k: v for k, v in metric_stats.items() if self._validate_metric_silent(k)},
            'unsupported_metrics': {k: v for k, v in metric_stats.items() if not self._validate_metric_silent(k)}
        }