"""æ•°æ®è¯»å–å’Œæ•´åˆåŠŸèƒ½"""
import os
import json
from src.AIquest.config import DATA_SOURCES, OUTPUT_CONFIG, DATA_SOURCE_PRIORITY, REQUIRED_DIRECTORIES


class DataReader:
    """æ•°æ®è¯»å–å’Œæ•´åˆåŠŸèƒ½"""
    
    def __init__(self):
        # ğŸ”¥ ä¿®æ­£è·¯å¾„è®¡ç®— - dataç›®å½•å’ŒAIqueståŒçº§ ğŸ”¥
        self.current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # src/AIquest
        self.src_dir = os.path.dirname(self.current_dir)  # src
        self.data_dir = os.path.join(self.src_dir, 'data')  # src/data
        self._ensure_directories_exist()
        
        print(f"ğŸ“ DataReaderè·¯å¾„ä¿¡æ¯:")
        print(f"  AIquestç›®å½•: {self.current_dir}")
        print(f"  srcç›®å½•: {self.src_dir}")
        print(f"  dataç›®å½•: {self.data_dir}")
    
    def _ensure_directories_exist(self):
        """ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„ç›®å½•å­˜åœ¨ï¼ˆè·³è¿‡å·²å­˜åœ¨çš„moepoliciesï¼‰"""
        for directory in REQUIRED_DIRECTORIES:
            dir_path = os.path.join(self.data_dir, directory)
            if not os.path.exists(dir_path):
                try:
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  ğŸ“ åˆ›å»ºç›®å½•: {dir_path}")
                    
                    # ä¸ºæ–°åˆ›å»ºçš„ç›®å½•æ·»åŠ READMEæ–‡ä»¶
                    readme_path = os.path.join(dir_path, 'README.md')
                    readme_content = self._generate_readme_content(directory)
                    with open(readme_path, 'w', encoding='utf-8') as f:
                        f.write(readme_content)
                        
                except Exception as e:
                    print(f"  âŒ åˆ›å»ºç›®å½•å¤±è´¥ {dir_path}: {e}")
    
    def _generate_readme_content(self, dir_name):
        """ç”Ÿæˆç›®å½•è¯´æ˜æ–‡æ¡£"""
        descriptions = {
            'esi_subjects': 'ESIå­¦ç§‘ç›¸å…³æ•°æ®ï¼ŒåŒ…å«è¿›å…¥ESIæ’åçš„å­¦ç§‘ä¿¡æ¯',
            'esi_subjects/esi_top1percent': 'ESIå‰1%å­¦ç§‘ä¸“é—¨æ•°æ®',
            'esi_subjects/esi_top1permille': 'ESIå‰1â€°å­¦ç§‘ä¸“é—¨æ•°æ®',
            'ruanke_subjects': 'è½¯ç§‘ä¸­å›½æœ€å¥½å­¦ç§‘æ’åç›¸å…³æ•°æ®',
            'subject_evaluation': 'æ•™è‚²éƒ¨å­¦ç§‘è¯„ä¼°ç›¸å…³æ•°æ®ï¼ŒåŒ…å«A+ã€Aã€A-ç­‰è¯„çº§ä¿¡æ¯',
            'undergraduate_majors': 'æœ¬ç§‘ä¸“ä¸šç›¸å…³æ•°æ®',
            'undergraduate_majors/total_majors': 'æœ¬ç§‘ä¸“ä¸šæ€»æ•°ç»Ÿè®¡æ•°æ®',
            'undergraduate_majors/certified_majors': 'é€šè¿‡ä¸“ä¸šè®¤è¯çš„æœ¬ç§‘ä¸“ä¸šæ•°æ®',
            'undergraduate_majors/national_first_class': 'å›½å®¶çº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹æ•°æ®',
            'undergraduate_majors/provincial_first_class': 'çœçº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹æ•°æ®',
            'consolidated': 'æ•´åˆåçš„æ•°æ®æ–‡ä»¶'
        }
        
        description = descriptions.get(dir_name, 'æ•°æ®å­˜å‚¨ç›®å½•')
        
        return f"""# {dir_name} æ•°æ®ç›®å½•

## ç›®å½•è¯´æ˜
{description}

## æ•°æ®æ ¼å¼è¦æ±‚
- æ–‡ä»¶æ ¼å¼ï¼šJSON
- ç¼–ç ï¼šUTF-8
- æ–‡ä»¶å‘½åï¼šå»ºè®®ä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°ï¼Œå¦‚ `university_name.json`

## ä½¿ç”¨è¯´æ˜
1. å°†ç›¸å…³æ•°æ®æ–‡ä»¶æ”¾ç½®åœ¨æ­¤ç›®å½•ä¸­
2. ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰«æå¹¶è¯»å–æ‰€æœ‰ `.json` æ–‡ä»¶
3. æ•°æ®ä¼šè¢«è‡ªåŠ¨æ•´åˆåˆ°é—®ç­”ç³»ç»Ÿä¸­

## æ›´æ–°æ—¶é—´
{self._get_current_time()}
"""
    
    def _get_current_time(self):
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def read_data_from_source(self, source_name):
        """ä»æŒ‡å®šæ•°æ®æºè¯»å–æ•°æ®"""
        if source_name not in DATA_SOURCES:
            print(f"è­¦å‘Š: æœªçŸ¥çš„æ•°æ®æº: {source_name}")
            return []
        
        # ğŸ”¥ ä¿®æ­£è·¯å¾„è®¡ç®— - åŸºäºsrc/dataç›®å½• ğŸ”¥
        source_relative_path = DATA_SOURCES[source_name]
        
        # ç§»é™¤è·¯å¾„å‰ç¼€å¹¶è®¡ç®—æ­£ç¡®è·¯å¾„
        if source_relative_path.startswith('../data/'):
            clean_path = source_relative_path.replace('../data/', '')
        else:
            clean_path = source_relative_path
        
        source_path = os.path.join(self.data_dir, clean_path)
        print(f"    æ­£åœ¨è¯»å–æ•°æ®æº: {source_path}")
        
        all_data = []
        
        if not os.path.exists(source_path):
            print(f"    è­¦å‘Š: æ•°æ®æºè·¯å¾„ä¸å­˜åœ¨: {source_path}")
            # ğŸ”¥ æ·»åŠ è°ƒè¯•ä¿¡æ¯ ğŸ”¥
            print(f"    è°ƒè¯•ä¿¡æ¯:")
            print(f"      é…ç½®çš„æºè·¯å¾„: {source_relative_path}")
            print(f"      æ¸…ç†åçš„è·¯å¾„: {clean_path}")
            print(f"      dataç›®å½•: {self.data_dir}")
            print(f"      æœ€ç»ˆè·¯å¾„: {source_path}")
            print(f"      ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(source_path)}")
            
            # ğŸ”¥ åˆ—å‡ºdataç›®å½•å†…å®¹ ğŸ”¥
            if os.path.exists(self.data_dir):
                print(f"    dataç›®å½•å†…å®¹:")
                for item in os.listdir(self.data_dir):
                    item_path = os.path.join(self.data_dir, item)
                    if os.path.isdir(item_path):
                        print(f"      ğŸ“ {item}/")
                        # å¦‚æœæ˜¯subject_evaluationç›®å½•ï¼Œåˆ—å‡ºå…¶å†…å®¹
                        if item == 'subject_evaluation':
                            try:
                                sub_items = os.listdir(item_path)
                                json_files = [f for f in sub_items if f.endswith('.json')]
                                print(f"        ğŸ“„ JSONæ–‡ä»¶: {len(json_files)} ä¸ª")
                                for json_file in json_files[:3]:
                                    print(f"          - {json_file}")
                            except Exception as e:
                                print(f"        âŒ æ— æ³•è¯»å–ç›®å½•å†…å®¹: {e}")
                    else:
                        print(f"      ğŸ“„ {item}")
            else:
                print(f"    âŒ dataç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            
            return all_data
        
        # é€’å½’è¯»å–æ‰€æœ‰JSONæ–‡ä»¶
        file_count = 0
        for root, _, files in os.walk(source_path):
            for file_name in files:
                if file_name.endswith('.json') and not file_name.startswith('combined_'):
                    file_path = os.path.join(root, file_name)
                    print(f"      è¯»å–æ–‡ä»¶: {file_path}")
                    data = self._read_json_file(file_path)
                    if data:
                        normalized_data = self._normalize_json_data(data, file_path)
                        all_data.extend(normalized_data)
                        file_count += 1
        
        print(f"    ä» {source_path} è¯»å–åˆ° {len(all_data)} æ¡æ•°æ®ï¼Œå¤„ç†äº† {file_count} ä¸ªJSONæ–‡ä»¶")
        return all_data
    
    def _read_json_file(self, file_path):
        """è¯»å–å•ä¸ªJSONæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            print(f"      è­¦å‘Š: {file_path} ä¸æ˜¯æœ‰æ•ˆçš„JSONæ–‡ä»¶: {e}")
            return None
        except Exception as e:
            print(f"      é”™è¯¯: è¯»å– {file_path} å¤±è´¥: {e}")
            return None
    
    def _normalize_json_data(self, data, file_path):
        """æ ‡å‡†åŒ–JSONæ•°æ®æ ¼å¼"""
        if isinstance(data, list):
            return [{**item, "__file_source": file_path} if isinstance(item, dict) 
                   else {"__file_source": file_path, "value": item} for item in data]
        elif isinstance(data, dict):
            return [{**data, "__file_source": file_path}]
        else:
            return [{"__file_source": file_path, "value": data}]
    
    def consolidate_data_for_metric(self, metric_name, data_sources=None):
        """ä¸ºç‰¹å®šæŒ‡æ ‡æ•´åˆæ•°æ®ï¼Œæ”¯æŒä¼˜å…ˆçº§"""
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®æºï¼Œä½¿ç”¨é…ç½®ä¸­çš„ä¼˜å…ˆçº§åˆ—è¡¨
        if data_sources is None:
            data_sources = DATA_SOURCE_PRIORITY.get(metric_name, [])
        
        print(f"  æ­£åœ¨ä¸ºæŒ‡æ ‡ '{metric_name}' æ•´åˆæ•°æ®æº: {data_sources}")
        
        # ğŸ”¥ ä¿®æ­£consolidatedç›®å½•è·¯å¾„ ğŸ”¥
        consolidated_dir = os.path.join(self.data_dir, 'consolidated')
        os.makedirs(consolidated_dir, exist_ok=True)
        
        metric_data_file = os.path.join(consolidated_dir, f"{metric_name}_data.json")
        
        # æŒ‰ä¼˜å…ˆçº§æ”¶é›†æ•°æ®
        all_metric_data = []
        successful_sources = []
        
        for data_source in data_sources:
            source_data = self.read_data_from_source(data_source)
            if source_data:
                all_metric_data.extend(source_data)
                successful_sources.append(data_source)
                print(f"    âœ… æˆåŠŸä» {data_source} è·å– {len(source_data)} æ¡æ•°æ®")
            else:
                print(f"    âš ï¸  æ•°æ®æº {data_source} æš‚æ— æ•°æ®")
        
        if not all_metric_data:
            print(f"  è­¦å‘Š: æœªèƒ½ä»ä»»ä½•æ•°æ®æº {data_sources} ä¸­è¯»å–åˆ°æ•°æ®")
            # åˆ›å»ºç©ºçš„æ•°æ®æ–‡ä»¶ï¼Œé¿å…åç»­å¤„ç†å¤±è´¥
            empty_data = {
                "metric": metric_name,
                "data_sources": data_sources,
                "successful_sources": successful_sources,
                "total_items": 0,
                "results": [],
                "status": "no_data_found"
            }
            try:
                with open(metric_data_file, 'w', encoding='utf-8') as f:
                    json.dump(empty_data, f, ensure_ascii=False, indent=OUTPUT_CONFIG['json_indent'])
                return metric_data_file
            except Exception as e:
                print(f"  ä¿å­˜ç©ºæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
                return None
        
        # ä¿å­˜æ•´åˆåçš„æ•°æ®
        consolidated_data = {
            "metric": metric_name,
            "data_sources": data_sources,
            "successful_sources": successful_sources,
            "total_items": len(all_metric_data),
            "results": all_metric_data,
            "status": "success"
        }
        
        try:
            with open(metric_data_file, 'w', encoding='utf-8') as f:
                json.dump(consolidated_data, f, ensure_ascii=False, indent=OUTPUT_CONFIG['json_indent'])
            print(f"  æˆåŠŸæ•´åˆæ•°æ®åˆ°: {metric_data_file}")
            return metric_data_file
        except Exception as e:
            print(f"  ä¿å­˜æ•´åˆæ•°æ®å¤±è´¥: {e}")
            return None
    
    def extract_text_content(self, data_file_path):
        """ä»JSONæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹ä¾›LLMä½¿ç”¨"""
        try:
            with open(data_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            text_list = []
            
            def extract_strings(value):
                if isinstance(value, str):
                    text_list.append(value)
                elif isinstance(value, list):
                    for item in value:
                        extract_strings(item)
                elif isinstance(value, dict):
                    for v in value.values():
                        extract_strings(v)
            
            if 'results' in data:
                extract_strings(data['results'])
            else:
                extract_strings(data)
            
            return "\n".join(text_list)
            
        except Exception as e:
            print(f"æå–æ–‡æœ¬å†…å®¹å¤±è´¥: {e}")
            return ""
    
    def get_data_source_info(self):
        """è·å–æ•°æ®æºä¿¡æ¯å’ŒçŠ¶æ€"""
        info = {
            'configured_sources': len(DATA_SOURCES),
            'existing_sources': 0,
            'missing_sources': [],
            'source_details': {}
        }
        
        for source_name, source_path in DATA_SOURCES.items():
            # ğŸ”¥ ä¿®æ­£è·¯å¾„è®¡ç®— ğŸ”¥
            if source_path.startswith('../data/'):
                clean_path = source_path.replace('../data/', '')
            else:
                clean_path = source_path
            
            full_path = os.path.join(self.data_dir, clean_path)
            exists = os.path.exists(full_path)
            
            if exists:
                info['existing_sources'] += 1
                # ç»Ÿè®¡è¯¥æ•°æ®æºä¸­çš„æ–‡ä»¶æ•°é‡
                file_count = 0
                if os.path.isdir(full_path):
                    for root, _, files in os.walk(full_path):
                        file_count += len([f for f in files if f.endswith('.json')])
                
                info['source_details'][source_name] = {
                    'path': full_path,
                    'exists': True,
                    'file_count': file_count
                }
            else:
                info['missing_sources'].append(source_name)
                info['source_details'][source_name] = {
                    'path': full_path,
                    'exists': False,
                    'file_count': 0
                }
        
        return info