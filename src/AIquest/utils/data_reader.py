"""æ•°æ®è¯»å–å’Œæ•´åˆåŠŸèƒ½"""
import os
import json
from src.AIquest.config import (
    DATA_SOURCES, OUTPUT_CONFIG, METRIC_DATA_MAPPING, REQUIRED_DIRECTORIES,
    is_school_extraction_enabled, get_school_extraction_config, 
    get_traditional_extraction_config, get_attachment_config,
    get_consolidated_dir_path,get_consolidated_dir_name,get_output_config
)


class DataReader:
    """æ•°æ®è¯»å–å’Œæ•´åˆåŠŸèƒ½"""
    
    def __init__(self):
        # ğŸ”¥ ä¿®æ­£è·¯å¾„è®¡ç®— - dataç›®å½•å’ŒAIqueståŒçº§ ğŸ”¥
        self.current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # src/AIquest
        self.src_dir = os.path.dirname(self.current_dir)  # src
        self.data_dir = os.path.join(self.src_dir, 'data')  # src/data
        self._ensure_directories_exist()
        
        print(f"ğŸ“ DataReaderè·¯å¾„ä¿¡æ¯:")
        print(f"  AIquestç›®å½•: {self.current_dir}")
        print(f"  srcç›®å½•: {self.src_dir}")
        print(f"  dataç›®å½•: {self.data_dir}")

        # ğŸ”¥ æ–°å¢ï¼šæ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ•´åˆç›®å½• ğŸ”¥
        current_consolidated_dir = get_consolidated_dir_path(self.data_dir)
        mode = "æ™ºèƒ½æˆªå–æ¨¡å¼" if is_school_extraction_enabled() else "ä¼ ç»Ÿæ¨¡å¼"
        print(f"  ğŸ“‚ å½“å‰æ•´åˆç›®å½•: {current_consolidated_dir} ({mode})")
    
    def _ensure_directories_exist(self):
        """ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„ç›®å½•å­˜åœ¨ï¼ˆè·³è¿‡å·²å­˜åœ¨çš„moepoliciesï¼‰"""
        for directory in REQUIRED_DIRECTORIES:
            dir_path = os.path.join(self.data_dir, directory)
            if not os.path.exists(dir_path):
                try:
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  ğŸ“ åˆ›å»ºç›®å½•: {dir_path}")
                    
                    # ä¸ºæ–°åˆ›å»ºçš„ç›®å½•æ·»åŠ READMEæ–‡ä»¶
                    readme_path = os.path.join(dir_path, 'README.md')
                    readme_content = self._generate_readme_content(directory)
                    with open(readme_path, 'w', encoding='utf-8') as f:
                        f.write(readme_content)
                        
                except Exception as e:
                    print(f"  âŒ åˆ›å»ºç›®å½•å¤±è´¥ {dir_path}: {e}")
    
    def _generate_readme_content(self, dir_name):
        """ç”Ÿæˆç›®å½•è¯´æ˜æ–‡æ¡£"""
        descriptions = {
            'esi_subjects': 'ESIå­¦ç§‘ç›¸å…³æ•°æ®ï¼ŒåŒ…å«è¿›å…¥ESIæ’åçš„å­¦ç§‘ä¿¡æ¯',
            'esi_subjects/esi_top1percent': 'ESIå‰1%å­¦ç§‘ä¸“é—¨æ•°æ®',
            'esi_subjects/esi_top1permille': 'ESIå‰1â€°å­¦ç§‘ä¸“é—¨æ•°æ®',
            'ruanke_subjects': 'è½¯ç§‘ä¸­å›½æœ€å¥½å­¦ç§‘æ’åç›¸å…³æ•°æ®',
            'subject_evaluation': 'æ•™è‚²éƒ¨å­¦ç§‘è¯„ä¼°ç›¸å…³æ•°æ®ï¼ŒåŒ…å«A+ã€Aã€A-ç­‰è¯„çº§ä¿¡æ¯',
            'undergraduate_majors': 'æœ¬ç§‘ä¸“ä¸šç›¸å…³æ•°æ®',
            'undergraduate_majors/total_majors': 'æœ¬ç§‘ä¸“ä¸šæ€»æ•°ç»Ÿè®¡æ•°æ®',
            'undergraduate_majors/certified_majors': 'é€šè¿‡ä¸“ä¸šè®¤è¯çš„æœ¬ç§‘ä¸“ä¸šæ•°æ®',
            'undergraduate_majors/national_first_class': 'å›½å®¶çº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹æ•°æ®',
            'undergraduate_majors/provincial_first_class': 'çœçº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹æ•°æ®',
            'consolidated': 'æ•´åˆåçš„æ•°æ®æ–‡ä»¶'
        }
        
        description = descriptions.get(dir_name, 'æ•°æ®å­˜å‚¨ç›®å½•')
        
        return f"""# {dir_name} æ•°æ®ç›®å½•

## ç›®å½•è¯´æ˜
{description}

## æ•°æ®æ ¼å¼è¦æ±‚
- æ–‡ä»¶æ ¼å¼ï¼šJSON
- ç¼–ç ï¼šUTF-8
- æ–‡ä»¶å‘½åï¼šå»ºè®®ä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°ï¼Œå¦‚ `university_name.json`

## ä½¿ç”¨è¯´æ˜
1. å°†ç›¸å…³æ•°æ®æ–‡ä»¶æ”¾ç½®åœ¨æ­¤ç›®å½•ä¸­
2. ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰«æå¹¶è¯»å–æ‰€æœ‰ `.json` æ–‡ä»¶
3. æ•°æ®ä¼šè¢«è‡ªåŠ¨æ•´åˆåˆ°é—®ç­”ç³»ç»Ÿä¸­

## æ›´æ–°æ—¶é—´
{self._get_current_time()}
"""
    
    def _get_current_time(self):
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def read_data_from_source(self, source_name):
        """ä»æŒ‡å®šæ•°æ®æºè¯»å–æ•°æ®"""
        if source_name not in DATA_SOURCES:
            print(f"è­¦å‘Š: æœªçŸ¥çš„æ•°æ®æº: {source_name}")
            return []
        
        # ğŸ”¥ ä¿®æ­£è·¯å¾„è®¡ç®— - åŸºäºsrc/dataç›®å½• ğŸ”¥
        source_relative_path = DATA_SOURCES[source_name]
        
        # ç§»é™¤è·¯å¾„å‰ç¼€å¹¶è®¡ç®—æ­£ç¡®è·¯å¾„
        if source_relative_path.startswith('../data/'):
            clean_path = source_relative_path.replace('../data/', '')
        else:
            clean_path = source_relative_path
        
        source_path = os.path.join(self.data_dir, clean_path)
        print(f"    æ­£åœ¨è¯»å–æ•°æ®æº: {source_path}")
        
        all_data = []
        
        if not os.path.exists(source_path):
            print(f"    è­¦å‘Š: æ•°æ®æºè·¯å¾„ä¸å­˜åœ¨: {source_path}")
            # ğŸ”¥ æ·»åŠ è°ƒè¯•ä¿¡æ¯ ğŸ”¥
            print(f"    è°ƒè¯•ä¿¡æ¯:")
            print(f"      é…ç½®çš„æºè·¯å¾„: {source_relative_path}")
            print(f"      æ¸…ç†åçš„è·¯å¾„: {clean_path}")
            print(f"      dataç›®å½•: {self.data_dir}")
            print(f"      æœ€ç»ˆè·¯å¾„: {source_path}")
            print(f"      ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(source_path)}")
            
            # ğŸ”¥ åˆ—å‡ºdataç›®å½•å†…å®¹ ğŸ”¥
            if os.path.exists(self.data_dir):
                print(f"    dataç›®å½•å†…å®¹:")
                for item in os.listdir(self.data_dir):
                    item_path = os.path.join(self.data_dir, item)
                    if os.path.isdir(item_path):
                        print(f"      ğŸ“ {item}/")
                        # å¦‚æœæ˜¯subject_evaluationç›®å½•ï¼Œåˆ—å‡ºå…¶å†…å®¹
                        if item == 'subject_evaluation':
                            try:
                                sub_items = os.listdir(item_path)
                                json_files = [f for f in sub_items if f.endswith('.json')]
                                print(f"        ğŸ“„ JSONæ–‡ä»¶: {len(json_files)} ä¸ª")
                                for json_file in json_files[:3]:
                                    print(f"          - {json_file}")
                            except Exception as e:
                                print(f"        âŒ æ— æ³•è¯»å–ç›®å½•å†…å®¹: {e}")
                    else:
                        print(f"      ğŸ“„ {item}")
            else:
                print(f"    âŒ dataç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            
            return all_data
        
        # é€’å½’è¯»å–æ‰€æœ‰JSONæ–‡ä»¶
        file_count = 0
        for root, _, files in os.walk(source_path):
            for file_name in files:
                if file_name.endswith('.json') and not file_name.startswith('combined_'):
                    file_path = os.path.join(root, file_name)
                    print(f"      è¯»å–æ–‡ä»¶: {file_path}")
                    data = self._read_json_file(file_path)
                    if data:
                        normalized_data = self._normalize_json_data(data, file_path)
                        all_data.extend(normalized_data)
                        file_count += 1
        
        print(f"    ä» {source_path} è¯»å–åˆ° {len(all_data)} æ¡æ•°æ®ï¼Œå¤„ç†äº† {file_count} ä¸ªJSONæ–‡ä»¶")
        return all_data
    
    def _read_json_file(self, file_path):
        """è¯»å–å•ä¸ªJSONæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            print(f"      è­¦å‘Š: {file_path} ä¸æ˜¯æœ‰æ•ˆçš„JSONæ–‡ä»¶: {e}")
            return None
        except Exception as e:
            print(f"      é”™è¯¯: è¯»å– {file_path} å¤±è´¥: {e}")
            return None
    
    def _normalize_json_data(self, data, file_path):
        """æ ‡å‡†åŒ–JSONæ•°æ®æ ¼å¼ï¼ŒåŒ…å«é™„ä»¶å†…å®¹æå–"""
        if isinstance(data, list):
            result = []
            for item in data:
                if isinstance(item, dict):
                    # ğŸ”¥ æå–é™„ä»¶å†…å®¹ ğŸ”¥
                    attachment_content = self._extract_attachments_content(item, file_path)
                    if attachment_content:
                        # å°†é™„ä»¶å†…å®¹åˆå¹¶åˆ°ç°æœ‰å†…å®¹ä¸­
                        if 'content' in item and isinstance(item['content'], dict):
                            if 'æ­£æ–‡å†…å®¹' in item['content']:
                                item['content']['æ­£æ–‡å†…å®¹'] += f"\n\n--- é™„ä»¶å†…å®¹ ---\n{attachment_content}"
                            else:
                                item['content']['é™„ä»¶å†…å®¹'] = attachment_content
                        else:
                            item['é™„ä»¶å†…å®¹'] = attachment_content
                    
                    result.append(item)
                else:
                    result.append({"__file_source": file_path, "value": item})
            return result
            
        elif isinstance(data, dict):
            # ğŸ”¥ æå–é™„ä»¶å†…å®¹ ğŸ”¥
            attachment_content = self._extract_attachments_content(data, file_path)
            if attachment_content:
                # å°†é™„ä»¶å†…å®¹åˆå¹¶åˆ°ç°æœ‰å†…å®¹ä¸­
                if 'content' in data and isinstance(data['content'], dict):
                    if 'æ­£æ–‡å†…å®¹' in data['content']:
                        data['content']['æ­£æ–‡å†…å®¹'] += f"\n\n--- é™„ä»¶å†…å®¹ ---\n{attachment_content}"
                    else:
                        data['content']['é™„ä»¶å†…å®¹'] = attachment_content
                else:
                    data['é™„ä»¶å†…å®¹'] = attachment_content
            
            return [data]
        else:
            return [{"__file_source": file_path, "value": data}]
    
    def consolidate_data_for_metric(self, metric_name, data_sources=None):
        """ä¸ºç‰¹å®šæŒ‡æ ‡æ•´åˆæ•°æ®ï¼Œæ”¯æŒä¼˜å…ˆçº§"""
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®æºï¼Œä½¿ç”¨é…ç½®ä¸­çš„ä¼˜å…ˆçº§åˆ—è¡¨
        if data_sources is None:
            data_sources = METRIC_DATA_MAPPING.get(metric_name, [])
            
        if not data_sources:
            print(f"  é”™è¯¯: æœªæ‰¾åˆ°æŒ‡æ ‡ '{metric_name}' å¯¹åº”çš„æ•°æ®æºé…ç½®")
            return None
        
        print(f"  æ­£åœ¨ä¸ºæŒ‡æ ‡ '{metric_name}' æ•´åˆæ•°æ®æº: {data_sources}")
        
        # ğŸ”¥ ä½¿ç”¨åŠ¨æ€ç›®å½•è·¯å¾„ ğŸ”¥
        consolidated_dir = get_consolidated_dir_path(self.data_dir)
        dir_mode = "æ™ºèƒ½æˆªå–æ¨¡å¼" if is_school_extraction_enabled() else "ä¼ ç»Ÿæ¨¡å¼"
        print(f"  ğŸ“‚ ä½¿ç”¨æ•´åˆç›®å½•: {consolidated_dir} ({dir_mode})")

        os.makedirs(consolidated_dir, exist_ok=True)
        
        # ğŸ”¥ æ–‡ä»¶ååŒ…å«æ¨¡å¼æ ‡è¯† ğŸ”¥
        mode_suffix = "_intelligent" if is_school_extraction_enabled() else "_traditional"
        metric_data_file = os.path.join(consolidated_dir, f"{metric_name}{mode_suffix}_data.json")

        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ¨¡å¼çš„æ–‡ä»¶ ğŸ”¥
        if os.path.exists(metric_data_file):
            file_age = os.path.getmtime(metric_data_file)
            from datetime import datetime
            age_str = datetime.fromtimestamp(file_age).strftime("%Y-%m-%d %H:%M:%S")
            print(f"  â„¹ï¸  å‘ç°å·²å­˜åœ¨çš„æ–‡ä»¶: {os.path.basename(metric_data_file)} (ä¿®æ”¹æ—¶é—´: {age_str})")
            
            # å¯ä»¥é€‰æ‹©æ˜¯å¦é‡æ–°ç”Ÿæˆ
            # è¿™é‡Œé»˜è®¤é‡æ–°ç”Ÿæˆï¼Œæ‚¨ä¹Ÿå¯ä»¥æ·»åŠ å‚æ•°æ§åˆ¶
        
        # æŒ‰ä¼˜å…ˆçº§æ”¶é›†æ•°æ®
        all_metric_data = []
        successful_sources = []
        
        for data_source in data_sources:
            source_data = self.read_data_from_source(data_source)
            if source_data:
                all_metric_data.extend(source_data)
                successful_sources.append(data_source)
                print(f"    âœ… æˆåŠŸä» {data_source} è·å– {len(source_data)} æ¡æ•°æ®")
            else:
                print(f"    âš ï¸  æ•°æ®æº {data_source} æš‚æ— æ•°æ®")
        
        if not all_metric_data:
            print(f"  è­¦å‘Š: æœªèƒ½ä»ä»»ä½•æ•°æ®æº {data_sources} ä¸­è¯»å–åˆ°æ•°æ®")
            # åˆ›å»ºç©ºçš„æ•°æ®æ–‡ä»¶ï¼Œé¿å…åç»­å¤„ç†å¤±è´¥
            empty_data = {
                "metric": metric_name,
                "processing_mode": "intelligent" if is_school_extraction_enabled() else "traditional",
                "data_sources": data_sources,
                "successful_sources": successful_sources,
                "total_items": 0,
                "results": [],
                "status": "no_data_found",
                "generated_at": self._get_current_time()
            }
            try:
                output_config = get_output_config(self.data_dir)
                with open(metric_data_file, 'w', encoding='utf-8') as f:
                    json.dump(empty_data, f, ensure_ascii=False, indent=output_config['json_indent'])
                return metric_data_file
            except Exception as e:
                print(f"  ä¿å­˜ç©ºæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
                return None
        
        # ä¿å­˜æ•´åˆåçš„æ•°æ®
        consolidated_data = {
            "metric": metric_name,
            "processing_mode": "intelligent" if is_school_extraction_enabled() else "traditional",  # ğŸ”¥ æ–°å¢ï¼šæ ‡è¯†å¤„ç†æ¨¡å¼ ğŸ”¥
            "data_sources": data_sources,
            "successful_sources": successful_sources,
            "total_items": len(all_metric_data),
            "results": all_metric_data,
            "status": "success",
            "generated_at": self._get_current_time(),  # ğŸ”¥ æ–°å¢ï¼šç”Ÿæˆæ—¶é—´ ğŸ”¥
            "attachment_extraction_enabled": is_school_extraction_enabled()  # ğŸ”¥ æ–°å¢ï¼šè®°å½•é™„ä»¶å¤„ç†æ¨¡å¼ ğŸ”¥
        }
        
        try:
            output_config = get_output_config(self.data_dir)
            with open(metric_data_file, 'w', encoding='utf-8') as f:
                json.dump(consolidated_data, f, ensure_ascii=False, indent=output_config['json_indent'])
            print(f"  âœ… æˆåŠŸæ•´åˆæ•°æ®åˆ°: {metric_data_file}")
            print(f"  ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(all_metric_data)} æ¡è®°å½•ï¼Œå¤„ç†æ¨¡å¼: {dir_mode}")
            return metric_data_file
        except Exception as e:
            print(f"  ä¿å­˜æ•´åˆæ•°æ®å¤±è´¥: {e}")
            return None
        
    def find_existing_consolidated_file(self, metric_name):
        """ğŸ”¥ æ–°å¢ï¼šæŸ¥æ‰¾å·²å­˜åœ¨çš„æ•´åˆæ–‡ä»¶ï¼ˆä¼˜å…ˆå½“å‰æ¨¡å¼ï¼Œåå¤‡å…¶ä»–æ¨¡å¼ï¼‰ğŸ”¥"""
        # å½“å‰æ¨¡å¼çš„æ–‡ä»¶
        current_consolidated_dir = get_consolidated_dir_path(self.data_dir)
        current_mode_suffix = "_intelligent" if is_school_extraction_enabled() else "_traditional"
        current_file = os.path.join(current_consolidated_dir, f"{metric_name}{current_mode_suffix}_data.json")
        
        if os.path.exists(current_file):
            print(f"  âœ… æ‰¾åˆ°å½“å‰æ¨¡å¼çš„æ–‡ä»¶: {os.path.basename(current_file)}")
            return current_file
        
        # æŸ¥æ‰¾å…¶ä»–æ¨¡å¼çš„æ–‡ä»¶ä½œä¸ºå¤‡é€‰
        other_mode = "traditional" if is_school_extraction_enabled() else "intelligent"
        other_dir = os.path.join(self.data_dir, f"consolidated_{other_mode}" if other_mode == "intelligent" else "consolidated")
        other_mode_suffix = "_traditional" if current_mode_suffix == "_intelligent" else "_intelligent"
        other_file = os.path.join(other_dir, f"{metric_name}{other_mode_suffix}_data.json")
        
        if os.path.exists(other_file):
            print(f"  âš ï¸  å½“å‰æ¨¡å¼æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰¾åˆ°å…¶ä»–æ¨¡å¼æ–‡ä»¶: {os.path.basename(other_file)}")
            print(f"  ğŸ’¡ å»ºè®®é‡æ–°ç”Ÿæˆå½“å‰æ¨¡å¼çš„æ•°æ®æ–‡ä»¶")
            return other_file
        
        # æŸ¥æ‰¾ä¸å¸¦æ¨¡å¼åç¼€çš„æ—§æ–‡ä»¶
        legacy_files = [
            os.path.join(current_consolidated_dir, f"{metric_name}_data.json"),
            os.path.join(self.data_dir, "consolidated", f"{metric_name}_data.json")
        ]
        
        for legacy_file in legacy_files:
            if os.path.exists(legacy_file):
                print(f"  âš ï¸  æ‰¾åˆ°æ—§æ ¼å¼æ–‡ä»¶: {os.path.basename(legacy_file)}")
                print(f"  ğŸ’¡ å»ºè®®é‡æ–°ç”Ÿæˆä»¥ä½¿ç”¨æ–°çš„å¤„ç†æ¨¡å¼")
                return legacy_file
        
        print(f"  âŒ æœªæ‰¾åˆ°æŒ‡æ ‡ '{metric_name}' çš„ä»»ä½•æ•´åˆæ–‡ä»¶")
        return None
    
    def get_consolidated_file_info(self, metric_name):
        """ğŸ”¥ æ–°å¢ï¼šè·å–æ•´åˆæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯ ğŸ”¥"""
        current_file = self.find_existing_consolidated_file(metric_name)
        if not current_file:
            return None
        
        try:
            with open(current_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_stat = os.stat(current_file)
            from datetime import datetime
            
            info = {
                'file_path': current_file,
                'file_name': os.path.basename(current_file),
                'file_size': file_stat.st_size,
                'modified_time': datetime.fromtimestamp(file_stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S"),
                'processing_mode': data.get('processing_mode', 'unknown'),
                'attachment_extraction': data.get('attachment_extraction_enabled', 'unknown'),
                'total_items': data.get('total_items', 0),
                'status': data.get('status', 'unknown'),
                'data_sources': data.get('successful_sources', [])
            }
            
            return info
            
        except Exception as e:
            print(f"  âŒ è¯»å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def list_all_consolidated_files(self):
        """ğŸ”¥ æ–°å¢ï¼šåˆ—å‡ºæ‰€æœ‰æ•´åˆæ–‡ä»¶ ğŸ”¥"""
        all_files = {}
        
        # æ£€æŸ¥ä¸¤ä¸ªç›®å½•
        directories = [
            ('traditional', os.path.join(self.data_dir, 'consolidated')),
            ('intelligent', os.path.join(self.data_dir, 'consolidated_intelligent'))
        ]
        
        for mode, dir_path in directories:
            if os.path.exists(dir_path):
                for filename in os.listdir(dir_path):
                    if filename.endswith('_data.json'):
                        file_path = os.path.join(dir_path, filename)
                        
                        # æå–æŒ‡æ ‡åç§°
                        if filename.endswith('_traditional_data.json'):
                            metric_name = filename.replace('_traditional_data.json', '')
                        elif filename.endswith('_intelligent_data.json'):
                            metric_name = filename.replace('_intelligent_data.json', '')
                        else:
                            metric_name = filename.replace('_data.json', '')
                        
                        if metric_name not in all_files:
                            all_files[metric_name] = {}
                        
                        # è·å–æ–‡ä»¶ä¿¡æ¯
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                            
                            file_stat = os.stat(file_path)
                            from datetime import datetime
                            
                            all_files[metric_name][mode] = {
                                'file_path': file_path,
                                'file_name': filename,
                                'total_items': data.get('total_items', 0),
                                'status': data.get('status', 'unknown'),
                                'modified_time': datetime.fromtimestamp(file_stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
                            }
                        except:
                            all_files[metric_name][mode] = {
                                'file_path': file_path,
                                'file_name': filename,
                                'error': 'Failed to read file'
                            }
        
        return all_files
    
    def extract_text_content(self, data_file_path):
        """ä»JSONæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹ä¾›LLMä½¿ç”¨"""
        try:
            with open(data_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            text_list = []
            
            def extract_strings(value, path=""):
                if isinstance(value, str):
                    # ğŸ”¥ è¿‡æ»¤æ–‡ä»¶è·¯å¾„ï¼Œä¿ç•™æ‰€æœ‰å…¶ä»–å­—ç¬¦ä¸² ğŸ”¥
                    if not (len(value) > 20 and ('\\' in value or '/' in value) and value.endswith('.json')):
                        text_list.append(value)
                elif isinstance(value, (int, float)):
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒ…å«æ‰€æœ‰æ•°å­—æ•°æ® ğŸ”¥
                    text_list.append(str(value))
                elif isinstance(value, list):
                    for i, item in enumerate(value):
                        extract_strings(item, f"{path}[{i}]")
                elif isinstance(value, dict):
                    for k, v in value.items():
                        if k != '__file_source':
                            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿ç•™å­—æ®µåç§°ï¼Œä¾¿äºLLMç†è§£æ•°æ®ç»“æ„ ğŸ”¥
                            if isinstance(v, (str, int, float)):
                                text_list.append(f"{k}: {v}")
                            else:
                                text_list.append(f"{k}:")  # æ·»åŠ å­—æ®µå
                                extract_strings(v, f"{path}.{k}" if path else k)
            
            if 'results' in data:
                print(f"    ğŸ“Š æå–resultsä¸­çš„å†…å®¹ï¼Œå…±{len(data['results'])}é¡¹")
                extract_strings(data['results'])
            else:
                print(f"    ğŸ“Š æå–æ•´ä¸ªæ•°æ®ç»“æ„")
                extract_strings(data)
            
            extracted_text = "\n".join(text_list)
            print(f"    ğŸ“„ æå–åˆ°æ–‡æœ¬é•¿åº¦: {len(extracted_text)} å­—ç¬¦")
            
            return extracted_text
            
        except Exception as e:
            print(f"    âŒ æå–æ–‡æœ¬å†…å®¹å¤±è´¥: {e}")
            return ""
    
    def get_data_source_info(self):
        """è·å–æ•°æ®æºä¿¡æ¯å’ŒçŠ¶æ€"""
        info = {
            'configured_sources': len(DATA_SOURCES),
            'existing_sources': 0,
            'missing_sources': [],
            'source_details': {}
        }
        
        for source_name, source_path in DATA_SOURCES.items():
            # ğŸ”¥ ä¿®æ­£è·¯å¾„è®¡ç®— ğŸ”¥
            if source_path.startswith('../data/'):
                clean_path = source_path.replace('../data/', '')
            else:
                clean_path = source_path
            
            full_path = os.path.join(self.data_dir, clean_path)
            exists = os.path.exists(full_path)
            
            if exists:
                info['existing_sources'] += 1
                # ç»Ÿè®¡è¯¥æ•°æ®æºä¸­çš„æ–‡ä»¶æ•°é‡
                file_count = 0
                if os.path.isdir(full_path):
                    for root, _, files in os.walk(full_path):
                        file_count += len([f for f in files if f.endswith('.json')])
                
                info['source_details'][source_name] = {
                    'path': full_path,
                    'exists': True,
                    'file_count': file_count
                }
            else:
                info['missing_sources'].append(source_name)
                info['source_details'][source_name] = {
                    'path': full_path,
                    'exists': False,
                    'file_count': 0
                }
        
        return info
    
    def _extract_attachments_content(self, data_item, file_path):
        """æå–æ•°æ®é¡¹ä¸­çš„é™„ä»¶å†…å®¹ - æ”¯æŒå¯é€‰çš„æ™ºèƒ½æˆªå–æ¨¡å¼"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é™„ä»¶å¤„ç†
        attachment_config = get_attachment_config()
        if not attachment_config.get('enable_attachment_processing', True):
            print(f"    âš ï¸  é™„ä»¶å¤„ç†å·²ç¦ç”¨")
            return ""
        
        attachment_contents = []
        
        if 'content' in data_item and 'é™„ä»¶' in data_item['content']:
            attachments = data_item['content']['é™„ä»¶']
            if isinstance(attachments, list):
                base_path = os.path.dirname(file_path)
                
                for attachment in attachments:
                    if isinstance(attachment, dict) and attachment.get('download_status') == 'success':
                        local_path = attachment.get('local_path', '')
                        if local_path:
                            full_path = os.path.join(base_path, local_path)
                            
                            if os.path.exists(full_path):
                                file_ext = os.path.splitext(full_path)[1].lower()
                                attachment_name = attachment.get('name', 'æœªçŸ¥é™„ä»¶')
                                
                                # æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
                                if file_ext not in attachment_config.get('supported_formats', []):
                                    print(f"    âš ï¸  ä¸æ”¯æŒçš„é™„ä»¶æ ¼å¼: {file_ext}")
                                    continue
                                
                                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                                try:
                                    file_size = os.path.getsize(full_path)
                                    max_size = attachment_config.get('max_attachment_size', 50 * 1024 * 1024)
                                    if file_size > max_size:
                                        print(f"    âš ï¸  é™„ä»¶è¿‡å¤§: {file_size} bytes (é™åˆ¶: {max_size} bytes)")
                                        continue
                                except:
                                    pass
                                
                                try:
                                    # è¯»å–åŸå§‹å†…å®¹
                                    raw_content = self._read_attachment_by_type(full_path, file_ext)
                                    
                                    if raw_content:
                                        # ğŸ”¥ æ ¹æ®é…ç½®é€‰æ‹©å¤„ç†æ¨¡å¼ ğŸ”¥
                                        if is_school_extraction_enabled():
                                            print(f"    ğŸ“ ä½¿ç”¨æ™ºèƒ½æˆªå–æ¨¡å¼å¤„ç†é™„ä»¶: {attachment_name}")
                                            extracted_content = self._extract_school_relevant_content(raw_content, attachment_name)
                                        else:
                                            print(f"    ğŸ“„ ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼å¤„ç†é™„ä»¶: {attachment_name}")
                                            extracted_content = self._extract_traditional_content(raw_content, attachment_name)
                                        
                                        if extracted_content and len(extracted_content) > 20:
                                            formatted_content = f"\n=== é™„ä»¶ï¼š{attachment_name} ===\n{extracted_content}\n=== é™„ä»¶ç»“æŸ ===\n"
                                            attachment_contents.append(formatted_content)
                                            print(f"    âœ… æˆåŠŸæå–é™„ä»¶å†…å®¹: {attachment_name} ({len(extracted_content)} å­—ç¬¦)")
                                        else:
                                            print(f"    âš ï¸  é™„ä»¶å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­: {attachment_name}")
                                    else:
                                        print(f"    âš ï¸  æ— æ³•è¯»å–é™„ä»¶å†…å®¹: {attachment_name}")
                                        
                                except Exception as e:
                                    print(f"    âŒ å¤„ç†é™„ä»¶ {attachment_name} æ—¶å‡ºé”™: {e}")
                            else:
                                print(f"    âš ï¸  é™„ä»¶æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
        
        return "\n".join(attachment_contents)
    
    def _read_attachment_by_type(self, file_path, file_ext):
        """æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–é™„ä»¶å†…å®¹"""
        try:
            if file_ext == '.pdf':
                return self._read_pdf_attachment(file_path)
            elif file_ext in ['.docx', '.doc']:
                return self._read_docx_attachment(file_path)
            elif file_ext == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                print(f"    âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
                return ""
        except Exception as e:
            print(f"    âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return ""
    
    def _extract_traditional_content(self, raw_content, attachment_name):
        """ä¼ ç»Ÿæ¨¡å¼ï¼šæå–é™„ä»¶çš„å®Œæ•´å†…å®¹ï¼ˆå¸¦é•¿åº¦é™åˆ¶å’Œæ¸…ç†ï¼‰"""
        if not raw_content or not isinstance(raw_content, str):
            return ""
        
        print(f"    ğŸ“„ ä¼ ç»Ÿæ¨¡å¼å¤„ç†é™„ä»¶: {attachment_name}")
        
        # è·å–ä¼ ç»Ÿå¤„ç†é…ç½®
        traditional_config = get_traditional_extraction_config()
        
        # åŸºç¡€æ¸…ç†
        cleaned_content = raw_content
        
        # æ¸…ç†HTMLæ ‡ç­¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if traditional_config.get('clean_html_tags', True):
            cleaned_content = self._remove_html_tags(cleaned_content)
        
        # æ¸…ç†å¤šä½™ç©ºç™½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if traditional_config.get('remove_extra_whitespace', True):
            cleaned_content = self._basic_clean_text(cleaned_content)
        
        # é™åˆ¶å†…å®¹é•¿åº¦
        max_length = traditional_config.get('max_content_length', 10000)
        if len(cleaned_content) > max_length:
            cleaned_content = cleaned_content[:max_length]
            print(f"    âœ‚ï¸  å†…å®¹è¿‡é•¿ï¼Œæˆªå–å‰ {max_length} å­—ç¬¦")
        
        return cleaned_content
    
    def _remove_html_tags(self, text):
        """ç§»é™¤HTMLæ ‡ç­¾"""
        if not text:
            return ""
        
        import re
        # ç§»é™¤HTMLæ ‡ç­¾
        clean_text = re.sub(r'<[^>]+>', '', text)
        # è§£ç HTMLå®ä½“
        try:
            from html import unescape
            clean_text = unescape(clean_text)
        except:
            pass
        
        return clean_text
    
    def _extract_school_relevant_content(self, raw_content, attachment_name):
        """åŸºäºå­¦æ ¡åç§°æå–ç›¸å…³å†…å®¹ç‰‡æ®µ - æ”¯æŒé…ç½®åŒ–"""
        if not raw_content or not isinstance(raw_content, str):
            return ""
        
        # è·å–å­¦æ ¡æˆªå–é…ç½®
        school_config = get_school_extraction_config()
        
        # è·å–å­¦æ ¡åˆ—è¡¨
        school_list = self._get_school_list(school_config.get('school_list_source', 'csv'))
        
        if not school_list:
            print(f"    âš ï¸  æœªèƒ½è·å–å­¦æ ¡åˆ—è¡¨ï¼Œåˆ‡æ¢åˆ°ä¼ ç»Ÿæ¨¡å¼")
            return self._extract_traditional_content(raw_content, attachment_name)
        
        print(f"    ğŸ” æ™ºèƒ½æˆªå–æ¨¡å¼ï¼šåœ¨é™„ä»¶ {attachment_name} ä¸­æœç´¢ {len(school_list)} ä¸ªå­¦æ ¡çš„ç›¸å…³å†…å®¹...")
        
        # æ¸…ç†åŸå§‹å†…å®¹
        cleaned_content = self._basic_clean_text(raw_content)
        
        extracted_segments = []
        found_schools = []
        
        for school_name in school_list:
            # æŸ¥æ‰¾å­¦æ ¡ç›¸å…³ç‰‡æ®µ
            school_segments = self._find_school_segments_configurable(cleaned_content, school_name, school_config)
            if school_segments:
                found_schools.append(school_name)
                extracted_segments.extend(school_segments)
                print(f"    ğŸ“ æ‰¾åˆ° {school_name}ï¼Œå…± {len(school_segments)} å¤„")
        
        if extracted_segments:
            print(f"    âœ… æ‰¾åˆ° {len(found_schools)} ä¸ªå­¦æ ¡çš„ç›¸å…³å†…å®¹ï¼š{', '.join(found_schools[:5])}{'...' if len(found_schools) > 5 else ''}")
            
            # æŒ‰ä½ç½®æ’åºå¹¶æ„å»ºæœ€ç»ˆå†…å®¹
            extracted_segments.sort(key=lambda x: x['position'])
            
            final_content_parts = []
            for segment in extracted_segments:
                final_content_parts.append(
                    f"ğŸ“ {segment['school']}ï¼š{segment['content']}..."
                )
            
            return "\n\n".join(final_content_parts)
        else:
            print(f"    âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡å­¦æ ¡ï¼Œåˆ‡æ¢åˆ°ä¼ ç»Ÿæ¨¡å¼")
            return self._extract_traditional_content(raw_content, attachment_name)
    
    def _find_school_segments_configurable(self, content, school_name, config):
        """æ ¹æ®é…ç½®æŸ¥æ‰¾å­¦æ ¡ç‰‡æ®µ"""
        segments = []
        
        if not content or not school_name:
            return segments
        
        # è·å–é…ç½®å‚æ•°
        chars_after = config.get('characters_after_school', 100)
        chars_before = config.get('characters_before_school', 0)
        max_segments = config.get('max_segments_per_school', 10)
        min_length = config.get('min_segment_length', 10)
        
        start_pos = 0
        segment_count = 0
        
        while segment_count < max_segments:
            pos = content.find(school_name, start_pos)
            if pos == -1:
                break
            
            # è®¡ç®—ç‰‡æ®µèŒƒå›´
            school_start_pos = pos
            school_end_pos = pos + len(school_name)
            
            segment_start = max(0, school_start_pos - chars_before)
            segment_end = min(len(content), school_end_pos + chars_after)
            
            # æå–ç‰‡æ®µ
            segment_content = content[segment_start:segment_end]
            cleaned_segment = self._clean_segment_text(segment_content)
            
            if cleaned_segment and len(cleaned_segment.strip()) >= min_length:
                segments.append({
                    'school': school_name,
                    'position': pos,
                    'content': cleaned_segment,
                    'start': segment_start,
                    'end': segment_end
                })
                segment_count += 1
            
            start_pos = pos + len(school_name)
        
        return segments
    
    def _get_school_list(self, source_type='csv'):
        """è·å–å­¦æ ¡åˆ—è¡¨ - ä¿®å¤è·¯å¾„è®¡ç®—"""
        school_list = []
        
        print(f"    ğŸ” å°è¯•è·å–å­¦æ ¡åˆ—è¡¨ï¼Œæ¥æºç±»å‹: {source_type}")
        
        try:
            if source_type == 'csv':
                # ğŸ”¥ ä¿®å¤CSVæ–‡ä»¶è·¯å¾„è®¡ç®— ğŸ”¥
                # å½“å‰æ–‡ä»¶: scrapetest/src/AIquest/utils/data_reader.py
                # CSVæ–‡ä»¶: scrapetest/ai_evaluation_dataset_long.csv
                
                current_file_dir = os.path.dirname(os.path.abspath(__file__))  # utilsç›®å½•
                aiquest_dir = os.path.dirname(current_file_dir)  # AIquestç›®å½•  
                src_dir = os.path.dirname(aiquest_dir)  # srcç›®å½•
                project_root = os.path.dirname(src_dir)  # scrapetestç›®å½• (é¡¹ç›®æ ¹ç›®å½•)
                
                csv_file = os.path.join(project_root, "ai_evaluation_dataset_long.csv")
                
                print(f"    ğŸ“‚ è·¯å¾„è®¡ç®—è¯¦æƒ…:")
                print(f"      å½“å‰æ–‡ä»¶: {os.path.abspath(__file__)}")
                print(f"      utilsç›®å½•: {current_file_dir}")
                print(f"      AIquestç›®å½•: {aiquest_dir}")
                print(f"      srcç›®å½•: {src_dir}")
                print(f"      é¡¹ç›®æ ¹ç›®å½•: {project_root}")
                print(f"      CSVæ–‡ä»¶è·¯å¾„: {csv_file}")
                print(f"      CSVæ–‡ä»¶å­˜åœ¨: {os.path.exists(csv_file)}")
                
                # ğŸ”¥ å¦‚æœä¸»è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•å¤‡é€‰è·¯å¾„ ğŸ”¥
                if not os.path.exists(csv_file):
                    print(f"    ğŸ” ä¸»è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•å¤‡é€‰è·¯å¾„:")
                    alternative_paths = [
                        # å¯èƒ½åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ä¸åŒä½ç½®
                        os.path.join(project_root, "data", "ai_evaluation_dataset_long.csv"),
                        os.path.join(src_dir, "ai_evaluation_dataset_long.csv"),
                        os.path.join(src_dir, "data", "ai_evaluation_dataset_long.csv"),
                        # å¯èƒ½åœ¨ä¸Šçº§ç›®å½•
                        os.path.join(os.path.dirname(project_root), "ai_evaluation_dataset_long.csv"),
                        # ç»å¯¹è·¯å¾„ï¼ˆå¦‚æœæ‚¨ç¡®å®šæ–‡ä»¶åœ¨ç‰¹å®šä½ç½®ï¼‰
                        r"c:\Users\83789\PycharmProjects\scrapetest\ai_evaluation_dataset_long.csv"
                    ]
                    
                    for i, alt_path in enumerate(alternative_paths, 1):
                        exists = os.path.exists(alt_path)
                        print(f"      å¤‡é€‰è·¯å¾„{i}: {alt_path} ({'âœ…å­˜åœ¨' if exists else 'âŒä¸å­˜åœ¨'})")
                        if exists:
                            csv_file = alt_path
                            break
                
                # å¦‚æœæ‰¾åˆ°CSVæ–‡ä»¶ï¼Œå°è¯•è¯»å–
                if os.path.exists(csv_file):
                    try:
                        import csv
                        with open(csv_file, 'r', encoding='utf-8') as f:
                            reader = csv.DictReader(f)
                            schools = set()
                            row_count = 0
                            
                            # è¯»å–å‰å‡ è¡Œæ¥è°ƒè¯•
                            sample_rows = []
                            for row in reader:
                                row_count += 1
                                if row_count <= 3:
                                    sample_rows.append(dict(row))
                                
                                school_name = row.get('å­¦æ ¡åç§°', '').strip()
                                if school_name and school_name != 'å­¦æ ¡åç§°' and school_name != '':
                                    schools.add(school_name)
                            
                            school_list = list(schools)
                            
                            print(f"    ğŸ“Š CSVæ–‡ä»¶è¯»å–ç»“æœ:")
                            print(f"      æ€»è¡Œæ•°: {row_count}")
                            print(f"      å”¯ä¸€å­¦æ ¡æ•°: {len(school_list)}")
                            print(f"      å‰3è¡Œç¤ºä¾‹: {sample_rows}")
                            print(f"      å‰5ä¸ªå­¦æ ¡: {school_list[:5] if school_list else 'æ— '}")
                            
                            if school_list:
                                return school_list
                            else:
                                print(f"    âš ï¸  CSVæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å­¦æ ¡åç§°")
                                
                    except Exception as csv_error:
                        print(f"    âŒ è¯»å–CSVæ–‡ä»¶å‡ºé”™: {csv_error}")
                        import traceback
                        print(f"    é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                else:
                    print(f"    âŒ æ‰€æœ‰è·¯å¾„éƒ½æ‰¾ä¸åˆ°CSVæ–‡ä»¶")
            
            # å¦‚æœCSVå¤±è´¥ï¼Œä½¿ç”¨é¢„å®šä¹‰åˆ—è¡¨
            if source_type == 'predefined' or not school_list:
                print(f"    ğŸ“‹ ä½¿ç”¨é¢„å®šä¹‰å­¦æ ¡åˆ—è¡¨")
                school_list = [
                    'ä¸­å±±å¤§å­¦', 'æš¨å—å¤§å­¦', 'åå—ç†å·¥å¤§å­¦', 'åå—å†œä¸šå¤§å­¦', 'å¹¿å·åŒ»ç§‘å¤§å­¦',
                    'å¹¿å·ä¸­åŒ»è¯å¤§å­¦', 'å¹¿ä¸œè¯ç§‘å¤§å­¦', 'åå—å¸ˆèŒƒå¤§å­¦', 'å¹¿å·ä½“è‚²å­¦é™¢', 'å¹¿å·ç¾æœ¯å­¦é™¢',
                    'æ˜Ÿæµ·éŸ³ä¹å­¦é™¢', 'å¹¿ä¸œæŠ€æœ¯å¸ˆèŒƒå¤§å­¦', 'å¹¿ä¸œè´¢ç»å¤§å­¦', 'å¹¿å·å¤§å­¦', 'å¹¿å·èˆªæµ·å­¦é™¢',
                    'å¹¿ä¸œè­¦å®˜å­¦é™¢', 'ä»²æºå†œä¸šå·¥ç¨‹å­¦é™¢', 'å¹¿ä¸œé‡‘èå­¦é™¢', 'å¹¿ä¸œå·¥ä¸šå¤§å­¦', 'å¹¿ä¸œå¤–è¯­å¤–è´¸å¤§å­¦',
                    'å—æ–¹åŒ»ç§‘å¤§å­¦', 'å¹¿ä¸œç¬¬äºŒå¸ˆèŒƒå­¦é™¢', 'å¹¿ä¸œè½»å·¥èŒä¸šæŠ€æœ¯å¤§å­¦', 'å¹¿ä¸œç™½äº‘å­¦é™¢', 'å¹¿ä¸œåŸ¹æ­£å­¦é™¢',
                    'å¹¿å·åŸå¸‚ç†å·¥å­¦é™¢', 'å¹¿å·è½¯ä»¶å­¦é™¢', 'å¹¿å·å—æ–¹å­¦é™¢', 'å¹¿ä¸œå¤–è¯­å¤–è´¸å¤§å­¦å—å›½å•†å­¦é™¢',
                    'å¹¿å·åå•†å­¦é™¢', 'åå—å†œä¸šå¤§å­¦ç æ±Ÿå­¦é™¢', 'å¹¿å·ç†å·¥å­¦é™¢', 'å¹¿å·åç«‹å­¦é™¢', 'å¹¿å·åº”ç”¨ç§‘æŠ€å­¦é™¢',
                    'å¹¿å·å•†å­¦é™¢', 'å¹¿å·å·¥å•†å­¦é™¢', 'å¹¿å·ç§‘æŠ€èŒä¸šæŠ€æœ¯å¤§å­¦', 'å¹¿å·æ–°åå­¦é™¢', 'é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰',
                    'å¹¿å·ç•ªç¦ºèŒä¸šæŠ€æœ¯å­¦é™¢'
                ]
                print(f"    âœ… é¢„å®šä¹‰å­¦æ ¡åˆ—è¡¨åŠ è½½æˆåŠŸï¼Œå…± {len(school_list)} ä¸ªå­¦æ ¡")
                return school_list
            
            elif source_type == 'auto':
                # è‡ªåŠ¨æ¨¡å¼ï¼šå…ˆå°è¯•CSVï¼Œå¤±è´¥åˆ™ç”¨é¢„å®šä¹‰
                csv_result = self._get_school_list('csv')
                if csv_result:
                    return csv_result
                return self._get_school_list('predefined')
        
        except Exception as e:
            print(f"    âŒ è·å–å­¦æ ¡åˆ—è¡¨å‡ºé”™: {e}")
            import traceback
            print(f"    é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        if not school_list:
            school_list = ['ä¸­å±±å¤§å­¦', 'æš¨å—å¤§å­¦', 'åå—ç†å·¥å¤§å­¦', 'åå—å†œä¸šå¤§å­¦', 'åå—å¸ˆèŒƒå¤§å­¦']
            print(f"    ğŸš‘ ä½¿ç”¨ç´§æ€¥å¤‡é€‰å­¦æ ¡åˆ—è¡¨ï¼Œå…± {len(school_list)} ä¸ªå­¦æ ¡")
        
        return school_list
    
    def _find_school_segments(self, content, school_name):
        """ğŸ”¥ å·²å¼ƒç”¨ï¼šä½¿ç”¨ _find_school_segments_configurable æ›¿ä»£ ğŸ”¥"""
        # ä½¿ç”¨é»˜è®¤é…ç½®è°ƒç”¨æ–°æ–¹æ³•
        default_config = {
            'characters_after_school': 100,
            'characters_before_school': 0,
            'max_segments_per_school': 10,
            'min_segment_length': 10
        }
        return self._find_school_segments_configurable(content, school_name, default_config)
    
    def _basic_clean_text(self, text):
        """åŸºç¡€æ–‡æœ¬æ¸…ç†ï¼Œä¿ç•™åŸºæœ¬æ ¼å¼"""
        if not text or not isinstance(text, str):
            return ""
        
        import re
        
        # ç§»é™¤è¿‡å¤šçš„æ¢è¡Œç¬¦å’Œç©ºç™½
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        text = re.sub(r'\r\n', '\n', text)
        text = re.sub(r'\t+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦ï¼Œä½†ä¿ç•™åŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f]', '', text)
        
        # æ¸…ç†å¤šä½™ç©ºæ ¼ï¼Œä½†ä¿ç•™å•ä¸ªç©ºæ ¼
        text = re.sub(r' +', ' ', text)
        
        return text.strip()
    
    def _clean_segment_text(self, segment_text):
        """æ¸…ç†ç‰‡æ®µæ–‡æœ¬"""
        if not segment_text or not isinstance(segment_text, str):
            return ""
        
        import re
        
        # åŸºç¡€æ¸…ç†
        segment_text = self._basic_clean_text(segment_text)
        
        # ç§»é™¤å¯èƒ½çš„æ–‡ä»¶è·¯å¾„å’ŒURL
        segment_text = re.sub(r'[A-Za-z]:\\[^\s]+', '', segment_text)
        segment_text = re.sub(r'https?://[^\s]+', '', segment_text)
        
        # ç§»é™¤è¿‡é•¿çš„æ•°å­—ä¸²ï¼ˆå¯èƒ½æ˜¯IDæˆ–æ— æ„ä¹‰æ•°æ®ï¼‰
        segment_text = re.sub(r'\b\d{10,}\b', '', segment_text)
        
        # æ¸…ç†å¤šä½™çš„æ ‡ç‚¹ç¬¦å·
        segment_text = re.sub(r'[ã€‚ï¼Œ]{3,}', '...', segment_text)
        segment_text = re.sub(r'[-_=]{5,}', '---', segment_text)
        
        # æœ€ç»ˆæ¸…ç†
        segment_text = re.sub(r'\s+', ' ', segment_text)
        
        return segment_text.strip()
    
    def _clean_attachment_text(self, text):
        """æ¸…ç†é™„ä»¶æ–‡æœ¬ä¸­çš„æ ¼å¼å­—ç¬¦ - ä¿æŒåŸæœ‰åŠŸèƒ½"""
        if not text or not isinstance(text, str):
            return ""
        
        import re
        
        # ç§»é™¤æ¢è¡Œç¬¦å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\n\r\t]+', '', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤è½¬ä¹‰å­—ç¬¦
        text = re.sub(r'\\[nrt]', ' ', text)
        
        # æ¸…ç†Unicodeç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\u00a0\u2000-\u200f\u2028-\u202f\u3000]', ' ', text)
        
        # ä¿ç•™ä¸­æ–‡å’Œå¸¸ç”¨æ ‡ç‚¹
        text = re.sub(r'[^\w\s\u4e00-\u9fff\u3001.,;:ï¼š!?()[\]{}"\'-]', '', text)
        text = text.replace('ã€', ', ')
        
        return text.strip()
    
    def _read_pdf_attachment(self, pdf_path):
        """è¯»å–PDFé™„ä»¶å†…å®¹"""
        try:
            import PyPDF2
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text_content = ""
                for page in reader.pages:
                    text_content += page.extract_text() + " "
                return text_content
        except ImportError:
            print(f"    âš ï¸  PyPDF2æœªå®‰è£…ï¼Œæ— æ³•è¯»å–PDF")
            return ""
        except Exception as e:
            print(f"    âŒ è¯»å–PDFæ—¶å‡ºé”™: {e}")
            return ""
    
    def _read_docx_attachment(self, docx_path):
        """è¯»å–DOCXé™„ä»¶å†…å®¹"""
        try:
            import docx
            doc = docx.Document(docx_path)
            text_content = ""
            
            # æå–æ®µè½
            for para in doc.paragraphs:
                if para.text.strip():
                    text_content += para.text + " "
            
            # æå–è¡¨æ ¼
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip():
                            text_content += cell.text + " "
            
            return text_content
        except ImportError:
            print(f"    âš ï¸  python-docxæœªå®‰è£…ï¼Œæ— æ³•è¯»å–DOCX")
            return ""
        except Exception as e:
            print(f"    âŒ è¯»å–DOCXæ—¶å‡ºé”™: {e}")
            return ""