"""æ•°æ®æ ¼å¼è½¬æ¢å™¨ - CSVåˆ°JSONè½¬æ¢"""
import json
import csv
import os
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

class DataConverter:
    """CSVåˆ°JSONæ•°æ®è½¬æ¢å™¨"""
    
    def __init__(self, data_dir: str = "src/data"):
        self.data_dir = Path(data_dir)
        self.consolidated_dir = self.data_dir / "consolidated"
        self.consolidated_dir.mkdir(parents=True, exist_ok=True)
        
    def scan_csv_files(self) -> Dict[str, List[Path]]:
        """æ‰«ææ‰€æœ‰CSVæ–‡ä»¶"""
        csv_files = {}
        
        for root, dirs, files in os.walk(self.data_dir):
            for file in files:
                if file.endswith('.csv'):
                    file_path = Path(root) / file
                    relative_dir = Path(root).relative_to(self.data_dir)
                    
                    if str(relative_dir) not in csv_files:
                        csv_files[str(relative_dir)] = []
                    csv_files[str(relative_dir)].append(file_path)
        
        return csv_files
    
    def convert_csv_to_json(self, csv_file: Path, output_file: Path = None) -> bool:
        """å°†å•ä¸ªCSVæ–‡ä»¶è½¬æ¢ä¸ºJSON"""
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(csv_file, encoding='utf-8')
            
            # è½¬æ¢ä¸ºJSONæ ¼å¼
            json_data = {
                "source_file": str(csv_file),
                "converted_time": datetime.now().isoformat(),
                "total_records": len(df),
                "columns": df.columns.tolist(),
                "data": df.to_dict('records')  # è½¬æ¢ä¸ºè®°å½•åˆ—è¡¨
            }
            
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
            if output_file is None:
                output_file = csv_file.with_suffix('.json')
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… è½¬æ¢æˆåŠŸ: {csv_file} -> {output_file}")
            return True
            
        except Exception as e:
            print(f"âŒ è½¬æ¢å¤±è´¥: {csv_file} - {e}")
            return False
    
    def convert_all_csv_files(self) -> Dict[str, Any]:
        """è½¬æ¢æ‰€æœ‰CSVæ–‡ä»¶"""
        csv_files = self.scan_csv_files()
        conversion_results = {
            "total_csv_files": 0,
            "successful_conversions": 0,
            "failed_conversions": 0,
            "converted_files": [],
            "failed_files": []
        }
        
        for dir_name, files in csv_files.items():
            print(f"ğŸ“ å¤„ç†ç›®å½•: {dir_name}")
            
            for csv_file in files:
                conversion_results["total_csv_files"] += 1
                
                if self.convert_csv_to_json(csv_file):
                    conversion_results["successful_conversions"] += 1
                    conversion_results["converted_files"].append(str(csv_file))
                else:
                    conversion_results["failed_conversions"] += 1
                    conversion_results["failed_files"].append(str(csv_file))
        
        return conversion_results
    
    def create_consolidated_data_for_metric(self, metric_name: str, source_dirs: List[str]) -> bool:
        """ä¸ºç‰¹å®šæŒ‡æ ‡åˆ›å»ºæ•´åˆçš„JSONæ•°æ®æ–‡ä»¶"""
        try:
            consolidated_data = {
                "metric": metric_name,
                "source_directories": source_dirs,
                "consolidated_time": datetime.now().isoformat(),
                "universities": {},
                "summary": {
                    "total_universities": 0,
                    "data_sources": len(source_dirs),
                    "total_records": 0
                }
            }
            
            total_records = 0
            
            # ä»æ¯ä¸ªæºç›®å½•æ”¶é›†æ•°æ®
            for source_dir in source_dirs:
                source_path = self.data_dir / source_dir
                
                if not source_path.exists():
                    print(f"âš ï¸  æºç›®å½•ä¸å­˜åœ¨: {source_path}")
                    continue
                
                print(f"ğŸ“‚ å¤„ç†æºç›®å½•: {source_path}")
                
                # æŸ¥æ‰¾JSONæ–‡ä»¶
                for json_file in source_path.glob("*.json"):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        # æå–å¤§å­¦æ•°æ®
                        if isinstance(data, dict) and "data" in data:
                            records = data["data"]
                            if isinstance(records, list):
                                for record in records:
                                    if isinstance(record, dict) and "å­¦æ ¡åç§°" in record:
                                        university = record["å­¦æ ¡åç§°"]
                                        if university not in consolidated_data["universities"]:
                                            consolidated_data["universities"][university] = []
                                        consolidated_data["universities"][university].append({
                                            "source_file": str(json_file),
                                            "data": record
                                        })
                                        total_records += 1
                        
                    except Exception as e:
                        print(f"âŒ è¯»å–JSONæ–‡ä»¶å¤±è´¥: {json_file} - {e}")
                        continue
                
                # æŸ¥æ‰¾CSVæ–‡ä»¶å¹¶ä¸´æ—¶è½¬æ¢
                for csv_file in source_path.glob("*.csv"):
                    try:
                        df = pd.read_csv(csv_file, encoding='utf-8')
                        
                        for _, row in df.iterrows():
                            if "å­¦æ ¡åç§°" in row:
                                university = row["å­¦æ ¡åç§°"]
                                if university not in consolidated_data["universities"]:
                                    consolidated_data["universities"][university] = []
                                consolidated_data["universities"][university].append({
                                    "source_file": str(csv_file),
                                    "data": row.to_dict()
                                })
                                total_records += 1
                        
                    except Exception as e:
                        print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {csv_file} - {e}")
                        continue
            
            # æ›´æ–°æ‘˜è¦ä¿¡æ¯
            consolidated_data["summary"]["total_universities"] = len(consolidated_data["universities"])
            consolidated_data["summary"]["total_records"] = total_records
            
            # ä¿å­˜æ•´åˆæ•°æ®
            output_file = self.consolidated_dir / f"{metric_name}_data.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(consolidated_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… åˆ›å»ºæ•´åˆæ•°æ®: {output_file}")
            print(f"   ğŸ“Š æ€»å¤§å­¦æ•°: {consolidated_data['summary']['total_universities']}")
            print(f"   ğŸ“ˆ æ€»è®°å½•æ•°: {consolidated_data['summary']['total_records']}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ•´åˆæ•°æ®å¤±è´¥: {metric_name} - {e}")
            return False

class EnhancedDataProcessor:
    """å¢å¼ºç‰ˆæ•°æ®å¤„ç†å™¨ - æ”¯æŒCSVå’ŒJSON"""
    
    def __init__(self, data_dir: str = "src/data"):
        self.data_dir = Path(data_dir)
        self.converter = DataConverter(data_dir)
        
    def auto_prepare_data_for_metrics(self) -> Dict[str, Any]:
        """è‡ªåŠ¨ä¸ºæ‰€æœ‰æŒ‡æ ‡å‡†å¤‡æ•°æ®"""
        
        # æŒ‡æ ‡åˆ°æ•°æ®æºçš„æ˜ å°„
        metric_mappings = {
            'ESIå‰1%å­¦ç§‘æ•°é‡': ['esi_subjects/esi_top1percent', 'esi_subjects'],
            'ESIå‰1â€°å­¦ç§‘æ•°é‡': ['esi_subjects/esi_top1permille', 'esi_subjects'],
            'è½¯ç§‘ä¸­å›½æœ€å¥½å­¦ç§‘æ’åå‰10%å­¦ç§‘æ•°é‡': ['ruanke_subjects'],
            'å›½å®¶åŒä¸€æµå­¦ç§‘æ•°é‡': ['moepolicies'],
            'æ•™è‚²éƒ¨è¯„ä¼°Aç±»å­¦ç§‘æ•°é‡': ['subject_evaluation'],
            'æœ¬ç§‘ä¸“ä¸šæ€»æ•°': ['undergraduate_majors/total_majors', 'undergraduate_majors'],
            'æœ¬ç§‘ä¸“ä¸šè®¤è¯é€šè¿‡æ•°': ['undergraduate_majors/certified_majors', 'undergraduate_majors'],
            'å›½å®¶çº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹': ['undergraduate_majors/national_first_class', 'undergraduate_majors'],
            'çœçº§ä¸€æµæœ¬ç§‘ä¸“ä¸šå»ºè®¾ç‚¹': ['undergraduate_majors/provincial_first_class', 'undergraduate_majors'],
            'å›½å®¶çº§æ•™å­¦æˆæœå¥–': ['teaching_awards'],
            'çœçº§æ•™å­¦æˆæœå¥–': ['teaching_awards'],
            'é’å¹´æ•™å¸ˆç«èµ›': ['teacher_competition'],
            'å›½å®¶çº§ä¸€æµè¯¾ç¨‹': ['courses/national_first_class'],
            'çœçº§ä¸€æµè¯¾ç¨‹': ['courses/provincial_first_class'],
            'å›½å®¶çº§æ™ºæ…§æ•™è‚²å¹³å°è¯¾ç¨‹': ['courses/smart_platform'],
            'çœçº§æ™ºæ…§æ•™è‚²å¹³å°è¯¾ç¨‹': ['courses/smart_platform']
        }
        
        results = {
            "processed_metrics": 0,
            "successful_metrics": 0,
            "failed_metrics": 0,
            "metric_results": {}
        }
        
        print("ğŸ”„ å¼€å§‹è‡ªåŠ¨æ•°æ®å‡†å¤‡...")
        
        for metric_name, source_dirs in metric_mappings.items():
            print(f"\nğŸ“Š å¤„ç†æŒ‡æ ‡: {metric_name}")
            
            results["processed_metrics"] += 1
            
            # åˆ›å»ºæ•´åˆæ•°æ®
            success = self.converter.create_consolidated_data_for_metric(metric_name, source_dirs)
            
            if success:
                results["successful_metrics"] += 1
                results["metric_results"][metric_name] = "æˆåŠŸ"
            else:
                results["failed_metrics"] += 1
                results["metric_results"][metric_name] = "å¤±è´¥"
        
        return results
    
    def check_data_availability(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®å¯ç”¨æ€§"""
        
        csv_files = self.converter.scan_csv_files()
        consolidated_files = list(self.converter.consolidated_dir.glob("*.json"))
        
        availability_report = {
            "csv_files_found": sum(len(files) for files in csv_files.values()),
            "csv_directories": list(csv_files.keys()),
            "consolidated_files": len(consolidated_files),
            "consolidated_metrics": [f.stem.replace('_data', '') for f in consolidated_files],
            "recommendations": []
        }
        
        # æä¾›å»ºè®®
        if availability_report["csv_files_found"] > 0:
            availability_report["recommendations"].append("å‘ç°CSVæ–‡ä»¶ï¼Œå»ºè®®è¿è¡Œæ•°æ®è½¬æ¢")
        
        if availability_report["consolidated_files"] == 0:
            availability_report["recommendations"].append("æœªæ‰¾åˆ°æ•´åˆæ•°æ®æ–‡ä»¶ï¼Œå»ºè®®è¿è¡Œè‡ªåŠ¨æ•°æ®å‡†å¤‡")
        
        return availability_report

def create_data_preparation_script():
    """åˆ›å»ºæ•°æ®å‡†å¤‡è„šæœ¬"""
    
    script_content = '''
"""æ•°æ®å‡†å¤‡è„šæœ¬ - è‡ªåŠ¨è½¬æ¢CSVå¹¶å‡†å¤‡æ•°æ®"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from data_converter import DataConverter, EnhancedDataProcessor

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ•°æ®å‡†å¤‡å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = EnhancedDataProcessor()
    
    # æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
    print("ğŸ“‹ æ£€æŸ¥æ•°æ®å¯ç”¨æ€§...")
    availability = processor.check_data_availability()
    
    print(f"   CSVæ–‡ä»¶æ•°é‡: {availability['csv_files_found']}")
    print(f"   CSVç›®å½•: {availability['csv_directories']}")
    print(f"   æ•´åˆæ–‡ä»¶æ•°é‡: {availability['consolidated_files']}")
    print(f"   å·²æ•´åˆæŒ‡æ ‡: {availability['consolidated_metrics']}")
    
    # æ˜¾ç¤ºå»ºè®®
    if availability["recommendations"]:
        print("\\nğŸ’¡ å»ºè®®æ“ä½œ:")
        for rec in availability["recommendations"]:
            print(f"   â€¢ {rec}")
    
    # å¦‚æœæœ‰CSVæ–‡ä»¶ï¼Œè¿›è¡Œè½¬æ¢
    if availability['csv_files_found'] > 0:
        print("\\nğŸ”„ å¼€å§‹CSVè½¬æ¢...")
        conversion_results = processor.converter.convert_all_csv_files()
        
        print(f"âœ… è½¬æ¢å®Œæˆ:")
        print(f"   æ€»æ–‡ä»¶: {conversion_results['total_csv_files']}")
        print(f"   æˆåŠŸ: {conversion_results['successful_conversions']}")
        print(f"   å¤±è´¥: {conversion_results['failed_conversions']}")
    
    # è‡ªåŠ¨å‡†å¤‡æŒ‡æ ‡æ•°æ®
    print("\\nğŸ“Š è‡ªåŠ¨å‡†å¤‡æŒ‡æ ‡æ•°æ®...")
    prep_results = processor.auto_prepare_data_for_metrics()
    
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
    print(f"   å¤„ç†æŒ‡æ ‡: {prep_results['processed_metrics']}")
    print(f"   æˆåŠŸ: {prep_results['successful_metrics']}")
    print(f"   å¤±è´¥: {prep_results['failed_metrics']}")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    print("\\nğŸ“‹ æŒ‡æ ‡å¤„ç†è¯¦æƒ…:")
    for metric, status in prep_results['metric_results'].items():
        status_icon = "âœ…" if status == "æˆåŠŸ" else "âŒ"
        print(f"   {status_icon} {metric}: {status}")
    
    print("\\nğŸ‰ æ•°æ®å‡†å¤‡å·¥å…·æ‰§è¡Œå®Œæˆ!")
    print("ğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œ python -m src.AIquest.main æ¥å¤„ç†é—®ç­”ä»»åŠ¡")

if __name__ == "__main__":
    main()
'''
    
    script_file = Path("prepare_data.py")
    with open(script_file, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print(f"âœ… æ•°æ®å‡†å¤‡è„šæœ¬å·²åˆ›å»º: {script_file}")
    return script_file

# ç°åœ¨åˆ›å»ºæµ‹è¯•é…ç½®
def create_test_configuration():
    """åˆ›å»ºæµ‹è¯•é…ç½®"""
    
    test_settings = {
        "python.testing.unittestArgs": [
            "-v",
            "-s",
            "tests",
            "-p",
            "test_*.py"
        ],
        "python.testing.pytestEnabled": True,
        "python.testing.unittestEnabled": False,
        "python.testing.pytestArgs": [
            "--verbose",
            "--tb=short",
            "--maxfail=5",
            "tests/"
        ],
        "python.testing.cwd": "${workspaceFolder}",
        "python.testing.autoTestDiscoverOnSaveEnabled": true,
        "python.defaultInterpreterPath": "python",
        "python.linting.enabled": true,
        "python.linting.pylintEnabled": false,
        "python.linting.flake8Enabled": true,
        "python.formatting.provider": "black",
        "files.exclude": {
            "**/__pycache__": true,
            "**/*.pyc": true,
            "**/.pytest_cache": true,
            "**/debug_logs": true,
            "**/temp": true
        }
    }
    
    # åˆ›å»º.vscodeç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    vscode_dir = Path(".vscode")
    vscode_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜æµ‹è¯•è®¾ç½®
    import json
    with open(vscode_dir / "test_settings.json", 'w', encoding='utf-8') as f:
        json.dump(test_settings, f, indent=4)
    
    print("âœ… æµ‹è¯•é…ç½®å·²åˆ›å»º: .vscode/test_settings.json")

def create_comprehensive_tests():
    """åˆ›å»ºå…¨é¢çš„æµ‹è¯•å¥—ä»¶"""
    
    # åˆ›å»ºtestsç›®å½•
    tests_dir = Path("tests")
    tests_dir.mkdir(exist_ok=True)
    
    # åˆ›å»º__init__.py
    (tests_dir / "__init__.py").write_text("")
    
    # 1. æ•°æ®è½¬æ¢å™¨æµ‹è¯•
    test_data_converter = '''
import unittest
import tempfile
import pandas as pd
import json
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from data_converter import DataConverter, EnhancedDataProcessor

class TestDataConverter(unittest.TestCase):
    """æ•°æ®è½¬æ¢å™¨æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        self.converter = DataConverter(self.temp_dir)
        
        # åˆ›å»ºæµ‹è¯•CSVæ–‡ä»¶
        test_data = {
            'å­¦æ ¡åç§°': ['ä¸­å±±å¤§å­¦', 'åå—ç†å·¥å¤§å­¦', 'æš¨å—å¤§å­¦'],
            'ESIå‰1%å­¦ç§‘æ•°é‡': [15, 12, 8],
            'æ’å': [1, 2, 3]
        }
        
        self.test_csv = Path(self.temp_dir) / "test_data.csv"
        df = pd.DataFrame(test_data)
        df.to_csv(self.test_csv, index=False, encoding='utf-8')
    
    def test_scan_csv_files(self):
        """æµ‹è¯•CSVæ–‡ä»¶æ‰«æ"""
        csv_files = self.converter.scan_csv_files()
        self.assertIsInstance(csv_files, dict)
        self.assertTrue(len(csv_files) > 0)
    
    def test_convert_csv_to_json(self):
        """æµ‹è¯•CSVåˆ°JSONè½¬æ¢"""
        output_file = self.test_csv.with_suffix('.json')
        success = self.converter.convert_csv_to_json(self.test_csv, output_file)
        
        self.assertTrue(success)
        self.assertTrue(output_file.exists())
        
        # éªŒè¯JSONå†…å®¹
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.assertIn('data', data)
        self.assertIn('total_records', data)
        self.assertEqual(data['total_records'], 3)
        self.assertEqual(len(data['data']), 3)
    
    def test_create_consolidated_data(self):
        """æµ‹è¯•æ•´åˆæ•°æ®åˆ›å»º"""
        # å…ˆè½¬æ¢CSVåˆ°JSON
        self.converter.convert_csv_to_json(self.test_csv)
        
        # åˆ›å»ºæ•´åˆæ•°æ®
        success = self.converter.create_consolidated_data_for_metric(
            "ESIå‰1%å­¦ç§‘æ•°é‡", ["."]
        )
        
        self.assertTrue(success)
        
        # éªŒè¯æ•´åˆæ–‡ä»¶
        consolidated_file = self.converter.consolidated_dir / "ESIå‰1%å­¦ç§‘æ•°é‡_data.json"
        self.assertTrue(consolidated_file.exists())

class TestEnhancedDataProcessor(unittest.TestCase):
    """å¢å¼ºæ•°æ®å¤„ç†å™¨æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        self.processor = EnhancedDataProcessor(self.temp_dir)
    
    def test_check_data_availability(self):
        """æµ‹è¯•æ•°æ®å¯ç”¨æ€§æ£€æŸ¥"""
        report = self.processor.check_data_availability()
        
        self.assertIsInstance(report, dict)
        self.assertIn('csv_files_found', report)
        self.assertIn('consolidated_files', report)
        self.assertIn('recommendations', report)

if __name__ == '__main__':
    unittest.main()
'''
    
    with open(tests_dir / "test_data_converter.py", 'w', encoding='utf-8') as f:
        f.write(test_data_converter)
    
    # 2. çˆ¬è™«åŠŸèƒ½æµ‹è¯•
    test_crawler = '''
import unittest
import asyncio
import sys
from pathlib import Path
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class TestCrawlerFunctionality(unittest.TestCase):
    """çˆ¬è™«åŠŸèƒ½æµ‹è¯•"""
    
    def test_basic_connection(self):
        """æµ‹è¯•åŸºç¡€ç½‘ç»œè¿æ¥"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç½‘ç»œè¿æ¥æµ‹è¯•
        pass
    
    def test_html_parsing(self):
        """æµ‹è¯•HTMLè§£æåŠŸèƒ½"""
        from bs4 import BeautifulSoup
        
        test_html = """
        <html>
            <body>
                <div class="content">
                    <h1>æµ‹è¯•æ ‡é¢˜</h1>
                    <p>æµ‹è¯•å†…å®¹</p>
                </div>
            </body>
        </html>
        """
        
        soup = BeautifulSoup(test_html, 'html.parser')
        content = soup.find('.content')
        
        self.assertIsNotNone(content)
        self.assertEqual(content.find('h1').text, 'æµ‹è¯•æ ‡é¢˜')
    
    def test_data_extraction(self):
        """æµ‹è¯•æ•°æ®æå–åŠŸèƒ½"""
        # æ¨¡æ‹Ÿæ•°æ®æå–æµ‹è¯•
        test_data = {
            'url': 'https://test.com',
            'content': 'æµ‹è¯•å†…å®¹',
            'images': [],
            'success': True
        }
        
        self.assertTrue(test_data['success'])
        self.assertIsInstance(test_data['images'], list)

if __name__ == '__main__':
    unittest.main()
'''
    
    with open(tests_dir / "test_crawler.py", 'w', encoding='utf-8') as f:
        f.write(test_crawler)
    
    # 3. AIé—®ç­”ç³»ç»Ÿæµ‹è¯•
    test_ai_quest = '''
import unittest
import tempfile
import json
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class TestAIQuestSystem(unittest.TestCase):
    """AIé—®ç­”ç³»ç»Ÿæµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        self.test_data = {
            "metric": "ESIå‰1%å­¦ç§‘æ•°é‡",
            "universities": {
                "ä¸­å±±å¤§å­¦": [
                    {
                        "source_file": "test.json",
                        "data": {
                            "å­¦æ ¡åç§°": "ä¸­å±±å¤§å­¦",
                            "ESIå‰1%å­¦ç§‘æ•°é‡": 15
                        }
                    }
                ]
            },
            "summary": {
                "total_universities": 1,
                "total_records": 1
            }
        }
        
        # ä¿å­˜æµ‹è¯•æ•°æ®æ–‡ä»¶
        self.test_file = Path(self.temp_dir) / "ESIå‰1%å­¦ç§‘æ•°é‡_data.json"
        with open(self.test_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_data, f, ensure_ascii=False, indent=2)
    
    def test_data_loading(self):
        """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
        # è¯»å–æµ‹è¯•æ•°æ®
        with open(self.test_file, 'r', encoding='utf-8') as f:
            loaded_data = json.load(f)
        
        self.assertEqual(loaded_data['metric'], "ESIå‰1%å­¦ç§‘æ•°é‡")
        self.assertIn('ä¸­å±±å¤§å­¦', loaded_data['universities'])
    
    def test_question_processing(self):
        """æµ‹è¯•é—®é¢˜å¤„ç†åŠŸèƒ½"""
        # æ¨¡æ‹Ÿé—®é¢˜å¤„ç†
        test_question = "ä¸­å±±å¤§å­¦æœ‰å¤šå°‘ä¸ªESIå‰1%å­¦ç§‘ï¼Ÿ"
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„é—®é¢˜å¤„ç†é€»è¾‘æµ‹è¯•
        self.assertIsInstance(test_question, str)
        self.assertTrue(len(test_question) > 0)

if __name__ == '__main__':
    unittest.main()
'''
    
    with open(tests_dir / "test_ai_quest.py", 'w', encoding='utf-8') as f:
        f.write(test_ai_quest)
    
    # 4. é›†æˆæµ‹è¯•
    test_integration = '''
import unittest
import tempfile
import asyncio
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class TestIntegration(unittest.TestCase):
    """é›†æˆæµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
    
    def test_full_workflow(self):
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
        # 1. æ•°æ®å‡†å¤‡
        # 2. æ•°æ®è½¬æ¢
        # 3. é—®ç­”å¤„ç†
        # 4. ç»“æœéªŒè¯
        
        # è¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚æ·»åŠ å…·ä½“æµ‹è¯•
        workflow_steps = [
            "æ•°æ®æ‰«æ",
            "æ ¼å¼è½¬æ¢", 
            "æ•°æ®æ•´åˆ",
            "é—®ç­”å¤„ç†"
        ]
        
        for step in workflow_steps:
            with self.subTest(step=step):
                # æ¯ä¸ªæ­¥éª¤çš„æµ‹è¯•é€»è¾‘
                self.assertTrue(isinstance(step, str))
    
    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        # æµ‹è¯•å„ç§é”™è¯¯æƒ…å†µçš„å¤„ç†
        test_errors = [
            "æ–‡ä»¶ä¸å­˜åœ¨",
            "æ ¼å¼é”™è¯¯",
            "ç½‘ç»œè¿æ¥å¤±è´¥"
        ]
        
        for error_type in test_errors:
            with self.subTest(error=error_type):
                # é”™è¯¯å¤„ç†æµ‹è¯•é€»è¾‘
                self.assertIsInstance(error_type, str)

if __name__ == '__main__':
    unittest.main()
'''
    
    with open(tests_dir / "test_integration.py", 'w', encoding='utf-8') as f:
        f.write(test_integration)
    
    print("âœ… æµ‹è¯•å¥—ä»¶å·²åˆ›å»º:")
    print("   ğŸ“ tests/")
    print("      ğŸ“„ test_data_converter.py")
    print("      ğŸ“„ test_crawler.py") 
    print("      ğŸ“„ test_ai_quest.py")
    print("      ğŸ“„ test_integration.py")

def main():
    """ä¸»å‡½æ•° - å®Œæ•´çš„è§£å†³æ–¹æ¡ˆ"""
    print("ğŸš€ AIé—®ç­”ç³»ç»Ÿæ•°æ®å‡†å¤‡å’Œæµ‹è¯•é…ç½®")
    print("=" * 60)
    
    # 1. åˆ›å»ºæ•°æ®è½¬æ¢å™¨è„šæœ¬
    print("ğŸ“ åˆ›å»ºæ•°æ®è½¬æ¢å™¨...")
    create_data_preparation_script()
    
    # 2. åˆ›å»ºæµ‹è¯•é…ç½®
    print("âš™ï¸  åˆ›å»ºæµ‹è¯•é…ç½®...")
    create_test_configuration()
    
    # 3. åˆ›å»ºæµ‹è¯•å¥—ä»¶
    print("ğŸ§ª åˆ›å»ºæµ‹è¯•å¥—ä»¶...")
    create_comprehensive_tests()
    
    print("\nâœ… å®Œæˆï¼ç°åœ¨æ‚¨å¯ä»¥:")
    print("   1. è¿è¡Œ python prepare_data.py æ¥è½¬æ¢CSVæ•°æ®")
    print("   2. è¿è¡Œ python -m pytest tests/ æ¥æ‰§è¡Œæµ‹è¯•")
    print("   3. è¿è¡Œ python -m src.AIquest.main æ¥ä½¿ç”¨AIé—®ç­”ç³»ç»Ÿ")
    
    print("\nğŸ’¡ è§£å†³æ‚¨çš„é—®é¢˜:")
    print("   â€¢ CSVæ–‡ä»¶å°†è‡ªåŠ¨è½¬æ¢ä¸ºJSONæ ¼å¼")
    print("   â€¢ æ‰€æœ‰æŒ‡æ ‡æ•°æ®å°†æ•´åˆåˆ°consolidatedç›®å½•")
    print("   â€¢ AIé—®ç­”ç³»ç»Ÿå°†èƒ½å¤Ÿè¯»å–å¤„ç†åçš„æ•°æ®")
    print("   â€¢ å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§")

if __name__ == "__main__":
    main()