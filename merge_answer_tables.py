"""
åˆ†è¡¨æ ¼æ±‡æ€»è„šæœ¬
å°†å¤šä¸ªæŒ‡æ ‡çš„ç­”æ¡ˆCSVæ–‡ä»¶æ±‡æ€»æˆä¸€ä¸ªæ€»è¡¨æ ¼
"""

import os
import pandas as pd
import glob
from datetime import datetime
import sys

def find_answer_csv_files(directory="."):
    """
    æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¨¡å¼çš„ç­”æ¡ˆCSVæ–‡ä»¶
    æ–‡ä»¶åæ¨¡å¼: *_answers.csv
    """
    pattern = os.path.join(directory, "*_answers.csv")
    files = glob.glob(pattern)
    
    # è¿‡æ»¤æ‰å·²ç»æ˜¯æ±‡æ€»æ–‡ä»¶çš„æ–‡ä»¶
    answer_files = []
    for file in files:
        filename = os.path.basename(file)
        # æ’é™¤å¯èƒ½çš„æ±‡æ€»æ–‡ä»¶
        if not any(keyword in filename.lower() for keyword in ['merged', 'combined', 'summary', 'all']):
            answer_files.append(file)
    
    return answer_files

def extract_metric_from_filename(filename):
    """
    ä»æ–‡ä»¶åä¸­æå–æŒ‡æ ‡åç§°
    ä¾‹å¦‚: ai_evaluation_dataset_long_ESIå‰1%å­¦ç§‘æ•°é‡_answers.csv -> ESIå‰1%å­¦ç§‘æ•°é‡
    """
    basename = os.path.basename(filename)
    
    # ç§»é™¤æ–‡ä»¶æ‰©å±•å
    name_without_ext = basename.replace('.csv', '')
    
    # ç§»é™¤å‰ç¼€å’Œåç¼€
    if name_without_ext.startswith('ai_evaluation_dataset_long_'):
        name_without_ext = name_without_ext[len('ai_evaluation_dataset_long_'):]
    
    if name_without_ext.endswith('_answers'):
        name_without_ext = name_without_ext[:-len('_answers')]
    
    return name_without_ext

def read_answer_file(file_path):
    """
    è¯»å–ç­”æ¡ˆCSVæ–‡ä»¶
    è¿”å›å¤„ç†åçš„æ•°æ®
    """
    try:
        # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼
        encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312']
        df = None
        
        for encoding in encodings:
            try:
                df = pd.read_csv(file_path, encoding=encoding)
                print(f"  âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–: {os.path.basename(file_path)}")
                break
            except UnicodeDecodeError:
                continue
        
        if df is None:
            print(f"  âŒ æ— æ³•è¯»å–æ–‡ä»¶ (å°è¯•äº†æ‰€æœ‰ç¼–ç ): {file_path}")
            return None
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        required_columns = ['å­¦æ ¡åç§°', 'æŒ‡æ ‡åç§°', 'AIç­”æ¡ˆ']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"  âš ï¸  æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ— {missing_columns}: {file_path}")
            return None
        
        # åªä¿ç•™éœ€è¦çš„åˆ—
        df = df[['å­¦æ ¡åç§°', 'æŒ‡æ ‡åç§°', 'AIç­”æ¡ˆ']].copy()
        
        # æ¸…ç†æ•°æ®
        df = df.dropna(subset=['å­¦æ ¡åç§°'])  # åˆ é™¤å­¦æ ¡åç§°ä¸ºç©ºçš„è¡Œ
        df['å­¦æ ¡åç§°'] = df['å­¦æ ¡åç§°'].astype(str).str.strip()
        df['æŒ‡æ ‡åç§°'] = df['æŒ‡æ ‡åç§°'].astype(str).str.strip()
        df['AIç­”æ¡ˆ'] = df['AIç­”æ¡ˆ'].astype(str).str.strip()
        
        print(f"  ğŸ“Š è¯»å–åˆ° {len(df)} æ¡è®°å½•")
        return df
        
    except Exception as e:
        print(f"  âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None

def merge_answer_tables(input_directory=".", output_file=None):
    """
    åˆå¹¶æ‰€æœ‰ç­”æ¡ˆè¡¨æ ¼
    """
    print("ğŸ” å¼€å§‹æŸ¥æ‰¾ç­”æ¡ˆCSVæ–‡ä»¶...")
    
    # æŸ¥æ‰¾æ‰€æœ‰ç­”æ¡ˆæ–‡ä»¶
    answer_files = find_answer_csv_files(input_directory)
    
    if not answer_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç­”æ¡ˆCSVæ–‡ä»¶")
        print("ğŸ’¡ è¯·ç¡®ä¿æ–‡ä»¶åæ ¼å¼ä¸º: *_answers.csv")
        return False
    
    print(f"ğŸ“ æ‰¾åˆ° {len(answer_files)} ä¸ªç­”æ¡ˆæ–‡ä»¶:")
    for file in answer_files:
        metric_name = extract_metric_from_filename(file)
        print(f"  ğŸ“„ {os.path.basename(file)} -> æŒ‡æ ‡: {metric_name}")
    
    print(f"\nğŸ“Š å¼€å§‹è¯»å–å’Œåˆå¹¶æ•°æ®...")
    
    # å­˜å‚¨æ‰€æœ‰æ•°æ®
    all_data = []
    successful_files = 0
    
    for file_path in answer_files:
        print(f"\nğŸ”„ å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        
        # è¯»å–æ–‡ä»¶
        df = read_answer_file(file_path)
        if df is not None and not df.empty:
            all_data.append(df)
            successful_files += 1
        else:
            print(f"  âš ï¸  è·³è¿‡æ–‡ä»¶: {os.path.basename(file_path)}")
    
    if not all_data:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡ä»¶")
        return False
    
    print(f"\nğŸ”— åˆå¹¶æ•°æ®...")
    print(f"  æˆåŠŸå¤„ç†: {successful_files}/{len(answer_files)} ä¸ªæ–‡ä»¶")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    merged_df = pd.concat(all_data, ignore_index=True)
    
    # æ•°æ®æ¸…ç†å’Œæ•´ç†
    print(f"ğŸ“‹ æ•°æ®æ•´ç†...")
    
    # å»é‡ï¼ˆåŸºäºå­¦æ ¡åç§°å’ŒæŒ‡æ ‡åç§°ï¼‰
    before_dedup = len(merged_df)
    merged_df = merged_df.drop_duplicates(subset=['å­¦æ ¡åç§°', 'æŒ‡æ ‡åç§°'], keep='last')
    after_dedup = len(merged_df)
    
    if before_dedup != after_dedup:
        print(f"  ğŸ”„ å»é‡: {before_dedup} -> {after_dedup} æ¡è®°å½•")
    
    # æ’åº
    merged_df = merged_df.sort_values(['å­¦æ ¡åç§°', 'æŒ‡æ ‡åç§°'])
    
    # æ·»åŠ æ ‡å‡†ç­”æ¡ˆåˆ—ï¼ˆåˆå§‹å€¼ä¸º"å¾…å¡«å……"ï¼‰
    merged_df.insert(2, 'æ ‡å‡†ç­”æ¡ˆ', 'å¾…å¡«å……')
    
    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
    merged_df = merged_df[['å­¦æ ¡åç§°', 'æŒ‡æ ‡åç§°', 'æ ‡å‡†ç­”æ¡ˆ', 'AIç­”æ¡ˆ']]
    
    # ç»Ÿè®¡ä¿¡æ¯
    unique_schools = merged_df['å­¦æ ¡åç§°'].nunique()
    unique_metrics = merged_df['æŒ‡æ ‡åç§°'].nunique()
    
    print(f"\nğŸ“ˆ åˆå¹¶ç»“æœç»Ÿè®¡:")
    print(f"  æ€»è®°å½•æ•°: {len(merged_df)}")
    print(f"  å­¦æ ¡æ•°é‡: {unique_schools}")
    print(f"  æŒ‡æ ‡æ•°é‡: {unique_metrics}")
    print(f"  å¹³å‡æ¯æ ¡æŒ‡æ ‡æ•°: {len(merged_df) / unique_schools:.1f}")
    
    # æ˜¾ç¤ºå­¦æ ¡åˆ—è¡¨
    print(f"\nğŸ« åŒ…å«çš„å­¦æ ¡:")
    schools = sorted(merged_df['å­¦æ ¡åç§°'].unique())
    for i, school in enumerate(schools, 1):
        record_count = len(merged_df[merged_df['å­¦æ ¡åç§°'] == school])
        print(f"  {i:2d}. {school} ({record_count} æ¡è®°å½•)")
    
    # æ˜¾ç¤ºæŒ‡æ ‡åˆ—è¡¨
    print(f"\nğŸ“Š åŒ…å«çš„æŒ‡æ ‡:")
    metrics = sorted(merged_df['æŒ‡æ ‡åç§°'].unique())
    for i, metric in enumerate(metrics, 1):
        record_count = len(merged_df[merged_df['æŒ‡æ ‡åç§°'] == metric])
        print(f"  {i:2d}. {metric} ({record_count} æ¡è®°å½•)")
    
    # ä¿å­˜ç»“æœ
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"merged_evaluation_dataset_{timestamp}.csv"
    
    try:
        merged_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ… åˆå¹¶å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
        file_size = os.path.getsize(output_file)
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size / 1024:.1f} KB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return False

def show_file_analysis(directory="."):
    """
    åˆ†æç›®å½•ä¸­çš„CSVæ–‡ä»¶
    """
    print("ğŸ” æ–‡ä»¶åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    all_csv_files = glob.glob(os.path.join(directory, "*.csv"))
    answer_files = find_answer_csv_files(directory)
    
    print(f"ğŸ“ ç›®å½•: {os.path.abspath(directory)}")
    print(f"ğŸ“„ æ€»CSVæ–‡ä»¶æ•°: {len(all_csv_files)}")
    print(f"ğŸ“Š ç­”æ¡ˆæ–‡ä»¶æ•°: {len(answer_files)}")
    
    if answer_files:
        print(f"\nğŸ“‹ ç­”æ¡ˆæ–‡ä»¶è¯¦æƒ…:")
        for i, file in enumerate(answer_files, 1):
            filename = os.path.basename(file)
            metric = extract_metric_from_filename(file)
            file_size = os.path.getsize(file)
            
            print(f"  {i:2d}. {filename}")
            print(f"      æŒ‡æ ‡: {metric}")
            print(f"      å¤§å°: {file_size / 1024:.1f} KB")
            
            # å°è¯•è¯»å–è¡Œæ•°
            try:
                df = pd.read_csv(file, encoding='utf-8-sig')
                print(f"      è®°å½•: {len(df)} è¡Œ")
            except:
                print(f"      è®°å½•: æ— æ³•è¯»å–")
            print()
    
    # åˆ†æéç­”æ¡ˆæ–‡ä»¶
    other_files = [f for f in all_csv_files if f not in answer_files]
    if other_files:
        print(f"ğŸ“‹ å…¶ä»–CSVæ–‡ä»¶:")
        for file in other_files:
            filename = os.path.basename(file)
            file_size = os.path.getsize(file)
            print(f"  â€¢ {filename} ({file_size / 1024:.1f} KB)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ åˆ†è¡¨æ ¼æ±‡æ€»å·¥å…·")
    print("=" * 50)
    
    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command in ['help', '-h', '--help']:
            print("ä½¿ç”¨è¯´æ˜:")
            print("  python merge_answer_tables.py                    # åˆå¹¶å½“å‰ç›®å½•çš„æ‰€æœ‰ç­”æ¡ˆæ–‡ä»¶")
            print("  python merge_answer_tables.py analyze            # åˆ†æç›®å½•ä¸­çš„CSVæ–‡ä»¶")
            print("  python merge_answer_tables.py /path/to/directory  # æŒ‡å®šç›®å½•è¿›è¡Œåˆå¹¶")
            print("  python merge_answer_tables.py help               # æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
            return
        
        elif command == 'analyze':
            directory = sys.argv[2] if len(sys.argv) > 2 else "."
            show_file_analysis(directory)
            return
        
        elif os.path.isdir(command):
            # å¦‚æœå‚æ•°æ˜¯ç›®å½•è·¯å¾„
            directory = command
            output_file = sys.argv[2] if len(sys.argv) > 2 else None
        else:
            print(f"âŒ æ— æ•ˆçš„å‘½ä»¤æˆ–è·¯å¾„: {command}")
            return
    else:
        # é»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•
        directory = "."
        output_file = None
    
    # æ‰§è¡Œåˆå¹¶
    success = merge_answer_tables(directory, output_file)
    
    if success:
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆ!")
    else:
        print(f"\nâŒ ä»»åŠ¡å¤±è´¥!")

if __name__ == "__main__":
    main()